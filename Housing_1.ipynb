{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Housing 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caroref/model/blob/master/Housing_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ascUYWYXFlTS",
        "colab_type": "code",
        "outputId": "82f284ca-1b5b-4bf6-99f1-00bdd123ed17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TensorFlow version is \", tf.__version__)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0605 08:09:42.763592 139783162365824 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version is  1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs-4SeS3Fpc5",
        "colab_type": "code",
        "outputId": "94634b98-8c8f-470d-a8d8-dc8e02b68fde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Allow Colab to access drive\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "#linking Colab to use drive as if local\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDNLdWVxP2jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main directories\n",
        "trn_dir = 'drive/Colab/Final/Test1/Train/'\n",
        "tst_dir = 'drive/Colab/Final/Test1/Test/'\n",
        "\n",
        "#subdirectories\n",
        "trn_like = os.path.join(trn_dir, 'Like')\n",
        "tst_like = os.path.join(tst_dir, 'Like')\n",
        "\n",
        "trn_disl = os.path.join(trn_dir, 'Dislike')\n",
        "tst_disl = os.path.join(tst_dir, 'Dislike')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo3GVjj1FsEn",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5BtzOSmbZB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All images will be resized to image_size\n",
        "\n",
        "image_size = 640 \n",
        "batch_size = 100\n",
        "\n",
        "# create Image Data Generator for Image Data Augmentation\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "                rescale=1./255, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTvB7EieGDAN",
        "colab_type": "code",
        "outputId": "712e77de-ec6d-4e58-8050-081d14eb7128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Create the train and validation generators and specify the train dataset directory, \n",
        "# image size, batch size, binary classification, and subset (training or validation).\n",
        "\n",
        "\n",
        "# Flow training images in batches using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,  \n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode ='binary',\n",
        "                subset = 'training') \n",
        "\n",
        "# Flow validation images in batches using validation_datagen generator\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,\n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode='binary',\n",
        "                subset = 'validation')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ur5r8uGWQO",
        "colab_type": "text"
      },
      "source": [
        "# Create the base model from the pre-trained convnets\n",
        "\n",
        "Using the output of last layer (bottleneck layer). Keep other layers frozen (these are specific to their training, less useful here?) Bottleneck layer should be more general.\n",
        "\n",
        "Using Inception v3 model with weights trained on ImageNet.\n",
        "The argument, include_top=False, means that we have a network that isn't including classification layers at the top. This is good for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCLmagoGbc3",
        "colab_type": "code",
        "outputId": "2d53dab4-6437-484a-9d45-ce07398e4608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "IMG_SHAPE = (image_size, image_size, 3)\n",
        "\n",
        "base_model = keras.applications.inception_v3.InceptionV3(\n",
        "             include_top=False, weights='imagenet', input_shape=IMG_SHAPE)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0605 08:13:34.451589 139783162365824 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 8s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wXzV2zlGduw",
        "colab_type": "text"
      },
      "source": [
        "# Feature extraction\n",
        "We will freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.\n",
        "\n",
        "It's important to freeze the convolutional based before we compile and train the model. By freezing (or setting layer.trainable = False), we prevent the weights in these layers from being updated during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCk3KhmGGhaH",
        "colab_type": "code",
        "outputId": "10a18cf6-ba30-4b58-ddcd-69e7542bcb1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11821
        }
      },
      "source": [
        "#freeze base\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "# Look at the base model architecture\n",
        "base_model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 640, 640, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 319, 319, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 319, 319, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 319, 319, 32) 0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 317, 317, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 317, 317, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 317, 317, 32) 0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 317, 317, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 317, 317, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 317, 317, 64) 0           batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 158, 158, 64) 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 158, 158, 80) 5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_3 (Batch (None, 158, 158, 80) 240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 158, 158, 80) 0           batch_normalization_v1_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 156, 156, 192 138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 156, 156, 192 576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 156, 156, 192 0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 77, 77, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_8 (Batch (None, 77, 77, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 77, 77, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 77, 77, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_6 (Batch (None, 77, 77, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_9 (Batch (None, 77, 77, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 77, 77, 48)   0           batch_normalization_v1_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 77, 77, 96)   0           batch_normalization_v1_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 77, 77, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 77, 77, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 77, 77, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 77, 77, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_5 (Batch (None, 77, 77, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_7 (Batch (None, 77, 77, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_10 (Batc (None, 77, 77, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_11 (Batc (None, 77, 77, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_10[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 77, 77, 32)   0           batch_normalization_v1_11[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 77, 77, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_15 (Batc (None, 77, 77, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_15[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 77, 77, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 77, 77, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_13 (Batc (None, 77, 77, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_16 (Batc (None, 77, 77, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_13[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_16[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 77, 77, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 77, 77, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 77, 77, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 77, 77, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_12 (Batc (None, 77, 77, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_14 (Batc (None, 77, 77, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_17 (Batc (None, 77, 77, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_18 (Batc (None, 77, 77, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_12[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_14[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_17[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_18[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 77, 77, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_22 (Batc (None, 77, 77, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_22[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 77, 77, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 77, 77, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_20 (Batc (None, 77, 77, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_23 (Batc (None, 77, 77, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_20[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_23[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 77, 77, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 77, 77, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 77, 77, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 77, 77, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_19 (Batc (None, 77, 77, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_21 (Batc (None, 77, 77, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_24 (Batc (None, 77, 77, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_25 (Batc (None, 77, 77, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_19[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_21[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_24[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_25[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 77, 77, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 77, 77, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_27 (Batc (None, 77, 77, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_27[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 77, 77, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_28 (Batc (None, 77, 77, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_28[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 38, 38, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 38, 38, 96)   82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_26 (Batc (None, 38, 38, 384)  1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_29 (Batc (None, 38, 38, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 38, 38, 384)  0           batch_normalization_v1_26[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 38, 38, 96)   0           batch_normalization_v1_29[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 38, 38, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 38, 38, 768)  0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_34 (Batc (None, 38, 38, 128)  384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_34[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 38, 38, 128)  114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_35 (Batc (None, 38, 38, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_35[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 38, 38, 128)  114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_31 (Batc (None, 38, 38, 128)  384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_36 (Batc (None, 38, 38, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_31[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_36[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 38, 38, 128)  114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 38, 38, 128)  114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_32 (Batc (None, 38, 38, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_37 (Batc (None, 38, 38, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_32[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_37[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 38, 38, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 38, 38, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 38, 38, 192)  172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 38, 38, 192)  172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_30 (Batc (None, 38, 38, 192)  576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_33 (Batc (None, 38, 38, 192)  576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_38 (Batc (None, 38, 38, 192)  576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_39 (Batc (None, 38, 38, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_30[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_33[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_38[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_39[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 38, 38, 768)  0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_44 (Batc (None, 38, 38, 160)  480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_44[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 38, 38, 160)  179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_45 (Batc (None, 38, 38, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_45[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 38, 38, 160)  179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_41 (Batc (None, 38, 38, 160)  480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_46 (Batc (None, 38, 38, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_41[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_46[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 38, 38, 160)  179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 38, 38, 160)  179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_42 (Batc (None, 38, 38, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_47 (Batc (None, 38, 38, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_42[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_47[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 38, 38, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 38, 38, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 38, 38, 192)  215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 38, 38, 192)  215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_40 (Batc (None, 38, 38, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_43 (Batc (None, 38, 38, 192)  576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_48 (Batc (None, 38, 38, 192)  576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_49 (Batc (None, 38, 38, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_40[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_43[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_48[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_49[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 38, 38, 768)  0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_54 (Batc (None, 38, 38, 160)  480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_54[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 38, 38, 160)  179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_55 (Batc (None, 38, 38, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_55[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 38, 38, 160)  179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_51 (Batc (None, 38, 38, 160)  480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_56 (Batc (None, 38, 38, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_51[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_56[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 38, 38, 160)  179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 38, 38, 160)  179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_52 (Batc (None, 38, 38, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_57 (Batc (None, 38, 38, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_52[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_57[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 38, 38, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 38, 38, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 38, 38, 192)  215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 38, 38, 192)  215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_50 (Batc (None, 38, 38, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_53 (Batc (None, 38, 38, 192)  576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_58 (Batc (None, 38, 38, 192)  576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_59 (Batc (None, 38, 38, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_50[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_53[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_58[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_59[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 38, 38, 768)  0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_64 (Batc (None, 38, 38, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_64[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 38, 38, 192)  258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_65 (Batc (None, 38, 38, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_65[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 38, 38, 192)  258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_61 (Batc (None, 38, 38, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_66 (Batc (None, 38, 38, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_61[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_66[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 38, 38, 192)  258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 38, 38, 192)  258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_62 (Batc (None, 38, 38, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_67 (Batc (None, 38, 38, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_62[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_67[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 38, 38, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 38, 38, 192)  258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 38, 38, 192)  258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_60 (Batc (None, 38, 38, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_63 (Batc (None, 38, 38, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_68 (Batc (None, 38, 38, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_69 (Batc (None, 38, 38, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_60[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_63[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_68[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_69[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 38, 38, 768)  0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_72 (Batc (None, 38, 38, 192)  576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_72[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 38, 38, 192)  258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_73 (Batc (None, 38, 38, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_73[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 38, 38, 192)  258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_70 (Batc (None, 38, 38, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_74 (Batc (None, 38, 38, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_70[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_74[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 18, 18, 320)  552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 18, 18, 192)  331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_71 (Batc (None, 18, 18, 320)  960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_75 (Batc (None, 18, 18, 192)  576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_71[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_75[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 18, 18, 768)  0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 18, 18, 1280) 0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 18, 18, 448)  573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_80 (Batc (None, 18, 18, 448)  1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_80[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 18, 18, 384)  491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 18, 18, 384)  1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_77 (Batc (None, 18, 18, 384)  1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_81 (Batc (None, 18, 18, 384)  1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_77[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_81[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 18, 18, 1280) 0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 18, 18, 320)  409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_78 (Batc (None, 18, 18, 384)  1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_79 (Batc (None, 18, 18, 384)  1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_82 (Batc (None, 18, 18, 384)  1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_83 (Batc (None, 18, 18, 384)  1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 18, 18, 192)  245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_76 (Batc (None, 18, 18, 320)  960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_78[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_79[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_82[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_83[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_84 (Batc (None, 18, 18, 192)  576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_76[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 18, 18, 768)  0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 18, 18, 768)  0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_84[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 18, 18, 2048) 0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 18, 18, 448)  917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_89 (Batc (None, 18, 18, 448)  1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_89[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 18, 18, 384)  786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 18, 18, 384)  1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_86 (Batc (None, 18, 18, 384)  1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_90 (Batc (None, 18, 18, 384)  1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_86[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_90[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 18, 18, 2048) 0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 18, 18, 320)  655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_87 (Batc (None, 18, 18, 384)  1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_88 (Batc (None, 18, 18, 384)  1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_91 (Batc (None, 18, 18, 384)  1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_92 (Batc (None, 18, 18, 384)  1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 18, 18, 192)  393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_85 (Batc (None, 18, 18, 320)  960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_87[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_88[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_91[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_92[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_93 (Batc (None, 18, 18, 192)  576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_85[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 18, 18, 768)  0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 18, 18, 768)  0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_93[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 18, 18, 2048) 0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 0\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUc1ad5qGnMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add a classification head\n",
        "#adding a few layers on top of the base model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  keras.layers.GlobalAveragePooling2D(),\n",
        "  keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "#relatively basic layer. no deep neural networks (all happening beforehand). Similar to a logit regression. plain classification\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbhLA3TGr1X",
        "colab_type": "code",
        "outputId": "d1e3fa72-73a2-49a4-b8bd-a40f46074bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "#Compile the model (Must do before training!)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 18, 18, 2048)      21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 21,804,833\n",
            "Trainable params: 2,049\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th655DVEGwop",
        "colab_type": "code",
        "outputId": "0873aed0-64e6-41d4-d0dd-b1a504dfe5ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#These 2K trainable parameters are divided among 2 TensorFlow Variable objects, the weights and biases of the two dense layers:\n",
        "\n",
        "\n",
        "len(model.trainable_variables)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZeMfEwoG3wm",
        "colab_type": "text"
      },
      "source": [
        "**Train the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNgpgcJjG8xl",
        "colab_type": "code",
        "outputId": "975f8bd9-01e3-425d-e07c-52628281e5ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8144
        }
      },
      "source": [
        "epochs = 150\n",
        "steps_per_epoch = train_generator.n // batch_size\n",
        "validation_steps = validation_generator.n // batch_size\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch = steps_per_epoch,\n",
        "                              epochs=epochs,\n",
        "                              workers=4,\n",
        "                              validation_data=validation_generator,\n",
        "                              validation_steps=validation_steps)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0605 08:13:58.317622 139783162365824 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "1/1 [==============================] - 108s 108s/step - loss: 0.6931 - acc: 0.5100\n",
            "4/4 [==============================] - 218s 54s/step - loss: 0.7028 - acc: 0.4475 - val_loss: 0.6931 - val_acc: 0.5100\n",
            "Epoch 2/150\n",
            "1/1 [==============================] - 29s 29s/step - loss: 0.6914 - acc: 0.5100\n",
            "4/4 [==============================] - 37s 9s/step - loss: 0.6963 - acc: 0.4800 - val_loss: 0.6914 - val_acc: 0.5100\n",
            "Epoch 3/150\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6920 - acc: 0.5100\n",
            "4/4 [==============================] - 35s 9s/step - loss: 0.6971 - acc: 0.4975 - val_loss: 0.6920 - val_acc: 0.5100\n",
            "Epoch 4/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6904 - acc: 0.5200\n",
            "4/4 [==============================] - 16s 4s/step - loss: 0.6954 - acc: 0.4825 - val_loss: 0.6904 - val_acc: 0.5200\n",
            "Epoch 5/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6893 - acc: 0.5500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6927 - acc: 0.5200 - val_loss: 0.6893 - val_acc: 0.5500\n",
            "Epoch 6/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6897 - acc: 0.5400\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6910 - acc: 0.5175 - val_loss: 0.6897 - val_acc: 0.5400\n",
            "Epoch 7/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6898 - acc: 0.5300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6897 - acc: 0.5550 - val_loss: 0.6898 - val_acc: 0.5300\n",
            "Epoch 8/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6891 - acc: 0.5600\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6889 - acc: 0.5550 - val_loss: 0.6891 - val_acc: 0.5600\n",
            "Epoch 9/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6889 - acc: 0.5700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6883 - acc: 0.5500 - val_loss: 0.6889 - val_acc: 0.5700\n",
            "Epoch 10/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6886 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6851 - acc: 0.5650 - val_loss: 0.6886 - val_acc: 0.6000\n",
            "Epoch 11/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6878 - acc: 0.5700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6838 - acc: 0.6050 - val_loss: 0.6878 - val_acc: 0.5700\n",
            "Epoch 12/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6868 - acc: 0.5800\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6819 - acc: 0.5925 - val_loss: 0.6868 - val_acc: 0.5800\n",
            "Epoch 13/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6864 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6822 - acc: 0.5950 - val_loss: 0.6864 - val_acc: 0.5900\n",
            "Epoch 14/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6859 - acc: 0.5300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6796 - acc: 0.5975 - val_loss: 0.6859 - val_acc: 0.5300\n",
            "Epoch 15/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6858 - acc: 0.5500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6766 - acc: 0.6025 - val_loss: 0.6858 - val_acc: 0.5500\n",
            "Epoch 16/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6854 - acc: 0.5300\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6769 - acc: 0.5950 - val_loss: 0.6854 - val_acc: 0.5300\n",
            "Epoch 17/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6849 - acc: 0.5200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6739 - acc: 0.6300 - val_loss: 0.6849 - val_acc: 0.5200\n",
            "Epoch 18/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6845 - acc: 0.5300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6752 - acc: 0.6050 - val_loss: 0.6845 - val_acc: 0.5300\n",
            "Epoch 19/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6840 - acc: 0.5500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6716 - acc: 0.6250 - val_loss: 0.6840 - val_acc: 0.5500\n",
            "Epoch 20/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6835 - acc: 0.5400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6711 - acc: 0.6250 - val_loss: 0.6835 - val_acc: 0.5400\n",
            "Epoch 21/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6827 - acc: 0.5700\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6708 - acc: 0.6400 - val_loss: 0.6827 - val_acc: 0.5700\n",
            "Epoch 22/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6821 - acc: 0.5500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6700 - acc: 0.6475 - val_loss: 0.6821 - val_acc: 0.5500\n",
            "Epoch 23/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6816 - acc: 0.5400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6658 - acc: 0.6475 - val_loss: 0.6816 - val_acc: 0.5400\n",
            "Epoch 24/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6810 - acc: 0.5600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6674 - acc: 0.6550 - val_loss: 0.6810 - val_acc: 0.5600\n",
            "Epoch 25/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6804 - acc: 0.5400\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6644 - acc: 0.6700 - val_loss: 0.6804 - val_acc: 0.5400\n",
            "Epoch 26/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6797 - acc: 0.5600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6629 - acc: 0.6575 - val_loss: 0.6797 - val_acc: 0.5600\n",
            "Epoch 27/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6788 - acc: 0.5500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6629 - acc: 0.6625 - val_loss: 0.6788 - val_acc: 0.5500\n",
            "Epoch 28/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6781 - acc: 0.5400\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6600 - acc: 0.6650 - val_loss: 0.6781 - val_acc: 0.5400\n",
            "Epoch 29/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6774 - acc: 0.5400\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6591 - acc: 0.6750 - val_loss: 0.6774 - val_acc: 0.5400\n",
            "Epoch 30/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6766 - acc: 0.5500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6575 - acc: 0.6925 - val_loss: 0.6766 - val_acc: 0.5500\n",
            "Epoch 31/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6759 - acc: 0.5500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6575 - acc: 0.6550 - val_loss: 0.6759 - val_acc: 0.5500\n",
            "Epoch 32/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6752 - acc: 0.5600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6558 - acc: 0.6650 - val_loss: 0.6752 - val_acc: 0.5600\n",
            "Epoch 33/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6747 - acc: 0.5600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6541 - acc: 0.6975 - val_loss: 0.6747 - val_acc: 0.5600\n",
            "Epoch 34/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6741 - acc: 0.5600\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6541 - acc: 0.6925 - val_loss: 0.6741 - val_acc: 0.5600\n",
            "Epoch 35/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6741 - acc: 0.5800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6534 - acc: 0.6825 - val_loss: 0.6741 - val_acc: 0.5800\n",
            "Epoch 36/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6734 - acc: 0.5800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6501 - acc: 0.7075 - val_loss: 0.6734 - val_acc: 0.5800\n",
            "Epoch 37/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6728 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6503 - acc: 0.6800 - val_loss: 0.6728 - val_acc: 0.6100\n",
            "Epoch 38/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6723 - acc: 0.6100\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6484 - acc: 0.6775 - val_loss: 0.6723 - val_acc: 0.6100\n",
            "Epoch 39/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6717 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6471 - acc: 0.6975 - val_loss: 0.6717 - val_acc: 0.6100\n",
            "Epoch 40/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6711 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6463 - acc: 0.7050 - val_loss: 0.6711 - val_acc: 0.6100\n",
            "Epoch 41/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6707 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6469 - acc: 0.7100 - val_loss: 0.6707 - val_acc: 0.6200\n",
            "Epoch 42/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6701 - acc: 0.6000\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6429 - acc: 0.6975 - val_loss: 0.6701 - val_acc: 0.6000\n",
            "Epoch 43/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6697 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6438 - acc: 0.7125 - val_loss: 0.6697 - val_acc: 0.6100\n",
            "Epoch 44/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6693 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6431 - acc: 0.6775 - val_loss: 0.6693 - val_acc: 0.6100\n",
            "Epoch 45/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6687 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6408 - acc: 0.7175 - val_loss: 0.6687 - val_acc: 0.6200\n",
            "Epoch 46/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6682 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6399 - acc: 0.7200 - val_loss: 0.6682 - val_acc: 0.6200\n",
            "Epoch 47/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6678 - acc: 0.6100\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6392 - acc: 0.7175 - val_loss: 0.6678 - val_acc: 0.6100\n",
            "Epoch 48/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6676 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6384 - acc: 0.7050 - val_loss: 0.6676 - val_acc: 0.6200\n",
            "Epoch 49/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6668 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6362 - acc: 0.7325 - val_loss: 0.6668 - val_acc: 0.6300\n",
            "Epoch 50/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6665 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6359 - acc: 0.7275 - val_loss: 0.6665 - val_acc: 0.6300\n",
            "Epoch 51/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6662 - acc: 0.6200\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6363 - acc: 0.7275 - val_loss: 0.6662 - val_acc: 0.6200\n",
            "Epoch 52/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6658 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6319 - acc: 0.7150 - val_loss: 0.6658 - val_acc: 0.6200\n",
            "Epoch 53/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6653 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6337 - acc: 0.6925 - val_loss: 0.6653 - val_acc: 0.6300\n",
            "Epoch 54/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6650 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6305 - acc: 0.7275 - val_loss: 0.6650 - val_acc: 0.6300\n",
            "Epoch 55/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6650 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6321 - acc: 0.7150 - val_loss: 0.6650 - val_acc: 0.5900\n",
            "Epoch 56/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6643 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6320 - acc: 0.7175 - val_loss: 0.6643 - val_acc: 0.6000\n",
            "Epoch 57/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6639 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6296 - acc: 0.7325 - val_loss: 0.6639 - val_acc: 0.6200\n",
            "Epoch 58/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6635 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6303 - acc: 0.7175 - val_loss: 0.6635 - val_acc: 0.6400\n",
            "Epoch 59/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6634 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6312 - acc: 0.7100 - val_loss: 0.6634 - val_acc: 0.6000\n",
            "Epoch 60/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6630 - acc: 0.6200\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6304 - acc: 0.7225 - val_loss: 0.6630 - val_acc: 0.6200\n",
            "Epoch 61/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6627 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6264 - acc: 0.7475 - val_loss: 0.6627 - val_acc: 0.6300\n",
            "Epoch 62/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6625 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6253 - acc: 0.7300 - val_loss: 0.6625 - val_acc: 0.6200\n",
            "Epoch 63/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6622 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6247 - acc: 0.7450 - val_loss: 0.6622 - val_acc: 0.6200\n",
            "Epoch 64/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6621 - acc: 0.6300\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6218 - acc: 0.7525 - val_loss: 0.6621 - val_acc: 0.6300\n",
            "Epoch 65/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6626 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6245 - acc: 0.7250 - val_loss: 0.6626 - val_acc: 0.6100\n",
            "Epoch 66/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6614 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6220 - acc: 0.7400 - val_loss: 0.6614 - val_acc: 0.6500\n",
            "Epoch 67/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6615 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6205 - acc: 0.7250 - val_loss: 0.6615 - val_acc: 0.6300\n",
            "Epoch 68/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6613 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6219 - acc: 0.7425 - val_loss: 0.6613 - val_acc: 0.6300\n",
            "Epoch 69/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6607 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6185 - acc: 0.7350 - val_loss: 0.6607 - val_acc: 0.6500\n",
            "Epoch 70/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6602 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6215 - acc: 0.7450 - val_loss: 0.6602 - val_acc: 0.6500\n",
            "Epoch 71/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6601 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6169 - acc: 0.7425 - val_loss: 0.6601 - val_acc: 0.6400\n",
            "Epoch 72/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6594 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6173 - acc: 0.7375 - val_loss: 0.6594 - val_acc: 0.6500\n",
            "Epoch 73/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6592 - acc: 0.6600\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6158 - acc: 0.7400 - val_loss: 0.6592 - val_acc: 0.6600\n",
            "Epoch 74/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6593 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6166 - acc: 0.7450 - val_loss: 0.6593 - val_acc: 0.6300\n",
            "Epoch 75/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6589 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6148 - acc: 0.7425 - val_loss: 0.6589 - val_acc: 0.6500\n",
            "Epoch 76/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6587 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6136 - acc: 0.7275 - val_loss: 0.6587 - val_acc: 0.6400\n",
            "Epoch 77/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6587 - acc: 0.6300\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6142 - acc: 0.7475 - val_loss: 0.6587 - val_acc: 0.6300\n",
            "Epoch 78/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6581 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6124 - acc: 0.7375 - val_loss: 0.6581 - val_acc: 0.6400\n",
            "Epoch 79/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6575 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6095 - acc: 0.7525 - val_loss: 0.6575 - val_acc: 0.6500\n",
            "Epoch 80/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6573 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6114 - acc: 0.7300 - val_loss: 0.6573 - val_acc: 0.6600\n",
            "Epoch 81/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6570 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6099 - acc: 0.7400 - val_loss: 0.6570 - val_acc: 0.6500\n",
            "Epoch 82/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6568 - acc: 0.6500\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6089 - acc: 0.7425 - val_loss: 0.6568 - val_acc: 0.6500\n",
            "Epoch 83/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6565 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6090 - acc: 0.7400 - val_loss: 0.6565 - val_acc: 0.6500\n",
            "Epoch 84/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6563 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6075 - acc: 0.7625 - val_loss: 0.6563 - val_acc: 0.6500\n",
            "Epoch 85/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6559 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6057 - acc: 0.7475 - val_loss: 0.6559 - val_acc: 0.6500\n",
            "Epoch 86/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6558 - acc: 0.6300\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6029 - acc: 0.7500 - val_loss: 0.6558 - val_acc: 0.6300\n",
            "Epoch 87/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6552 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6043 - acc: 0.7400 - val_loss: 0.6552 - val_acc: 0.6700\n",
            "Epoch 88/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6550 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6044 - acc: 0.7525 - val_loss: 0.6550 - val_acc: 0.6700\n",
            "Epoch 89/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6549 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6015 - acc: 0.7475 - val_loss: 0.6549 - val_acc: 0.6300\n",
            "Epoch 90/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6547 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6017 - acc: 0.7450 - val_loss: 0.6547 - val_acc: 0.6300\n",
            "Epoch 91/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6546 - acc: 0.6300\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6011 - acc: 0.7525 - val_loss: 0.6546 - val_acc: 0.6300\n",
            "Epoch 92/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6541 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6013 - acc: 0.7525 - val_loss: 0.6541 - val_acc: 0.6300\n",
            "Epoch 93/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6540 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6000 - acc: 0.7550 - val_loss: 0.6540 - val_acc: 0.6300\n",
            "Epoch 94/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6539 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5994 - acc: 0.7550 - val_loss: 0.6539 - val_acc: 0.6300\n",
            "Epoch 95/150\n",
            "1/1 [==============================] - 34s 34s/step - loss: 0.6540 - acc: 0.6300\n",
            "4/4 [==============================] - 42s 11s/step - loss: 0.6013 - acc: 0.7525 - val_loss: 0.6540 - val_acc: 0.6300\n",
            "Epoch 96/150\n",
            "1/1 [==============================] - 13s 13s/step - loss: 0.6538 - acc: 0.6300\n",
            "4/4 [==============================] - 21s 5s/step - loss: 0.5985 - acc: 0.7400 - val_loss: 0.6538 - val_acc: 0.6300\n",
            "Epoch 97/150\n",
            "1/1 [==============================] - 49s 49s/step - loss: 0.6534 - acc: 0.6400\n",
            "4/4 [==============================] - 90s 23s/step - loss: 0.5946 - acc: 0.7550 - val_loss: 0.6534 - val_acc: 0.6400\n",
            "Epoch 98/150\n",
            "1/1 [==============================] - 22s 22s/step - loss: 0.6532 - acc: 0.6400\n",
            "4/4 [==============================] - 31s 8s/step - loss: 0.5960 - acc: 0.7550 - val_loss: 0.6532 - val_acc: 0.6400\n",
            "Epoch 99/150\n",
            "1/1 [==============================] - 50s 50s/step - loss: 0.6530 - acc: 0.6700\n",
            "4/4 [==============================] - 88s 22s/step - loss: 0.5950 - acc: 0.7450 - val_loss: 0.6530 - val_acc: 0.6700\n",
            "Epoch 100/150\n",
            "1/1 [==============================] - 21s 21s/step - loss: 0.6528 - acc: 0.6700\n",
            "4/4 [==============================] - 30s 7s/step - loss: 0.5979 - acc: 0.7200 - val_loss: 0.6528 - val_acc: 0.6700\n",
            "Epoch 101/150\n",
            "1/1 [==============================] - 51s 51s/step - loss: 0.6526 - acc: 0.6700\n",
            "4/4 [==============================] - 91s 23s/step - loss: 0.5938 - acc: 0.7550 - val_loss: 0.6526 - val_acc: 0.6700\n",
            "Epoch 102/150\n",
            "1/1 [==============================] - 24s 24s/step - loss: 0.6525 - acc: 0.6300\n",
            "4/4 [==============================] - 32s 8s/step - loss: 0.5950 - acc: 0.7375 - val_loss: 0.6525 - val_acc: 0.6300\n",
            "Epoch 103/150\n",
            "1/1 [==============================] - 50s 50s/step - loss: 0.6526 - acc: 0.6200\n",
            "4/4 [==============================] - 85s 21s/step - loss: 0.5939 - acc: 0.7600 - val_loss: 0.6526 - val_acc: 0.6200\n",
            "Epoch 104/150\n",
            "1/1 [==============================] - 27s 27s/step - loss: 0.6520 - acc: 0.6300\n",
            "4/4 [==============================] - 36s 9s/step - loss: 0.5912 - acc: 0.7625 - val_loss: 0.6520 - val_acc: 0.6300\n",
            "Epoch 105/150\n",
            "1/1 [==============================] - 53s 53s/step - loss: 0.6516 - acc: 0.6500\n",
            "4/4 [==============================] - 88s 22s/step - loss: 0.5908 - acc: 0.7525 - val_loss: 0.6516 - val_acc: 0.6500\n",
            "Epoch 106/150\n",
            "1/1 [==============================] - 24s 24s/step - loss: 0.6516 - acc: 0.6300\n",
            "4/4 [==============================] - 33s 8s/step - loss: 0.5908 - acc: 0.7475 - val_loss: 0.6516 - val_acc: 0.6300\n",
            "Epoch 107/150\n",
            "1/1 [==============================] - 53s 53s/step - loss: 0.6511 - acc: 0.6900\n",
            "4/4 [==============================] - 85s 21s/step - loss: 0.5900 - acc: 0.7500 - val_loss: 0.6511 - val_acc: 0.6900\n",
            "Epoch 108/150\n",
            "1/1 [==============================] - 27s 27s/step - loss: 0.6509 - acc: 0.7000\n",
            "4/4 [==============================] - 36s 9s/step - loss: 0.5882 - acc: 0.7575 - val_loss: 0.6509 - val_acc: 0.7000\n",
            "Epoch 109/150\n",
            "1/1 [==============================] - 53s 53s/step - loss: 0.6506 - acc: 0.6700\n",
            "4/4 [==============================] - 89s 22s/step - loss: 0.5881 - acc: 0.7425 - val_loss: 0.6506 - val_acc: 0.6700\n",
            "Epoch 110/150\n",
            "1/1 [==============================] - 27s 27s/step - loss: 0.6505 - acc: 0.6500\n",
            "4/4 [==============================] - 36s 9s/step - loss: 0.5874 - acc: 0.7650 - val_loss: 0.6505 - val_acc: 0.6500\n",
            "Epoch 111/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6501 - acc: 0.6900\n",
            "4/4 [==============================] - 21s 5s/step - loss: 0.5904 - acc: 0.7550 - val_loss: 0.6501 - val_acc: 0.6900\n",
            "Epoch 112/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6500 - acc: 0.6900\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5857 - acc: 0.7550 - val_loss: 0.6500 - val_acc: 0.6900\n",
            "Epoch 113/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6497 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5876 - acc: 0.7475 - val_loss: 0.6497 - val_acc: 0.6800\n",
            "Epoch 114/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6496 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5845 - acc: 0.7575 - val_loss: 0.6496 - val_acc: 0.6700\n",
            "Epoch 115/150\n",
            "1/1 [==============================] - 56s 56s/step - loss: 0.6495 - acc: 0.6400\n",
            "4/4 [==============================] - 65s 16s/step - loss: 0.5833 - acc: 0.7475 - val_loss: 0.6495 - val_acc: 0.6400\n",
            "Epoch 116/150\n",
            "1/1 [==============================] - 49s 49s/step - loss: 0.6494 - acc: 0.6300\n",
            "4/4 [==============================] - 58s 14s/step - loss: 0.5830 - acc: 0.7550 - val_loss: 0.6494 - val_acc: 0.6300\n",
            "Epoch 117/150\n",
            "1/1 [==============================] - 43s 43s/step - loss: 0.6487 - acc: 0.6700\n",
            "4/4 [==============================] - 52s 13s/step - loss: 0.5865 - acc: 0.7500 - val_loss: 0.6487 - val_acc: 0.6700\n",
            "Epoch 118/150\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6485 - acc: 0.6900\n",
            "4/4 [==============================] - 12s 3s/step - loss: 0.5810 - acc: 0.7500 - val_loss: 0.6485 - val_acc: 0.6900\n",
            "Epoch 119/150\n",
            "1/1 [==============================] - 50s 50s/step - loss: 0.6482 - acc: 0.6800\n",
            "4/4 [==============================] - 96s 24s/step - loss: 0.5807 - acc: 0.7525 - val_loss: 0.6482 - val_acc: 0.6800\n",
            "Epoch 120/150\n",
            "1/1 [==============================] - 16s 16s/step - loss: 0.6480 - acc: 0.6800\n",
            "4/4 [==============================] - 24s 6s/step - loss: 0.5803 - acc: 0.7375 - val_loss: 0.6480 - val_acc: 0.6800\n",
            "Epoch 121/150\n",
            "1/1 [==============================] - 51s 51s/step - loss: 0.6480 - acc: 0.6500\n",
            "4/4 [==============================] - 91s 23s/step - loss: 0.5816 - acc: 0.7475 - val_loss: 0.6480 - val_acc: 0.6500\n",
            "Epoch 122/150\n",
            "1/1 [==============================] - 18s 18s/step - loss: 0.6477 - acc: 0.6600\n",
            "4/4 [==============================] - 27s 7s/step - loss: 0.5776 - acc: 0.7525 - val_loss: 0.6477 - val_acc: 0.6600\n",
            "Epoch 123/150\n",
            "1/1 [==============================] - 55s 55s/step - loss: 0.6473 - acc: 0.6900\n",
            "4/4 [==============================] - 95s 24s/step - loss: 0.5778 - acc: 0.7625 - val_loss: 0.6473 - val_acc: 0.6900\n",
            "Epoch 124/150\n",
            "1/1 [==============================] - 19s 19s/step - loss: 0.6471 - acc: 0.6700\n",
            "4/4 [==============================] - 28s 7s/step - loss: 0.5778 - acc: 0.7600 - val_loss: 0.6471 - val_acc: 0.6700\n",
            "Epoch 125/150\n",
            "1/1 [==============================] - 51s 51s/step - loss: 0.6473 - acc: 0.6500\n",
            "4/4 [==============================] - 89s 22s/step - loss: 0.5769 - acc: 0.7500 - val_loss: 0.6473 - val_acc: 0.6500\n",
            "Epoch 126/150\n",
            "1/1 [==============================] - 21s 21s/step - loss: 0.6469 - acc: 0.6500\n",
            "4/4 [==============================] - 30s 7s/step - loss: 0.5769 - acc: 0.7675 - val_loss: 0.6469 - val_acc: 0.6500\n",
            "Epoch 127/150\n",
            "1/1 [==============================] - 49s 49s/step - loss: 0.6464 - acc: 0.6600\n",
            "4/4 [==============================] - 85s 21s/step - loss: 0.5763 - acc: 0.7625 - val_loss: 0.6464 - val_acc: 0.6600\n",
            "Epoch 128/150\n",
            "1/1 [==============================] - 29s 29s/step - loss: 0.6460 - acc: 0.6800\n",
            "4/4 [==============================] - 38s 9s/step - loss: 0.5773 - acc: 0.7525 - val_loss: 0.6460 - val_acc: 0.6800\n",
            "Epoch 129/150\n",
            "1/1 [==============================] - 52s 52s/step - loss: 0.6460 - acc: 0.6600\n",
            "4/4 [==============================] - 81s 20s/step - loss: 0.5757 - acc: 0.7575 - val_loss: 0.6460 - val_acc: 0.6600\n",
            "Epoch 130/150\n",
            "1/1 [==============================] - 34s 34s/step - loss: 0.6456 - acc: 0.6800\n",
            "4/4 [==============================] - 42s 11s/step - loss: 0.5740 - acc: 0.7650 - val_loss: 0.6456 - val_acc: 0.6800\n",
            "Epoch 131/150\n",
            "1/1 [==============================] - 50s 50s/step - loss: 0.6455 - acc: 0.6800\n",
            "4/4 [==============================] - 75s 19s/step - loss: 0.5743 - acc: 0.7575 - val_loss: 0.6455 - val_acc: 0.6800\n",
            "Epoch 132/150\n",
            "1/1 [==============================] - 35s 35s/step - loss: 0.6458 - acc: 0.6500\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.5748 - acc: 0.7650 - val_loss: 0.6458 - val_acc: 0.6500\n",
            "Epoch 133/150\n",
            "1/1 [==============================] - 53s 53s/step - loss: 0.6452 - acc: 0.6700\n",
            "4/4 [==============================] - 78s 19s/step - loss: 0.5715 - acc: 0.7750 - val_loss: 0.6452 - val_acc: 0.6700\n",
            "Epoch 134/150\n",
            "1/1 [==============================] - 35s 35s/step - loss: 0.6449 - acc: 0.6800\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.5731 - acc: 0.7575 - val_loss: 0.6449 - val_acc: 0.6800\n",
            "Epoch 135/150\n",
            "1/1 [==============================] - 54s 54s/step - loss: 0.6447 - acc: 0.6700\n",
            "4/4 [==============================] - 77s 19s/step - loss: 0.5723 - acc: 0.7775 - val_loss: 0.6447 - val_acc: 0.6700\n",
            "Epoch 136/150\n",
            "1/1 [==============================] - 38s 38s/step - loss: 0.6445 - acc: 0.6800\n",
            "4/4 [==============================] - 46s 12s/step - loss: 0.5736 - acc: 0.7525 - val_loss: 0.6445 - val_acc: 0.6800\n",
            "Epoch 137/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6443 - acc: 0.6800\n",
            "4/4 [==============================] - 22s 5s/step - loss: 0.5698 - acc: 0.7625 - val_loss: 0.6443 - val_acc: 0.6800\n",
            "Epoch 138/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6441 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5735 - acc: 0.7575 - val_loss: 0.6441 - val_acc: 0.6800\n",
            "Epoch 139/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6440 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5671 - acc: 0.7675 - val_loss: 0.6440 - val_acc: 0.6700\n",
            "Epoch 140/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6438 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5650 - acc: 0.7775 - val_loss: 0.6438 - val_acc: 0.6800\n",
            "Epoch 141/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6434 - acc: 0.6900\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5704 - acc: 0.7650 - val_loss: 0.6434 - val_acc: 0.6900\n",
            "Epoch 142/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6433 - acc: 0.7000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5718 - acc: 0.7625 - val_loss: 0.6433 - val_acc: 0.7000\n",
            "Epoch 143/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6431 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5664 - acc: 0.7550 - val_loss: 0.6431 - val_acc: 0.6800\n",
            "Epoch 144/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6429 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5655 - acc: 0.7725 - val_loss: 0.6429 - val_acc: 0.6800\n",
            "Epoch 145/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6434 - acc: 0.6600\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5668 - acc: 0.7525 - val_loss: 0.6434 - val_acc: 0.6600\n",
            "Epoch 146/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6427 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5697 - acc: 0.7475 - val_loss: 0.6427 - val_acc: 0.6800\n",
            "Epoch 147/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6425 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5662 - acc: 0.7650 - val_loss: 0.6425 - val_acc: 0.6800\n",
            "Epoch 148/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6420 - acc: 0.6900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5642 - acc: 0.7650 - val_loss: 0.6420 - val_acc: 0.6900\n",
            "Epoch 149/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6420 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5690 - acc: 0.7550 - val_loss: 0.6420 - val_acc: 0.6800\n",
            "Epoch 150/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6417 - acc: 0.6800\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.5680 - acc: 0.7600 - val_loss: 0.6417 - val_acc: 0.6800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgtyZLsvG_ls",
        "colab_type": "text"
      },
      "source": [
        "**Learning curves**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-ketbynHDhE",
        "colab_type": "code",
        "outputId": "1d3ee78d-3d98-4482-b189-2a198ab0ccc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "#get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy - 1')\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.title('Training and Validation Loss - 1')\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POHLQd9PPMLX",
        "colab_type": "code",
        "outputId": "1d96305d-e2b7-4473-ab3b-94f8c49fd410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "_, acc = model.evaluate_generator(generator=train_generator)\n",
        "print('Accuracy on training set: {}%'.format(round(acc, 3)*100))\n",
        "\n",
        "_, acc = model.evaluate_generator(generator=validation_generator)\n",
        "print('Accuracy on validation set: {}%'.format(round(acc, 3)*100))\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set: 76.99999809265137%\n",
            "Accuracy on validation set: 68.00000071525574%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeN1ZqTAM42i",
        "colab_type": "text"
      },
      "source": [
        "**Export Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkUfyxqDM3NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keras library import  for Saving and loading model and weights\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwvXWQnGEENs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "model_json = model.to_json()\n",
        "\n",
        "\n",
        "with open(\"drive/Colab Notebooks/model/1_model_num.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"drive/Colab Notebooks/model/1_model_num.h5\")\n",
        "model.save('drive/Colab Notebooks/model/1_model_num.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}