{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Housing 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caroref/model/blob/master/Housing_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ascUYWYXFlTS",
        "colab_type": "code",
        "outputId": "277abb01-0bcd-402e-a946-403dbd151e63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TensorFlow version is \", tf.__version__)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0605 13:46:16.543437 139743018686336 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version is  1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs-4SeS3Fpc5",
        "colab_type": "code",
        "outputId": "4bb7543b-0dfc-4c87-e24e-05911979e5f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Allow Colab to access drive\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "#linking Colab to use drive as if local\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDNLdWVxP2jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main directories\n",
        "trn_dir = 'drive/Colab/Final/Test2/Train/'\n",
        "tst_dir = 'drive/Colab/Final/Test2/Test/'\n",
        "\n",
        "#subdirectories\n",
        "trn_like = os.path.join(trn_dir, 'Like')\n",
        "tst_like = os.path.join(tst_dir, 'Like')\n",
        "\n",
        "trn_disl = os.path.join(trn_dir, 'Dislike')\n",
        "tst_disl = os.path.join(tst_dir, 'Dislike')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo3GVjj1FsEn",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5BtzOSmbZB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All images will be resized to image_size\n",
        "\n",
        "image_size = 640 \n",
        "batch_size = 100\n",
        "\n",
        "# create Image Data Generator for Image Data Augmentation\n",
        "\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "                rescale=1./255, \n",
        "                validation_split=0.2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTvB7EieGDAN",
        "colab_type": "code",
        "outputId": "c5bf5f92-9ef2-4518-b14b-f3540640a721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Create the train and validation generators and specify the train dataset directory, \n",
        "# image size, batch size, binary classification, and subset (training or validation).\n",
        "\n",
        "\n",
        "# Flow training images in batches using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,  \n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode ='binary',\n",
        "                subset = 'training') \n",
        "\n",
        "# Flow validation images in batches using validation_datagen generator\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,\n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode='binary',\n",
        "                subset = 'validation')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ur5r8uGWQO",
        "colab_type": "text"
      },
      "source": [
        "# Create the base model from the pre-trained convnets\n",
        "\n",
        "Using the output of last layer (bottleneck layer). Keep other layers frozen (these are specific to their training, less useful here?) Bottleneck layer should be more general.\n",
        "\n",
        "Using Inception v3 model with weights trained on ImageNet.\n",
        "The argument, include_top=False, means that we have a network that isn't including classification layers at the top. This is good for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCLmagoGbc3",
        "colab_type": "code",
        "outputId": "1f6ff794-070f-4857-eef9-f7cbc67e5b1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "IMG_SHAPE = (image_size, image_size, 3)\n",
        "\n",
        "base_model = keras.applications.inception_v3.InceptionV3(\n",
        "             include_top=False, weights='imagenet', input_shape=IMG_SHAPE)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0605 13:54:54.569480 139743018686336 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 5s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wXzV2zlGduw",
        "colab_type": "text"
      },
      "source": [
        "# Feature extraction\n",
        "We will freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.\n",
        "\n",
        "It's important to freeze the convolutional based before we compile and train the model. By freezing (or setting layer.trainable = False), we prevent the weights in these layers from being updated during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCk3KhmGGhaH",
        "colab_type": "code",
        "outputId": "6a10dc41-22ca-4128-9409-cec8a9aea3a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11821
        }
      },
      "source": [
        "#freeze base\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "# Look at the base model architecture\n",
        "base_model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 640, 640, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 319, 319, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 319, 319, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 319, 319, 32) 0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 317, 317, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 317, 317, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 317, 317, 32) 0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 317, 317, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 317, 317, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 317, 317, 64) 0           batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 158, 158, 64) 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 158, 158, 80) 5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_3 (Batch (None, 158, 158, 80) 240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 158, 158, 80) 0           batch_normalization_v1_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 156, 156, 192 138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 156, 156, 192 576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 156, 156, 192 0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 77, 77, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_8 (Batch (None, 77, 77, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 77, 77, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 77, 77, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_6 (Batch (None, 77, 77, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_9 (Batch (None, 77, 77, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 77, 77, 48)   0           batch_normalization_v1_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 77, 77, 96)   0           batch_normalization_v1_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 77, 77, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 77, 77, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 77, 77, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 77, 77, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_5 (Batch (None, 77, 77, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_7 (Batch (None, 77, 77, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_10 (Batc (None, 77, 77, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_11 (Batc (None, 77, 77, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_10[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 77, 77, 32)   0           batch_normalization_v1_11[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 77, 77, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_15 (Batc (None, 77, 77, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_15[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 77, 77, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 77, 77, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_13 (Batc (None, 77, 77, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_16 (Batc (None, 77, 77, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_13[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_16[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 77, 77, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 77, 77, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 77, 77, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 77, 77, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_12 (Batc (None, 77, 77, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_14 (Batc (None, 77, 77, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_17 (Batc (None, 77, 77, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_18 (Batc (None, 77, 77, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_12[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_14[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_17[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_18[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 77, 77, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_22 (Batc (None, 77, 77, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_22[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 77, 77, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 77, 77, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_20 (Batc (None, 77, 77, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_23 (Batc (None, 77, 77, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_20[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_23[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 77, 77, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 77, 77, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 77, 77, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 77, 77, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_19 (Batc (None, 77, 77, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_21 (Batc (None, 77, 77, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_24 (Batc (None, 77, 77, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_25 (Batc (None, 77, 77, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_19[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_21[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_24[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_25[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 77, 77, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 77, 77, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_27 (Batc (None, 77, 77, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_27[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 77, 77, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_28 (Batc (None, 77, 77, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_28[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 38, 38, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 38, 38, 96)   82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_26 (Batc (None, 38, 38, 384)  1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_29 (Batc (None, 38, 38, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 38, 38, 384)  0           batch_normalization_v1_26[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 38, 38, 96)   0           batch_normalization_v1_29[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 38, 38, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 38, 38, 768)  0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_34 (Batc (None, 38, 38, 128)  384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_34[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 38, 38, 128)  114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_35 (Batc (None, 38, 38, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_35[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 38, 38, 128)  114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_31 (Batc (None, 38, 38, 128)  384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_36 (Batc (None, 38, 38, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_31[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_36[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 38, 38, 128)  114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 38, 38, 128)  114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_32 (Batc (None, 38, 38, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_37 (Batc (None, 38, 38, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_32[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_37[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 38, 38, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 38, 38, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 38, 38, 192)  172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 38, 38, 192)  172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_30 (Batc (None, 38, 38, 192)  576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_33 (Batc (None, 38, 38, 192)  576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_38 (Batc (None, 38, 38, 192)  576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_39 (Batc (None, 38, 38, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_30[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_33[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_38[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_39[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 38, 38, 768)  0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_44 (Batc (None, 38, 38, 160)  480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_44[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 38, 38, 160)  179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_45 (Batc (None, 38, 38, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_45[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 38, 38, 160)  179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_41 (Batc (None, 38, 38, 160)  480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_46 (Batc (None, 38, 38, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_41[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_46[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 38, 38, 160)  179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 38, 38, 160)  179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_42 (Batc (None, 38, 38, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_47 (Batc (None, 38, 38, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_42[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_47[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 38, 38, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 38, 38, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 38, 38, 192)  215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 38, 38, 192)  215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_40 (Batc (None, 38, 38, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_43 (Batc (None, 38, 38, 192)  576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_48 (Batc (None, 38, 38, 192)  576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_49 (Batc (None, 38, 38, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_40[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_43[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_48[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_49[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 38, 38, 768)  0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_54 (Batc (None, 38, 38, 160)  480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_54[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 38, 38, 160)  179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_55 (Batc (None, 38, 38, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_55[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 38, 38, 160)  179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_51 (Batc (None, 38, 38, 160)  480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_56 (Batc (None, 38, 38, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_51[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_56[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 38, 38, 160)  179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 38, 38, 160)  179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_52 (Batc (None, 38, 38, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_57 (Batc (None, 38, 38, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_52[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_57[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 38, 38, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 38, 38, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 38, 38, 192)  215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 38, 38, 192)  215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_50 (Batc (None, 38, 38, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_53 (Batc (None, 38, 38, 192)  576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_58 (Batc (None, 38, 38, 192)  576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_59 (Batc (None, 38, 38, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_50[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_53[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_58[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_59[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 38, 38, 768)  0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_64 (Batc (None, 38, 38, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_64[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 38, 38, 192)  258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_65 (Batc (None, 38, 38, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_65[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 38, 38, 192)  258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_61 (Batc (None, 38, 38, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_66 (Batc (None, 38, 38, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_61[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_66[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 38, 38, 192)  258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 38, 38, 192)  258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_62 (Batc (None, 38, 38, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_67 (Batc (None, 38, 38, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_62[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_67[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 38, 38, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 38, 38, 192)  258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 38, 38, 192)  258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_60 (Batc (None, 38, 38, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_63 (Batc (None, 38, 38, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_68 (Batc (None, 38, 38, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_69 (Batc (None, 38, 38, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_60[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_63[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_68[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_69[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 38, 38, 768)  0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_72 (Batc (None, 38, 38, 192)  576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_72[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 38, 38, 192)  258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_73 (Batc (None, 38, 38, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_73[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 38, 38, 192)  258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_70 (Batc (None, 38, 38, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_74 (Batc (None, 38, 38, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_70[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_74[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 18, 18, 320)  552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 18, 18, 192)  331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_71 (Batc (None, 18, 18, 320)  960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_75 (Batc (None, 18, 18, 192)  576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_71[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_75[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 18, 18, 768)  0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 18, 18, 1280) 0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 18, 18, 448)  573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_80 (Batc (None, 18, 18, 448)  1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_80[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 18, 18, 384)  491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 18, 18, 384)  1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_77 (Batc (None, 18, 18, 384)  1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_81 (Batc (None, 18, 18, 384)  1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_77[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_81[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 18, 18, 1280) 0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 18, 18, 320)  409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_78 (Batc (None, 18, 18, 384)  1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_79 (Batc (None, 18, 18, 384)  1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_82 (Batc (None, 18, 18, 384)  1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_83 (Batc (None, 18, 18, 384)  1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 18, 18, 192)  245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_76 (Batc (None, 18, 18, 320)  960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_78[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_79[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_82[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_83[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_84 (Batc (None, 18, 18, 192)  576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_76[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 18, 18, 768)  0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 18, 18, 768)  0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_84[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 18, 18, 2048) 0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 18, 18, 448)  917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_89 (Batc (None, 18, 18, 448)  1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_89[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 18, 18, 384)  786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 18, 18, 384)  1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_86 (Batc (None, 18, 18, 384)  1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_90 (Batc (None, 18, 18, 384)  1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_86[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_90[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 18, 18, 2048) 0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 18, 18, 320)  655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_87 (Batc (None, 18, 18, 384)  1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_88 (Batc (None, 18, 18, 384)  1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_91 (Batc (None, 18, 18, 384)  1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_92 (Batc (None, 18, 18, 384)  1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 18, 18, 192)  393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_85 (Batc (None, 18, 18, 320)  960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_87[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_88[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_91[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_92[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_93 (Batc (None, 18, 18, 192)  576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_85[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 18, 18, 768)  0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 18, 18, 768)  0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_93[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 18, 18, 2048) 0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 0\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUc1ad5qGnMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add a classification head\n",
        "#adding a few layers on top of the base model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  keras.layers.GlobalAveragePooling2D(),\n",
        "  keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "#relatively basic layer. no deep neural networks (all happening beforehand). Similar to a logit regression. plain classification\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbhLA3TGr1X",
        "colab_type": "code",
        "outputId": "42aa33bc-48bd-4613-cfbc-b42b527da43d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "#Compile the model (Must do before training!)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 18, 18, 2048)      21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 21,804,833\n",
            "Trainable params: 2,049\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th655DVEGwop",
        "colab_type": "code",
        "outputId": "94c96519-47db-443b-8589-000ae935bf91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#These 2K trainable parameters are divided among 2 TensorFlow Variable objects, the weights and biases of the two dense layers:\n",
        "\n",
        "\n",
        "len(model.trainable_variables)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZeMfEwoG3wm",
        "colab_type": "text"
      },
      "source": [
        "**Train the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNgpgcJjG8xl",
        "colab_type": "code",
        "outputId": "5fa8a2bd-5582-4b83-f741-82de86420a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8144
        }
      },
      "source": [
        "epochs = 150\n",
        "steps_per_epoch = train_generator.n // batch_size\n",
        "validation_steps = validation_generator.n // batch_size\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch = steps_per_epoch,\n",
        "                              epochs=epochs,\n",
        "                              workers=4,\n",
        "                              validation_data=validation_generator,\n",
        "                              validation_steps=validation_steps)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0605 13:55:16.820419 139743018686336 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "1/1 [==============================] - 54s 54s/step - loss: 0.7097 - acc: 0.4300\n",
            "4/4 [==============================] - 136s 34s/step - loss: 0.6911 - acc: 0.5425 - val_loss: 0.7097 - val_acc: 0.4300\n",
            "Epoch 2/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.7083 - acc: 0.4300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6858 - acc: 0.5550 - val_loss: 0.7083 - val_acc: 0.4300\n",
            "Epoch 3/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7064 - acc: 0.4200\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6834 - acc: 0.5800 - val_loss: 0.7064 - val_acc: 0.4200\n",
            "Epoch 4/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7051 - acc: 0.4300\n",
            "4/4 [==============================] - 22s 6s/step - loss: 0.6822 - acc: 0.5900 - val_loss: 0.7051 - val_acc: 0.4300\n",
            "Epoch 5/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7040 - acc: 0.4400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6796 - acc: 0.5725 - val_loss: 0.7040 - val_acc: 0.4400\n",
            "Epoch 6/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7030 - acc: 0.4500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6779 - acc: 0.5850 - val_loss: 0.7030 - val_acc: 0.4500\n",
            "Epoch 7/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.7021 - acc: 0.4400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6766 - acc: 0.6125 - val_loss: 0.7021 - val_acc: 0.4400\n",
            "Epoch 8/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.7011 - acc: 0.4600\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6736 - acc: 0.6050 - val_loss: 0.7011 - val_acc: 0.4600\n",
            "Epoch 9/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6997 - acc: 0.4800\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6731 - acc: 0.6150 - val_loss: 0.6997 - val_acc: 0.4800\n",
            "Epoch 10/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6986 - acc: 0.5000\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6721 - acc: 0.6050 - val_loss: 0.6986 - val_acc: 0.5000\n",
            "Epoch 11/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6971 - acc: 0.4700\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6702 - acc: 0.5875 - val_loss: 0.6971 - val_acc: 0.4700\n",
            "Epoch 12/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6959 - acc: 0.4800\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6689 - acc: 0.6350 - val_loss: 0.6959 - val_acc: 0.4800\n",
            "Epoch 13/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6948 - acc: 0.4800\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6683 - acc: 0.6100 - val_loss: 0.6948 - val_acc: 0.4800\n",
            "Epoch 14/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6933 - acc: 0.5300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6664 - acc: 0.6400 - val_loss: 0.6933 - val_acc: 0.5300\n",
            "Epoch 15/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6921 - acc: 0.4700\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6639 - acc: 0.6275 - val_loss: 0.6921 - val_acc: 0.4700\n",
            "Epoch 16/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6909 - acc: 0.4900\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6650 - acc: 0.6275 - val_loss: 0.6909 - val_acc: 0.4900\n",
            "Epoch 17/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6896 - acc: 0.5200\n",
            "4/4 [==============================] - 24s 6s/step - loss: 0.6630 - acc: 0.6350 - val_loss: 0.6896 - val_acc: 0.5200\n",
            "Epoch 18/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6885 - acc: 0.5400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6607 - acc: 0.6525 - val_loss: 0.6885 - val_acc: 0.5400\n",
            "Epoch 19/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6874 - acc: 0.5500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6609 - acc: 0.6825 - val_loss: 0.6874 - val_acc: 0.5500\n",
            "Epoch 20/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6867 - acc: 0.5800\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6599 - acc: 0.6625 - val_loss: 0.6867 - val_acc: 0.5800\n",
            "Epoch 21/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6854 - acc: 0.5700\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6576 - acc: 0.6650 - val_loss: 0.6854 - val_acc: 0.5700\n",
            "Epoch 22/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6844 - acc: 0.5700\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6572 - acc: 0.6425 - val_loss: 0.6844 - val_acc: 0.5700\n",
            "Epoch 23/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6839 - acc: 0.6200\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6562 - acc: 0.6625 - val_loss: 0.6839 - val_acc: 0.6200\n",
            "Epoch 24/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6828 - acc: 0.6000\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6556 - acc: 0.6475 - val_loss: 0.6828 - val_acc: 0.6000\n",
            "Epoch 25/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6818 - acc: 0.6100\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6539 - acc: 0.6600 - val_loss: 0.6818 - val_acc: 0.6100\n",
            "Epoch 26/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6810 - acc: 0.6100\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6537 - acc: 0.6675 - val_loss: 0.6810 - val_acc: 0.6100\n",
            "Epoch 27/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6806 - acc: 0.6200\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6530 - acc: 0.6700 - val_loss: 0.6806 - val_acc: 0.6200\n",
            "Epoch 28/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6797 - acc: 0.6000\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6493 - acc: 0.6875 - val_loss: 0.6797 - val_acc: 0.6000\n",
            "Epoch 29/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6787 - acc: 0.6100\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6500 - acc: 0.6950 - val_loss: 0.6787 - val_acc: 0.6100\n",
            "Epoch 30/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6780 - acc: 0.6100\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6472 - acc: 0.6750 - val_loss: 0.6780 - val_acc: 0.6100\n",
            "Epoch 31/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6774 - acc: 0.6200\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6486 - acc: 0.6825 - val_loss: 0.6774 - val_acc: 0.6200\n",
            "Epoch 32/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6769 - acc: 0.6100\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6442 - acc: 0.6875 - val_loss: 0.6769 - val_acc: 0.6100\n",
            "Epoch 33/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6767 - acc: 0.6000\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6452 - acc: 0.6850 - val_loss: 0.6767 - val_acc: 0.6000\n",
            "Epoch 34/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6760 - acc: 0.6000\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6457 - acc: 0.6875 - val_loss: 0.6760 - val_acc: 0.6000\n",
            "Epoch 35/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6754 - acc: 0.6000\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6429 - acc: 0.6825 - val_loss: 0.6754 - val_acc: 0.6000\n",
            "Epoch 36/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6746 - acc: 0.6000\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6417 - acc: 0.6875 - val_loss: 0.6746 - val_acc: 0.6000\n",
            "Epoch 37/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6742 - acc: 0.6000\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6387 - acc: 0.6925 - val_loss: 0.6742 - val_acc: 0.6000\n",
            "Epoch 38/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6733 - acc: 0.6100\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6388 - acc: 0.6900 - val_loss: 0.6733 - val_acc: 0.6100\n",
            "Epoch 39/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6728 - acc: 0.6100\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6380 - acc: 0.6850 - val_loss: 0.6728 - val_acc: 0.6100\n",
            "Epoch 40/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6717 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6360 - acc: 0.6900 - val_loss: 0.6717 - val_acc: 0.6300\n",
            "Epoch 41/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6718 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6342 - acc: 0.7025 - val_loss: 0.6718 - val_acc: 0.6300\n",
            "Epoch 42/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6708 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6347 - acc: 0.6925 - val_loss: 0.6708 - val_acc: 0.6300\n",
            "Epoch 43/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6697 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6323 - acc: 0.6850 - val_loss: 0.6697 - val_acc: 0.6400\n",
            "Epoch 44/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6692 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6319 - acc: 0.6975 - val_loss: 0.6692 - val_acc: 0.6400\n",
            "Epoch 45/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6688 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6295 - acc: 0.6875 - val_loss: 0.6688 - val_acc: 0.6400\n",
            "Epoch 46/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6682 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6300 - acc: 0.6950 - val_loss: 0.6682 - val_acc: 0.6400\n",
            "Epoch 47/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6677 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6289 - acc: 0.6975 - val_loss: 0.6677 - val_acc: 0.6400\n",
            "Epoch 48/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6672 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6274 - acc: 0.6950 - val_loss: 0.6672 - val_acc: 0.6400\n",
            "Epoch 49/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6672 - acc: 0.6700\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6271 - acc: 0.7100 - val_loss: 0.6672 - val_acc: 0.6700\n",
            "Epoch 50/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6663 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6256 - acc: 0.6925 - val_loss: 0.6663 - val_acc: 0.6400\n",
            "Epoch 51/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6665 - acc: 0.6700\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6242 - acc: 0.7025 - val_loss: 0.6665 - val_acc: 0.6700\n",
            "Epoch 52/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6652 - acc: 0.6600\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6255 - acc: 0.7025 - val_loss: 0.6652 - val_acc: 0.6600\n",
            "Epoch 53/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6650 - acc: 0.6600\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6225 - acc: 0.6925 - val_loss: 0.6650 - val_acc: 0.6600\n",
            "Epoch 54/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6642 - acc: 0.6600\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6205 - acc: 0.7125 - val_loss: 0.6642 - val_acc: 0.6600\n",
            "Epoch 55/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6637 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6202 - acc: 0.7000 - val_loss: 0.6637 - val_acc: 0.6500\n",
            "Epoch 56/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6640 - acc: 0.6700\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6197 - acc: 0.6975 - val_loss: 0.6640 - val_acc: 0.6700\n",
            "Epoch 57/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6627 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6223 - acc: 0.7000 - val_loss: 0.6627 - val_acc: 0.6400\n",
            "Epoch 58/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6624 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6184 - acc: 0.7250 - val_loss: 0.6624 - val_acc: 0.6300\n",
            "Epoch 59/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6623 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6159 - acc: 0.7075 - val_loss: 0.6623 - val_acc: 0.6500\n",
            "Epoch 60/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6620 - acc: 0.6600\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6139 - acc: 0.7225 - val_loss: 0.6620 - val_acc: 0.6600\n",
            "Epoch 61/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6616 - acc: 0.6600\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6143 - acc: 0.7125 - val_loss: 0.6616 - val_acc: 0.6600\n",
            "Epoch 62/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6612 - acc: 0.6700\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6142 - acc: 0.7200 - val_loss: 0.6612 - val_acc: 0.6700\n",
            "Epoch 63/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6614 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6147 - acc: 0.7075 - val_loss: 0.6614 - val_acc: 0.6400\n",
            "Epoch 64/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6606 - acc: 0.6600\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6124 - acc: 0.7150 - val_loss: 0.6606 - val_acc: 0.6600\n",
            "Epoch 65/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6603 - acc: 0.6600\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6120 - acc: 0.7200 - val_loss: 0.6603 - val_acc: 0.6600\n",
            "Epoch 66/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6604 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6092 - acc: 0.7175 - val_loss: 0.6604 - val_acc: 0.6400\n",
            "Epoch 67/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6598 - acc: 0.6600\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6092 - acc: 0.7150 - val_loss: 0.6598 - val_acc: 0.6600\n",
            "Epoch 68/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6596 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6086 - acc: 0.7400 - val_loss: 0.6596 - val_acc: 0.6500\n",
            "Epoch 69/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6594 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6064 - acc: 0.7175 - val_loss: 0.6594 - val_acc: 0.6500\n",
            "Epoch 70/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6596 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6050 - acc: 0.7225 - val_loss: 0.6596 - val_acc: 0.6500\n",
            "Epoch 71/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6593 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6121 - acc: 0.7150 - val_loss: 0.6593 - val_acc: 0.6400\n",
            "Epoch 72/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6592 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6049 - acc: 0.7175 - val_loss: 0.6592 - val_acc: 0.6500\n",
            "Epoch 73/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6595 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6057 - acc: 0.7200 - val_loss: 0.6595 - val_acc: 0.6300\n",
            "Epoch 74/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6591 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6032 - acc: 0.7225 - val_loss: 0.6591 - val_acc: 0.6500\n",
            "Epoch 75/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6594 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6055 - acc: 0.7075 - val_loss: 0.6594 - val_acc: 0.6500\n",
            "Epoch 76/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6586 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6026 - acc: 0.7375 - val_loss: 0.6586 - val_acc: 0.6400\n",
            "Epoch 77/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6585 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6038 - acc: 0.7225 - val_loss: 0.6585 - val_acc: 0.6500\n",
            "Epoch 78/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6586 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6036 - acc: 0.7325 - val_loss: 0.6586 - val_acc: 0.6500\n",
            "Epoch 79/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6583 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6010 - acc: 0.7250 - val_loss: 0.6583 - val_acc: 0.6400\n",
            "Epoch 80/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6583 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5977 - acc: 0.7350 - val_loss: 0.6583 - val_acc: 0.6500\n",
            "Epoch 81/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6579 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6010 - acc: 0.7425 - val_loss: 0.6579 - val_acc: 0.6500\n",
            "Epoch 82/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6578 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5964 - acc: 0.7275 - val_loss: 0.6578 - val_acc: 0.6400\n",
            "Epoch 83/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6575 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6018 - acc: 0.7075 - val_loss: 0.6575 - val_acc: 0.6400\n",
            "Epoch 84/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6574 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5953 - acc: 0.7250 - val_loss: 0.6574 - val_acc: 0.6400\n",
            "Epoch 85/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6574 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5950 - acc: 0.7225 - val_loss: 0.6574 - val_acc: 0.6500\n",
            "Epoch 86/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6571 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5954 - acc: 0.7175 - val_loss: 0.6571 - val_acc: 0.6400\n",
            "Epoch 87/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6570 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.6009 - acc: 0.7050 - val_loss: 0.6570 - val_acc: 0.6400\n",
            "Epoch 88/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6570 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5960 - acc: 0.7150 - val_loss: 0.6570 - val_acc: 0.6400\n",
            "Epoch 89/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6569 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5962 - acc: 0.7325 - val_loss: 0.6569 - val_acc: 0.6400\n",
            "Epoch 90/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6568 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5930 - acc: 0.7375 - val_loss: 0.6568 - val_acc: 0.6400\n",
            "Epoch 91/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6567 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5937 - acc: 0.7325 - val_loss: 0.6567 - val_acc: 0.6400\n",
            "Epoch 92/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6565 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5909 - acc: 0.7200 - val_loss: 0.6565 - val_acc: 0.6400\n",
            "Epoch 93/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6564 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5951 - acc: 0.7225 - val_loss: 0.6564 - val_acc: 0.6400\n",
            "Epoch 94/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6563 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5938 - acc: 0.7175 - val_loss: 0.6563 - val_acc: 0.6400\n",
            "Epoch 95/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6562 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5890 - acc: 0.7375 - val_loss: 0.6562 - val_acc: 0.6400\n",
            "Epoch 96/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6560 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5909 - acc: 0.7125 - val_loss: 0.6560 - val_acc: 0.6400\n",
            "Epoch 97/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6559 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5903 - acc: 0.7250 - val_loss: 0.6559 - val_acc: 0.6400\n",
            "Epoch 98/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6556 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5899 - acc: 0.7300 - val_loss: 0.6556 - val_acc: 0.6400\n",
            "Epoch 99/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6554 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5868 - acc: 0.7300 - val_loss: 0.6554 - val_acc: 0.6500\n",
            "Epoch 100/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6553 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5847 - acc: 0.7325 - val_loss: 0.6553 - val_acc: 0.6400\n",
            "Epoch 101/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6551 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5880 - acc: 0.7300 - val_loss: 0.6551 - val_acc: 0.6400\n",
            "Epoch 102/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6551 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5857 - acc: 0.7275 - val_loss: 0.6551 - val_acc: 0.6300\n",
            "Epoch 103/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6549 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5829 - acc: 0.7300 - val_loss: 0.6549 - val_acc: 0.6400\n",
            "Epoch 104/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6546 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5850 - acc: 0.7200 - val_loss: 0.6546 - val_acc: 0.6400\n",
            "Epoch 105/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6549 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5835 - acc: 0.7450 - val_loss: 0.6549 - val_acc: 0.6300\n",
            "Epoch 106/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6545 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5824 - acc: 0.7250 - val_loss: 0.6545 - val_acc: 0.6400\n",
            "Epoch 107/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6545 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5800 - acc: 0.7475 - val_loss: 0.6545 - val_acc: 0.6400\n",
            "Epoch 108/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6540 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5803 - acc: 0.7325 - val_loss: 0.6540 - val_acc: 0.6400\n",
            "Epoch 109/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6538 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5813 - acc: 0.7275 - val_loss: 0.6538 - val_acc: 0.6400\n",
            "Epoch 110/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6537 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5797 - acc: 0.7350 - val_loss: 0.6537 - val_acc: 0.6400\n",
            "Epoch 111/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6540 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5789 - acc: 0.7375 - val_loss: 0.6540 - val_acc: 0.6400\n",
            "Epoch 112/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6538 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5792 - acc: 0.7250 - val_loss: 0.6538 - val_acc: 0.6400\n",
            "Epoch 113/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6536 - acc: 0.6300\n",
            "4/4 [==============================] - 24s 6s/step - loss: 0.5762 - acc: 0.7375 - val_loss: 0.6536 - val_acc: 0.6300\n",
            "Epoch 114/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6533 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5760 - acc: 0.7350 - val_loss: 0.6533 - val_acc: 0.6300\n",
            "Epoch 115/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6531 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5765 - acc: 0.7350 - val_loss: 0.6531 - val_acc: 0.6400\n",
            "Epoch 116/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6533 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5761 - acc: 0.7350 - val_loss: 0.6533 - val_acc: 0.6400\n",
            "Epoch 117/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6530 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5738 - acc: 0.7400 - val_loss: 0.6530 - val_acc: 0.6300\n",
            "Epoch 118/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6528 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5733 - acc: 0.7550 - val_loss: 0.6528 - val_acc: 0.6300\n",
            "Epoch 119/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6527 - acc: 0.6300\n",
            "4/4 [==============================] - 24s 6s/step - loss: 0.5740 - acc: 0.7475 - val_loss: 0.6527 - val_acc: 0.6300\n",
            "Epoch 120/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6526 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5807 - acc: 0.7125 - val_loss: 0.6526 - val_acc: 0.6300\n",
            "Epoch 121/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6524 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5723 - acc: 0.7375 - val_loss: 0.6524 - val_acc: 0.6500\n",
            "Epoch 122/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6523 - acc: 0.6300\n",
            "4/4 [==============================] - 24s 6s/step - loss: 0.5727 - acc: 0.7400 - val_loss: 0.6523 - val_acc: 0.6300\n",
            "Epoch 123/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6521 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5711 - acc: 0.7450 - val_loss: 0.6521 - val_acc: 0.6500\n",
            "Epoch 124/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6520 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5696 - acc: 0.7475 - val_loss: 0.6520 - val_acc: 0.6400\n",
            "Epoch 125/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6519 - acc: 0.6300\n",
            "4/4 [==============================] - 24s 6s/step - loss: 0.5700 - acc: 0.7350 - val_loss: 0.6519 - val_acc: 0.6300\n",
            "Epoch 126/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6517 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5727 - acc: 0.7500 - val_loss: 0.6517 - val_acc: 0.6400\n",
            "Epoch 127/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6515 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5688 - acc: 0.7400 - val_loss: 0.6515 - val_acc: 0.6500\n",
            "Epoch 128/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6514 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5733 - acc: 0.7325 - val_loss: 0.6514 - val_acc: 0.6500\n",
            "Epoch 129/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6513 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5714 - acc: 0.7275 - val_loss: 0.6513 - val_acc: 0.6300\n",
            "Epoch 130/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6511 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5706 - acc: 0.7400 - val_loss: 0.6511 - val_acc: 0.6300\n",
            "Epoch 131/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6510 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5689 - acc: 0.7600 - val_loss: 0.6510 - val_acc: 0.6300\n",
            "Epoch 132/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6512 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5650 - acc: 0.7600 - val_loss: 0.6512 - val_acc: 0.6400\n",
            "Epoch 133/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6509 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5679 - acc: 0.7475 - val_loss: 0.6509 - val_acc: 0.6500\n",
            "Epoch 134/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6507 - acc: 0.6400\n",
            "4/4 [==============================] - 24s 6s/step - loss: 0.5652 - acc: 0.7375 - val_loss: 0.6507 - val_acc: 0.6400\n",
            "Epoch 135/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6503 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5643 - acc: 0.7525 - val_loss: 0.6503 - val_acc: 0.6500\n",
            "Epoch 136/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6502 - acc: 0.6400\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5649 - acc: 0.7450 - val_loss: 0.6502 - val_acc: 0.6400\n",
            "Epoch 137/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6502 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5639 - acc: 0.7425 - val_loss: 0.6502 - val_acc: 0.6300\n",
            "Epoch 138/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6498 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5632 - acc: 0.7350 - val_loss: 0.6498 - val_acc: 0.6500\n",
            "Epoch 139/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6497 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5619 - acc: 0.7550 - val_loss: 0.6497 - val_acc: 0.6300\n",
            "Epoch 140/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6496 - acc: 0.6600\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5625 - acc: 0.7575 - val_loss: 0.6496 - val_acc: 0.6600\n",
            "Epoch 141/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6494 - acc: 0.6600\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5661 - acc: 0.7400 - val_loss: 0.6494 - val_acc: 0.6600\n",
            "Epoch 142/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6493 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5604 - acc: 0.7425 - val_loss: 0.6493 - val_acc: 0.6300\n",
            "Epoch 143/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6491 - acc: 0.6300\n",
            "4/4 [==============================] - 24s 6s/step - loss: 0.5588 - acc: 0.7550 - val_loss: 0.6491 - val_acc: 0.6300\n",
            "Epoch 144/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6490 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5601 - acc: 0.7525 - val_loss: 0.6490 - val_acc: 0.6300\n",
            "Epoch 145/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6488 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5580 - acc: 0.7650 - val_loss: 0.6488 - val_acc: 0.6500\n",
            "Epoch 146/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6486 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5614 - acc: 0.7425 - val_loss: 0.6486 - val_acc: 0.6500\n",
            "Epoch 147/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6486 - acc: 0.6600\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5582 - acc: 0.7625 - val_loss: 0.6486 - val_acc: 0.6600\n",
            "Epoch 148/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6484 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5556 - acc: 0.7500 - val_loss: 0.6484 - val_acc: 0.6500\n",
            "Epoch 149/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6483 - acc: 0.6300\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5567 - acc: 0.7575 - val_loss: 0.6483 - val_acc: 0.6300\n",
            "Epoch 150/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6481 - acc: 0.6500\n",
            "4/4 [==============================] - 23s 6s/step - loss: 0.5579 - acc: 0.7525 - val_loss: 0.6481 - val_acc: 0.6500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgtyZLsvG_ls",
        "colab_type": "text"
      },
      "source": [
        "**Learning curves**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-ketbynHDhE",
        "colab_type": "code",
        "outputId": "6542901a-b72e-4fb8-b927-31a88810f5df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "#get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy - 2')\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.title('Training and Validation Loss - 2')\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFPH/Awdc6dRZCjHDAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POHLQd9PPMLX",
        "colab_type": "code",
        "outputId": "3c728b26-02cb-426c-d0cb-d432ee05d3d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "_, acc = model.evaluate_generator(generator=train_generator)\n",
        "print('Accuracy on training set: {}%'.format(round(acc, 3)*100))\n",
        "\n",
        "_, acc = model.evaluate_generator(generator=validation_generator)\n",
        "print('Accuracy on validation set: {}%'.format(round(acc, 3)*100))\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set: 74.80000257492065%\n",
            "Accuracy on validation set: 64.99999761581421%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeN1ZqTAM42i",
        "colab_type": "text"
      },
      "source": [
        "**Export Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkUfyxqDM3NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keras library import  for Saving and loading model and weights\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwvXWQnGEENs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "model_json = model.to_json()\n",
        "\n",
        "\n",
        "with open(\"drive/Colab Notebooks/model/2_model_num.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"drive/Colab Notebooks/model/2_model_num.h5\")\n",
        "model.save('drive/Colab Notebooks/model/2_model_num.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnyvG-KY1Xt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}