{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Housing 3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caroref/model/blob/master/Housing_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ascUYWYXFlTS",
        "colab_type": "code",
        "outputId": "61ef84b5-4fae-4cdb-94a4-ccd105bb6e5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TensorFlow version is \", tf.__version__)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0605 15:15:31.605102 140409186387840 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version is  1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs-4SeS3Fpc5",
        "colab_type": "code",
        "outputId": "c5dc54a4-7558-4303-cbe5-57d44403ef56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Allow Colab to access drive\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "#linking Colab to use drive as if local\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDNLdWVxP2jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main directories\n",
        "trn_dir = 'drive/Colab/Final/Test3/Train/'\n",
        "tst_dir = 'drive/Colab/Final/Test3/Test/'\n",
        "\n",
        "#subdirectories\n",
        "trn_like = os.path.join(trn_dir, 'Like')\n",
        "tst_like = os.path.join(tst_dir, 'Like')\n",
        "\n",
        "trn_disl = os.path.join(trn_dir, 'Dislike')\n",
        "tst_disl = os.path.join(tst_dir, 'Dislike')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo3GVjj1FsEn",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5BtzOSmbZB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All images will be resized to image_size\n",
        "\n",
        "image_size = 640 \n",
        "batch_size = 100\n",
        "\n",
        "# create Image Data Generator for Image Data Augmentation\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "                rescale=1./255, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTvB7EieGDAN",
        "colab_type": "code",
        "outputId": "4e0392aa-38f3-4e00-cd00-459199a3f4fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Create the train and validation generators and specify the train dataset directory, \n",
        "# image size, batch size, binary classification, and subset (training or validation).\n",
        "\n",
        "\n",
        "# Flow training images in batches using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,  \n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode ='binary',\n",
        "                subset = 'training') \n",
        "\n",
        "# Flow validation images in batches using validation_datagen generator\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,\n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode='binary',\n",
        "                subset = 'validation')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ur5r8uGWQO",
        "colab_type": "text"
      },
      "source": [
        "# Create the base model from the pre-trained convnets\n",
        "\n",
        "Using the output of last layer (bottleneck layer). Keep other layers frozen (these are specific to their training, less useful here?) Bottleneck layer should be more general.\n",
        "\n",
        "Using Inception v3 model with weights trained on ImageNet.\n",
        "The argument, include_top=False, means that we have a network that isn't including classification layers at the top. This is good for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCLmagoGbc3",
        "colab_type": "code",
        "outputId": "2a3338eb-0eb4-44cc-a1c6-db4a82932d43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "IMG_SHAPE = (image_size, image_size, 3)\n",
        "\n",
        "base_model = keras.applications.inception_v3.InceptionV3(\n",
        "             include_top=False, weights='imagenet', input_shape=IMG_SHAPE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0605 15:17:46.079791 140409186387840 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wXzV2zlGduw",
        "colab_type": "text"
      },
      "source": [
        "# Feature extraction\n",
        "We will freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.\n",
        "\n",
        "It's important to freeze the convolutional based before we compile and train the model. By freezing (or setting layer.trainable = False), we prevent the weights in these layers from being updated during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCk3KhmGGhaH",
        "colab_type": "code",
        "outputId": "46cc3de5-f367-4d60-98a7-44f90ff6a744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11821
        }
      },
      "source": [
        "#freeze base\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "# Look at the base model architecture\n",
        "base_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 640, 640, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 319, 319, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 319, 319, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 319, 319, 32) 0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 317, 317, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 317, 317, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 317, 317, 32) 0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 317, 317, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 317, 317, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 317, 317, 64) 0           batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 158, 158, 64) 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 158, 158, 80) 5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_3 (Batch (None, 158, 158, 80) 240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 158, 158, 80) 0           batch_normalization_v1_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 156, 156, 192 138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 156, 156, 192 576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 156, 156, 192 0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 77, 77, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_8 (Batch (None, 77, 77, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 77, 77, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 77, 77, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_6 (Batch (None, 77, 77, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_9 (Batch (None, 77, 77, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 77, 77, 48)   0           batch_normalization_v1_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 77, 77, 96)   0           batch_normalization_v1_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 77, 77, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 77, 77, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 77, 77, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 77, 77, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_5 (Batch (None, 77, 77, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_7 (Batch (None, 77, 77, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_10 (Batc (None, 77, 77, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_11 (Batc (None, 77, 77, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_10[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 77, 77, 32)   0           batch_normalization_v1_11[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 77, 77, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_15 (Batc (None, 77, 77, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_15[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 77, 77, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 77, 77, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_13 (Batc (None, 77, 77, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_16 (Batc (None, 77, 77, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_13[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_16[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 77, 77, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 77, 77, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 77, 77, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 77, 77, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_12 (Batc (None, 77, 77, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_14 (Batc (None, 77, 77, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_17 (Batc (None, 77, 77, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_18 (Batc (None, 77, 77, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_12[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_14[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_17[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_18[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 77, 77, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_22 (Batc (None, 77, 77, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_22[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 77, 77, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 77, 77, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_20 (Batc (None, 77, 77, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_23 (Batc (None, 77, 77, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_20[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_23[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 77, 77, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 77, 77, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 77, 77, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 77, 77, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_19 (Batc (None, 77, 77, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_21 (Batc (None, 77, 77, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_24 (Batc (None, 77, 77, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_25 (Batc (None, 77, 77, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_19[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_21[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_24[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_25[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 77, 77, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 77, 77, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_27 (Batc (None, 77, 77, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_27[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 77, 77, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_28 (Batc (None, 77, 77, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_28[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 38, 38, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 38, 38, 96)   82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_26 (Batc (None, 38, 38, 384)  1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_29 (Batc (None, 38, 38, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 38, 38, 384)  0           batch_normalization_v1_26[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 38, 38, 96)   0           batch_normalization_v1_29[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 38, 38, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 38, 38, 768)  0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_34 (Batc (None, 38, 38, 128)  384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_34[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 38, 38, 128)  114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_35 (Batc (None, 38, 38, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_35[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 38, 38, 128)  114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_31 (Batc (None, 38, 38, 128)  384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_36 (Batc (None, 38, 38, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_31[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_36[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 38, 38, 128)  114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 38, 38, 128)  114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_32 (Batc (None, 38, 38, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_37 (Batc (None, 38, 38, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_32[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_37[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 38, 38, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 38, 38, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 38, 38, 192)  172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 38, 38, 192)  172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_30 (Batc (None, 38, 38, 192)  576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_33 (Batc (None, 38, 38, 192)  576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_38 (Batc (None, 38, 38, 192)  576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_39 (Batc (None, 38, 38, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_30[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_33[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_38[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_39[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 38, 38, 768)  0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_44 (Batc (None, 38, 38, 160)  480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_44[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 38, 38, 160)  179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_45 (Batc (None, 38, 38, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_45[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 38, 38, 160)  179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_41 (Batc (None, 38, 38, 160)  480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_46 (Batc (None, 38, 38, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_41[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_46[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 38, 38, 160)  179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 38, 38, 160)  179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_42 (Batc (None, 38, 38, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_47 (Batc (None, 38, 38, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_42[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_47[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 38, 38, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 38, 38, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 38, 38, 192)  215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 38, 38, 192)  215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_40 (Batc (None, 38, 38, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_43 (Batc (None, 38, 38, 192)  576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_48 (Batc (None, 38, 38, 192)  576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_49 (Batc (None, 38, 38, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_40[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_43[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_48[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_49[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 38, 38, 768)  0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_54 (Batc (None, 38, 38, 160)  480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_54[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 38, 38, 160)  179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_55 (Batc (None, 38, 38, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_55[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 38, 38, 160)  179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_51 (Batc (None, 38, 38, 160)  480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_56 (Batc (None, 38, 38, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_51[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_56[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 38, 38, 160)  179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 38, 38, 160)  179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_52 (Batc (None, 38, 38, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_57 (Batc (None, 38, 38, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_52[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_57[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 38, 38, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 38, 38, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 38, 38, 192)  215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 38, 38, 192)  215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_50 (Batc (None, 38, 38, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_53 (Batc (None, 38, 38, 192)  576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_58 (Batc (None, 38, 38, 192)  576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_59 (Batc (None, 38, 38, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_50[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_53[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_58[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_59[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 38, 38, 768)  0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_64 (Batc (None, 38, 38, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_64[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 38, 38, 192)  258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_65 (Batc (None, 38, 38, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_65[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 38, 38, 192)  258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_61 (Batc (None, 38, 38, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_66 (Batc (None, 38, 38, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_61[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_66[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 38, 38, 192)  258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 38, 38, 192)  258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_62 (Batc (None, 38, 38, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_67 (Batc (None, 38, 38, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_62[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_67[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 38, 38, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 38, 38, 192)  258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 38, 38, 192)  258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_60 (Batc (None, 38, 38, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_63 (Batc (None, 38, 38, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_68 (Batc (None, 38, 38, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_69 (Batc (None, 38, 38, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_60[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_63[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_68[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_69[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 38, 38, 768)  0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_72 (Batc (None, 38, 38, 192)  576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_72[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 38, 38, 192)  258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_73 (Batc (None, 38, 38, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_73[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 38, 38, 192)  258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_70 (Batc (None, 38, 38, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_74 (Batc (None, 38, 38, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_70[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_74[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 18, 18, 320)  552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 18, 18, 192)  331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_71 (Batc (None, 18, 18, 320)  960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_75 (Batc (None, 18, 18, 192)  576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_71[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_75[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 18, 18, 768)  0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 18, 18, 1280) 0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 18, 18, 448)  573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_80 (Batc (None, 18, 18, 448)  1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_80[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 18, 18, 384)  491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 18, 18, 384)  1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_77 (Batc (None, 18, 18, 384)  1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_81 (Batc (None, 18, 18, 384)  1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_77[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_81[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 18, 18, 1280) 0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 18, 18, 320)  409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_78 (Batc (None, 18, 18, 384)  1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_79 (Batc (None, 18, 18, 384)  1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_82 (Batc (None, 18, 18, 384)  1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_83 (Batc (None, 18, 18, 384)  1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 18, 18, 192)  245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_76 (Batc (None, 18, 18, 320)  960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_78[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_79[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_82[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_83[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_84 (Batc (None, 18, 18, 192)  576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_76[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 18, 18, 768)  0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 18, 18, 768)  0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_84[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 18, 18, 2048) 0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 18, 18, 448)  917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_89 (Batc (None, 18, 18, 448)  1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_89[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 18, 18, 384)  786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 18, 18, 384)  1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_86 (Batc (None, 18, 18, 384)  1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_90 (Batc (None, 18, 18, 384)  1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_86[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_90[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 18, 18, 2048) 0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 18, 18, 320)  655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_87 (Batc (None, 18, 18, 384)  1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_88 (Batc (None, 18, 18, 384)  1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_91 (Batc (None, 18, 18, 384)  1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_92 (Batc (None, 18, 18, 384)  1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 18, 18, 192)  393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_85 (Batc (None, 18, 18, 320)  960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_87[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_88[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_91[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_92[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_93 (Batc (None, 18, 18, 192)  576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_85[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 18, 18, 768)  0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 18, 18, 768)  0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_93[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 18, 18, 2048) 0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 0\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUc1ad5qGnMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add a classification head\n",
        "#adding a few layers on top of the base model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  keras.layers.GlobalAveragePooling2D(),\n",
        "  keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "#relatively basic layer. no deep neural networks (all happening beforehand). Similar to a logit regression. plain classification\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbhLA3TGr1X",
        "colab_type": "code",
        "outputId": "bcd4fa4b-582a-4499-f876-dde2da5f7e84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "#Compile the model (Must do before training!)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 18, 18, 2048)      21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 21,804,833\n",
            "Trainable params: 2,049\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th655DVEGwop",
        "colab_type": "code",
        "outputId": "5eca3246-8d3f-4678-8672-935c403de131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#These 2K trainable parameters are divided among 2 TensorFlow Variable objects, the weights and biases of the two dense layers:\n",
        "\n",
        "\n",
        "len(model.trainable_variables)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZeMfEwoG3wm",
        "colab_type": "text"
      },
      "source": [
        "**Train the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNgpgcJjG8xl",
        "colab_type": "code",
        "outputId": "f697bf7d-e5f8-4494-e367-5fd0ff1903c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8144
        }
      },
      "source": [
        "epochs = 150\n",
        "steps_per_epoch = train_generator.n // batch_size\n",
        "validation_steps = validation_generator.n // batch_size\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch = steps_per_epoch,\n",
        "                              epochs=epochs,\n",
        "                              workers=4,\n",
        "                              validation_data=validation_generator,\n",
        "                              validation_steps=validation_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0605 15:18:01.454831 140409186387840 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "1/1 [==============================] - 56s 56s/step - loss: 0.6976 - acc: 0.5000\n",
            "4/4 [==============================] - 134s 34s/step - loss: 0.7250 - acc: 0.5025 - val_loss: 0.6976 - val_acc: 0.5000\n",
            "Epoch 2/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6937 - acc: 0.5400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.7079 - acc: 0.4400 - val_loss: 0.6937 - val_acc: 0.5400\n",
            "Epoch 3/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6905 - acc: 0.5000\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.7045 - acc: 0.4750 - val_loss: 0.6905 - val_acc: 0.5000\n",
            "Epoch 4/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6878 - acc: 0.5000\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.7030 - acc: 0.4750 - val_loss: 0.6878 - val_acc: 0.5000\n",
            "Epoch 5/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6867 - acc: 0.5100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.7016 - acc: 0.4675 - val_loss: 0.6867 - val_acc: 0.5100\n",
            "Epoch 6/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6856 - acc: 0.5200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6989 - acc: 0.4900 - val_loss: 0.6856 - val_acc: 0.5200\n",
            "Epoch 7/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6848 - acc: 0.5200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6982 - acc: 0.4875 - val_loss: 0.6848 - val_acc: 0.5200\n",
            "Epoch 8/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6842 - acc: 0.5500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6962 - acc: 0.4775 - val_loss: 0.6842 - val_acc: 0.5500\n",
            "Epoch 9/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6820 - acc: 0.5500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6943 - acc: 0.5250 - val_loss: 0.6820 - val_acc: 0.5500\n",
            "Epoch 10/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6820 - acc: 0.5800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6944 - acc: 0.5000 - val_loss: 0.6820 - val_acc: 0.5800\n",
            "Epoch 11/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6813 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6934 - acc: 0.5250 - val_loss: 0.6813 - val_acc: 0.5900\n",
            "Epoch 12/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6803 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6903 - acc: 0.5400 - val_loss: 0.6803 - val_acc: 0.6000\n",
            "Epoch 13/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6802 - acc: 0.5700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6893 - acc: 0.5275 - val_loss: 0.6802 - val_acc: 0.5700\n",
            "Epoch 14/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6797 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6884 - acc: 0.5450 - val_loss: 0.6797 - val_acc: 0.6200\n",
            "Epoch 15/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6797 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6857 - acc: 0.5550 - val_loss: 0.6797 - val_acc: 0.6200\n",
            "Epoch 16/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6796 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6871 - acc: 0.5525 - val_loss: 0.6796 - val_acc: 0.6400\n",
            "Epoch 17/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6792 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6843 - acc: 0.5525 - val_loss: 0.6792 - val_acc: 0.6300\n",
            "Epoch 18/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6788 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6833 - acc: 0.5850 - val_loss: 0.6788 - val_acc: 0.6500\n",
            "Epoch 19/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6783 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6812 - acc: 0.5875 - val_loss: 0.6783 - val_acc: 0.6400\n",
            "Epoch 20/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6779 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6799 - acc: 0.5775 - val_loss: 0.6779 - val_acc: 0.6500\n",
            "Epoch 21/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6776 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6777 - acc: 0.5975 - val_loss: 0.6776 - val_acc: 0.6500\n",
            "Epoch 22/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6774 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6778 - acc: 0.6100 - val_loss: 0.6774 - val_acc: 0.6400\n",
            "Epoch 23/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6771 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6769 - acc: 0.6100 - val_loss: 0.6771 - val_acc: 0.6200\n",
            "Epoch 24/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6768 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6764 - acc: 0.6050 - val_loss: 0.6768 - val_acc: 0.6400\n",
            "Epoch 25/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6765 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6747 - acc: 0.6150 - val_loss: 0.6765 - val_acc: 0.6300\n",
            "Epoch 26/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6759 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6714 - acc: 0.6225 - val_loss: 0.6759 - val_acc: 0.6400\n",
            "Epoch 27/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6754 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6705 - acc: 0.6350 - val_loss: 0.6754 - val_acc: 0.6300\n",
            "Epoch 28/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6747 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6697 - acc: 0.6475 - val_loss: 0.6747 - val_acc: 0.6300\n",
            "Epoch 29/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6739 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6671 - acc: 0.6350 - val_loss: 0.6739 - val_acc: 0.6200\n",
            "Epoch 30/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6732 - acc: 0.6000\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6662 - acc: 0.6375 - val_loss: 0.6732 - val_acc: 0.6000\n",
            "Epoch 31/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6727 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6673 - acc: 0.6300 - val_loss: 0.6727 - val_acc: 0.6100\n",
            "Epoch 32/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6717 - acc: 0.5800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6631 - acc: 0.6550 - val_loss: 0.6717 - val_acc: 0.5800\n",
            "Epoch 33/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6709 - acc: 0.5800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6631 - acc: 0.6425 - val_loss: 0.6709 - val_acc: 0.5800\n",
            "Epoch 34/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6701 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6614 - acc: 0.6525 - val_loss: 0.6701 - val_acc: 0.6000\n",
            "Epoch 35/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6692 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6616 - acc: 0.6475 - val_loss: 0.6692 - val_acc: 0.5900\n",
            "Epoch 36/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6687 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6600 - acc: 0.6450 - val_loss: 0.6687 - val_acc: 0.6300\n",
            "Epoch 37/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6674 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6600 - acc: 0.6725 - val_loss: 0.6674 - val_acc: 0.6000\n",
            "Epoch 38/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6666 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6580 - acc: 0.6600 - val_loss: 0.6666 - val_acc: 0.6000\n",
            "Epoch 39/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6659 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6563 - acc: 0.6625 - val_loss: 0.6659 - val_acc: 0.6000\n",
            "Epoch 40/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6652 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6550 - acc: 0.6725 - val_loss: 0.6652 - val_acc: 0.6000\n",
            "Epoch 41/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6643 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6549 - acc: 0.6550 - val_loss: 0.6643 - val_acc: 0.6100\n",
            "Epoch 42/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6636 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6542 - acc: 0.6750 - val_loss: 0.6636 - val_acc: 0.6100\n",
            "Epoch 43/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6630 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6523 - acc: 0.6725 - val_loss: 0.6630 - val_acc: 0.6100\n",
            "Epoch 44/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6625 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6531 - acc: 0.6625 - val_loss: 0.6625 - val_acc: 0.6100\n",
            "Epoch 45/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6619 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6512 - acc: 0.7125 - val_loss: 0.6619 - val_acc: 0.6200\n",
            "Epoch 46/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6612 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6475 - acc: 0.7025 - val_loss: 0.6612 - val_acc: 0.6100\n",
            "Epoch 47/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6606 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6487 - acc: 0.6850 - val_loss: 0.6606 - val_acc: 0.6200\n",
            "Epoch 48/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6599 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6471 - acc: 0.6925 - val_loss: 0.6599 - val_acc: 0.6200\n",
            "Epoch 49/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6593 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6494 - acc: 0.6850 - val_loss: 0.6593 - val_acc: 0.6200\n",
            "Epoch 50/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6590 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6467 - acc: 0.6825 - val_loss: 0.6590 - val_acc: 0.6600\n",
            "Epoch 51/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6582 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6456 - acc: 0.6825 - val_loss: 0.6582 - val_acc: 0.6500\n",
            "Epoch 52/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6575 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6436 - acc: 0.7000 - val_loss: 0.6575 - val_acc: 0.6300\n",
            "Epoch 53/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6572 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6440 - acc: 0.6700 - val_loss: 0.6572 - val_acc: 0.6600\n",
            "Epoch 54/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6563 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6419 - acc: 0.7025 - val_loss: 0.6563 - val_acc: 0.6600\n",
            "Epoch 55/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6556 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6397 - acc: 0.7150 - val_loss: 0.6556 - val_acc: 0.6600\n",
            "Epoch 56/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6548 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6392 - acc: 0.7100 - val_loss: 0.6548 - val_acc: 0.6700\n",
            "Epoch 57/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6541 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6375 - acc: 0.7225 - val_loss: 0.6541 - val_acc: 0.6800\n",
            "Epoch 58/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6534 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6385 - acc: 0.6950 - val_loss: 0.6534 - val_acc: 0.6500\n",
            "Epoch 59/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6529 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6372 - acc: 0.6950 - val_loss: 0.6529 - val_acc: 0.6600\n",
            "Epoch 60/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6529 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6344 - acc: 0.7000 - val_loss: 0.6529 - val_acc: 0.6500\n",
            "Epoch 61/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6520 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6335 - acc: 0.7125 - val_loss: 0.6520 - val_acc: 0.6600\n",
            "Epoch 62/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6512 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6357 - acc: 0.7025 - val_loss: 0.6512 - val_acc: 0.6500\n",
            "Epoch 63/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6505 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6322 - acc: 0.7250 - val_loss: 0.6505 - val_acc: 0.6600\n",
            "Epoch 64/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6498 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6345 - acc: 0.6900 - val_loss: 0.6498 - val_acc: 0.6600\n",
            "Epoch 65/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6493 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6315 - acc: 0.7175 - val_loss: 0.6493 - val_acc: 0.6500\n",
            "Epoch 66/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6486 - acc: 0.6600\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6291 - acc: 0.7100 - val_loss: 0.6486 - val_acc: 0.6600\n",
            "Epoch 67/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6480 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6273 - acc: 0.7150 - val_loss: 0.6480 - val_acc: 0.6500\n",
            "Epoch 68/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6474 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6293 - acc: 0.7050 - val_loss: 0.6474 - val_acc: 0.6500\n",
            "Epoch 69/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6464 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6257 - acc: 0.7175 - val_loss: 0.6464 - val_acc: 0.6600\n",
            "Epoch 70/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6458 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6256 - acc: 0.7100 - val_loss: 0.6458 - val_acc: 0.6600\n",
            "Epoch 71/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6456 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6249 - acc: 0.7050 - val_loss: 0.6456 - val_acc: 0.6500\n",
            "Epoch 72/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6453 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6242 - acc: 0.7075 - val_loss: 0.6453 - val_acc: 0.6500\n",
            "Epoch 73/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6440 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6233 - acc: 0.7125 - val_loss: 0.6440 - val_acc: 0.6600\n",
            "Epoch 74/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6436 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6243 - acc: 0.7100 - val_loss: 0.6436 - val_acc: 0.6600\n",
            "Epoch 75/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6434 - acc: 0.6600\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6244 - acc: 0.6850 - val_loss: 0.6434 - val_acc: 0.6600\n",
            "Epoch 76/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6425 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6226 - acc: 0.7075 - val_loss: 0.6425 - val_acc: 0.6500\n",
            "Epoch 77/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6420 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6201 - acc: 0.7275 - val_loss: 0.6420 - val_acc: 0.6600\n",
            "Epoch 78/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6416 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6204 - acc: 0.7025 - val_loss: 0.6416 - val_acc: 0.6600\n",
            "Epoch 79/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6412 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6225 - acc: 0.7100 - val_loss: 0.6412 - val_acc: 0.6600\n",
            "Epoch 80/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6406 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6189 - acc: 0.7225 - val_loss: 0.6406 - val_acc: 0.6700\n",
            "Epoch 81/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6405 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6193 - acc: 0.7075 - val_loss: 0.6405 - val_acc: 0.6600\n",
            "Epoch 82/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6400 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6176 - acc: 0.7225 - val_loss: 0.6400 - val_acc: 0.6600\n",
            "Epoch 83/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6396 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6171 - acc: 0.7325 - val_loss: 0.6396 - val_acc: 0.6600\n",
            "Epoch 84/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6395 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6139 - acc: 0.7300 - val_loss: 0.6395 - val_acc: 0.6600\n",
            "Epoch 85/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6387 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6141 - acc: 0.7175 - val_loss: 0.6387 - val_acc: 0.6600\n",
            "Epoch 86/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6388 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6132 - acc: 0.7225 - val_loss: 0.6388 - val_acc: 0.6600\n",
            "Epoch 87/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6378 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6128 - acc: 0.7200 - val_loss: 0.6378 - val_acc: 0.6600\n",
            "Epoch 88/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6374 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6110 - acc: 0.7175 - val_loss: 0.6374 - val_acc: 0.6600\n",
            "Epoch 89/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6374 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6112 - acc: 0.7200 - val_loss: 0.6374 - val_acc: 0.6600\n",
            "Epoch 90/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6373 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6100 - acc: 0.7275 - val_loss: 0.6373 - val_acc: 0.6700\n",
            "Epoch 91/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6360 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6103 - acc: 0.7175 - val_loss: 0.6360 - val_acc: 0.6700\n",
            "Epoch 92/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6359 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6113 - acc: 0.7350 - val_loss: 0.6359 - val_acc: 0.6600\n",
            "Epoch 93/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6357 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6095 - acc: 0.7325 - val_loss: 0.6357 - val_acc: 0.6600\n",
            "Epoch 94/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6350 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6085 - acc: 0.7100 - val_loss: 0.6350 - val_acc: 0.6700\n",
            "Epoch 95/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6347 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6066 - acc: 0.7425 - val_loss: 0.6347 - val_acc: 0.6700\n",
            "Epoch 96/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6346 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6055 - acc: 0.7225 - val_loss: 0.6346 - val_acc: 0.6600\n",
            "Epoch 97/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6342 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6044 - acc: 0.7325 - val_loss: 0.6342 - val_acc: 0.6600\n",
            "Epoch 98/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6339 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6044 - acc: 0.7250 - val_loss: 0.6339 - val_acc: 0.6600\n",
            "Epoch 99/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6337 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6029 - acc: 0.7425 - val_loss: 0.6337 - val_acc: 0.6700\n",
            "Epoch 100/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6335 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6064 - acc: 0.7150 - val_loss: 0.6335 - val_acc: 0.6700\n",
            "Epoch 101/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6329 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6029 - acc: 0.7300 - val_loss: 0.6329 - val_acc: 0.6700\n",
            "Epoch 102/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6328 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6045 - acc: 0.7425 - val_loss: 0.6328 - val_acc: 0.6700\n",
            "Epoch 103/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6324 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5994 - acc: 0.7300 - val_loss: 0.6324 - val_acc: 0.6700\n",
            "Epoch 104/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6325 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6021 - acc: 0.7300 - val_loss: 0.6325 - val_acc: 0.6700\n",
            "Epoch 105/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6321 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6019 - acc: 0.7500 - val_loss: 0.6321 - val_acc: 0.6700\n",
            "Epoch 106/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6315 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6022 - acc: 0.7250 - val_loss: 0.6315 - val_acc: 0.6800\n",
            "Epoch 107/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6317 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5978 - acc: 0.7325 - val_loss: 0.6317 - val_acc: 0.6700\n",
            "Epoch 108/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6312 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5995 - acc: 0.7250 - val_loss: 0.6312 - val_acc: 0.6800\n",
            "Epoch 109/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6309 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5988 - acc: 0.7300 - val_loss: 0.6309 - val_acc: 0.6800\n",
            "Epoch 110/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6306 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6006 - acc: 0.7150 - val_loss: 0.6306 - val_acc: 0.6800\n",
            "Epoch 111/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6306 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5967 - acc: 0.7325 - val_loss: 0.6306 - val_acc: 0.6800\n",
            "Epoch 112/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6302 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5968 - acc: 0.7400 - val_loss: 0.6302 - val_acc: 0.6800\n",
            "Epoch 113/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6302 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5961 - acc: 0.7375 - val_loss: 0.6302 - val_acc: 0.6800\n",
            "Epoch 114/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6298 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5947 - acc: 0.7450 - val_loss: 0.6298 - val_acc: 0.6800\n",
            "Epoch 115/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6297 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5947 - acc: 0.7375 - val_loss: 0.6297 - val_acc: 0.6800\n",
            "Epoch 116/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6294 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5937 - acc: 0.7400 - val_loss: 0.6294 - val_acc: 0.6800\n",
            "Epoch 117/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6295 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5933 - acc: 0.7325 - val_loss: 0.6295 - val_acc: 0.6700\n",
            "Epoch 118/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6289 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5926 - acc: 0.7400 - val_loss: 0.6289 - val_acc: 0.6800\n",
            "Epoch 119/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6288 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5893 - acc: 0.7375 - val_loss: 0.6288 - val_acc: 0.6800\n",
            "Epoch 120/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6283 - acc: 0.6900\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5913 - acc: 0.7350 - val_loss: 0.6283 - val_acc: 0.6900\n",
            "Epoch 121/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6282 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5917 - acc: 0.7325 - val_loss: 0.6282 - val_acc: 0.6800\n",
            "Epoch 122/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6278 - acc: 0.6900\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5932 - acc: 0.7325 - val_loss: 0.6278 - val_acc: 0.6900\n",
            "Epoch 123/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6277 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5951 - acc: 0.7300 - val_loss: 0.6277 - val_acc: 0.6800\n",
            "Epoch 124/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6278 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5913 - acc: 0.7300 - val_loss: 0.6278 - val_acc: 0.6700\n",
            "Epoch 125/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6274 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5887 - acc: 0.7475 - val_loss: 0.6274 - val_acc: 0.6800\n",
            "Epoch 126/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6275 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5862 - acc: 0.7350 - val_loss: 0.6275 - val_acc: 0.6700\n",
            "Epoch 127/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6279 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5854 - acc: 0.7450 - val_loss: 0.6279 - val_acc: 0.6800\n",
            "Epoch 128/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6277 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5899 - acc: 0.7400 - val_loss: 0.6277 - val_acc: 0.6800\n",
            "Epoch 129/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6268 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5902 - acc: 0.7300 - val_loss: 0.6268 - val_acc: 0.6700\n",
            "Epoch 130/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6265 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5862 - acc: 0.7400 - val_loss: 0.6265 - val_acc: 0.6800\n",
            "Epoch 131/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6266 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5841 - acc: 0.7400 - val_loss: 0.6266 - val_acc: 0.6700\n",
            "Epoch 132/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6259 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5924 - acc: 0.7275 - val_loss: 0.6259 - val_acc: 0.6800\n",
            "Epoch 133/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6257 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5837 - acc: 0.7400 - val_loss: 0.6257 - val_acc: 0.6800\n",
            "Epoch 134/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6258 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5886 - acc: 0.7275 - val_loss: 0.6258 - val_acc: 0.6800\n",
            "Epoch 135/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6260 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5841 - acc: 0.7475 - val_loss: 0.6260 - val_acc: 0.6700\n",
            "Epoch 136/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6253 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5839 - acc: 0.7450 - val_loss: 0.6253 - val_acc: 0.6800\n",
            "Epoch 137/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6252 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5872 - acc: 0.7475 - val_loss: 0.6252 - val_acc: 0.6800\n",
            "Epoch 138/150\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6252 - acc: 0.6700\n",
            "4/4 [==============================] - 16s 4s/step - loss: 0.5801 - acc: 0.7450 - val_loss: 0.6252 - val_acc: 0.6700\n",
            "Epoch 139/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6249 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5825 - acc: 0.7275 - val_loss: 0.6249 - val_acc: 0.6800\n",
            "Epoch 140/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6247 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5806 - acc: 0.7450 - val_loss: 0.6247 - val_acc: 0.6800\n",
            "Epoch 141/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6243 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5792 - acc: 0.7325 - val_loss: 0.6243 - val_acc: 0.6800\n",
            "Epoch 142/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6240 - acc: 0.6900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5800 - acc: 0.7400 - val_loss: 0.6240 - val_acc: 0.6900\n",
            "Epoch 143/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6239 - acc: 0.6900\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5820 - acc: 0.7400 - val_loss: 0.6239 - val_acc: 0.6900\n",
            "Epoch 144/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6241 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5769 - acc: 0.7425 - val_loss: 0.6241 - val_acc: 0.6700\n",
            "Epoch 145/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6242 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5784 - acc: 0.7525 - val_loss: 0.6242 - val_acc: 0.6700\n",
            "Epoch 146/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6235 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5771 - acc: 0.7425 - val_loss: 0.6235 - val_acc: 0.6800\n",
            "Epoch 147/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6231 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5765 - acc: 0.7375 - val_loss: 0.6231 - val_acc: 0.6800\n",
            "Epoch 148/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6231 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5762 - acc: 0.7300 - val_loss: 0.6231 - val_acc: 0.6800\n",
            "Epoch 149/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6234 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5759 - acc: 0.7425 - val_loss: 0.6234 - val_acc: 0.6700\n",
            "Epoch 150/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6240 - acc: 0.6800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5797 - acc: 0.7450 - val_loss: 0.6240 - val_acc: 0.6800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgtyZLsvG_ls",
        "colab_type": "text"
      },
      "source": [
        "**Learning curves**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-ketbynHDhE",
        "colab_type": "code",
        "outputId": "9e71eb9d-8746-4c4c-9990-b232ce7c01cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "#get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy - 3')\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.title('Training and Validation Loss - 3')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8XWW97/HPLzs785xOpAMptNh5\nohYQQREOFAQqykFQUFTEy3XgOHCtHkUOR68cjwdxQMTjBUWFykHRyiDHw6AiCrQMhdKWllJoOiZp\nMyc72cnv/rFW0p00SXfT7KbZ/b5fr/3aew37Wb+1V5vfWs/zrGeZuyMiIiKjX8ZIByAiIiLDQ0ld\nREQkTSipi4iIpAkldRERkTShpC4iIpImlNRFRETShJK6HNXMLGJmTWY2ZTjXHUlmNs3MUnKvat+y\nzey/zeyDqYjDzL5qZj8a6vdFjkZK6jKqhEm1+9VlZq0J0/0ml8G4e6e7F7j7m8O57pHKzP7HzK7v\nZ/77zGybmUUOpjx3P9vdfzkMcZ1lZlv6lP2v7v6/DrXsfrZ1lZk9MdzlDgcz+7yZvW5mDeHx+A8z\nyxzpuGT0UFKXUSVMqgXuXgC8CVyQMG+/5KI/iPv5GXBFP/OvAH7h7p2HOR7p7bfAAncvAuYBi4H/\nPbIhyWiipC5pxcy+bma/MrN7zKwRuNzMTjGzv5tZnZntMLPvmVk0XD/TzNzMKsPpX4TLHzazRjP7\nm5lNPdh1w+XnmtmrZlZvZt83s7+a2ZUDxJ1MjJ8ws01mttfMvpfw3YiZfcfMas1sM7B0kJ/oN8AE\nM3tbwvfLgfOAu8LpC83shfBq8U0z++ogv/eT3ft0oDjCK+R14W/1mpldFc4vBn4PTEmodRkXHsuf\nJnz/IjNbG/5Gj5nZWxKWVZnZ58zspfD3vsfMsgf5HQban0lm9oCZ7TGzjWb20YRlJ5vZc+HvssvM\n/j2cn2dmd4f7XWdmz5jZmIPdNoC7v+bu9d2bBLqAaUMpS45OSuqSji4C7gaKgV8BceBaYAxwKkGy\n+cQg3/8A8FWgjKA24F8Pdl0zGwfcC1wXbvd1YMkg5SQT43nAicBCgpOVs8L51wBnA/OBtwKXDLQR\nd28G7gM+lDD7UmCNu68Np5uADwIlwAXAtWZ2/iCxdztQHLuAdwNFwMeB75vZvDCJXQC8mVDrsjvx\ni2Y2E/g58GlgLPA/wMruE5/QJcA/AMcR/E791UgcyK8IjlUF8H7gW2b2jnDZ94F/D6+ipxH8jgAf\nAfKASUA5wZV12xC2DYCZXRGekFYDs4EfD7UsOfooqUs6etLdf+/uXe7e6u7PuvvT7h53980EfyTf\nMcj373P3Ve7eAfwSWDCEdc8HXnD334XLvgPUDFRIkjF+093r3X0L8ETCti4BvuPuVe5eC9w0SLwQ\nVMFfknAl+6FwXncsj7n72vD3exFY0U8s/Rk0jvCYbPbAY8CjwGlJlAvBicfKMLaOsOxi4KSEdW5x\n953hth9g8OO2n7CWZQmw3N3b3P054E72nRx0ANPNrNzdG9396YT5Y4BpYb+LVe7edDDbTuTuP3f3\nQmAGcDuw+wBfEemhpC7paGvihJnNMLMHzWynmTUANxL8ER7IzoTPLUDBENatSIzDgycnVQ1USJIx\nJrUt4I1B4gX4E9AAXGBmJxBc+d+TEMspZvaEmVWbWT1wVT+x9GfQOMzsfDN7OqzariO4qk+2mroi\nsTx37yL4PScmrHMwx22gbdSEtRnd3kjYxkeAWcCGsIr9vHD+TwlqDu61oHPbTdZPXw4z+3BC88Lv\nDxSMu28ANgA/OMj9kKOYkrqko763Ud0OvExwJVUEXE/QXplKOwiqYwEwM6N3AurrUGLcAUxOmB70\nlrvwBOMugiv0K4CH3D2xFmEF8GtgsrsXAz9JMpYB4zCzXILq6m8C4929BPjvhHIPdOvbduDYhPIy\nCH7fbUnElaztwBgzy0+YN6V7G+6+wd0vBcYB/wH82sxy3L3d3W9w95nA2wmaf/a7E8Pdf5bQvHBB\nkjFlAscfwj7JUUZJXY4GhUA90By2zQ7Wnj5cHgAWmdkF4VXbtQRtwamI8V7gn8xsYtjp7YtJfOcu\ngnb7j5JQ9Z4Qyx53bzOzkwmqvg81jmwgi6CduDNsoz8zYfkugoRaOEjZF5rZO8N29OuARuDpAdY/\nkAwzy0l8ufvrwCrg/5pZtpktILg6/wX0tHWPCWsJ6glORLrM7F1mNic80WggqI7vGkpQZvZxMxsb\nfp5N8Bs+OsR9lKOQkrocDT4PfJggCdxO0Bkqpdx9F0FHq5uBWoKrreeBWApivI3gD/9LwLPs68A1\nWHybgGcIku2DfRZfA3wz7Kz1ZYKEekhxuHsd8FngfmAPcDHBiU/38pcJage2hD3Ix/WJdy3B73Mb\nwYnBUuDCsH19KE4DWvu8IDhm0wmq8u8DvuzuT4TLzgPWhb/Lt4H3u3s7QbX9bwgS+lqCqvi7hxjX\n6cBaM2sm+H1WEnTEFEmKBTVxIpJKFgzqsh242N3/MtLxiEh60pW6SIqY2VIzKwl7mX+VoFr2mREO\nS0TSmJK6SOq8HdhMUF18DnCRuw9U/S4icshU/S4iIpImdKUuIiKSJpTURURE0sSoe4LVmDFjvLKy\ncqTDEBEROSxWr15d4+6DjXPRY9Ql9crKSlatWjUsZb2wtY6f/GUzN1+ygKxMVVqIiMiRx8wONPRz\nj6M6k9U2xXhgzQ5+9tSWkQ5FRETkkB3VSf3MmeN514xxfPfRjexuGPKTEkVERI4IR3VSB7j+/Fm0\nx7u46Q/rRzoUERGRQ5LSpB6OqLXBzDaZ2fJ+ln/HzF4IX6+Gj2M8rCrH5HPVaVP5zXPbWP3GnsO9\neRERkWGTsqQejnV9K3AuwTOILzOzWYnruPtn3X2Buy8Avk/wUITD7pNnTGNCUQ5X37WaG1au5dkt\ne4h3DukhSyIiIiMmlb3flwCb3H0zgJmtAJYBrwyw/mXA11IYz4DyszP58YdO5NbHN3H3M2/y06e2\nkJlhTCnLo3JMPpXl+Uwdm88J4wpYMKWE7MzISIQpIiIyqFQm9YnA1oTpKuCk/lY0s2OBqcBjKYxn\nf611sGczVCxk3qQSbr9iMY1tHTy+oZp1OxrYUtPM6zXNPPVaDW0dwZV7bjTCSceVMbuiiJLcLIrz\nohw/Np+ZxxSRlzXq7hAUEZE0cqRkoUuB+9y9s7+FZnY1cDXAlClThm+ra+6Fh6+D8XNg4RUw7xIK\n88q4cH4FF86v6Fmtq8vZ1djGy9saeHJjNX/ZVMNfNtbQ2bVv3HwzOG5MPnMmFjO7oogpZXl0OcS7\nnMLsTMYVZTOhKIfyguzhi19ERCRByh7oYmanADe4+znh9JcA3P2b/az7PPBJd3/qQOUuXrzYh2vw\nGVrr4OX74PlfwPbnwSJQeSrMOB+mnw1lUwf8qrvTFIuzp7mdV3c18fK2etZub2Dt9np21A98e9y0\ncQWcNXM8p08fQ0VJLuOKsnWFLyIiAzKz1e6+OKl1U5jUM4FXgTOBbcCzwAfcfW2f9WYAfwCmehLB\nDGtST7TzJVh7P6x7AGo2BPNKK2Hq6VA6FQonQMH48H0C5JUFl+f9qG2KsbOhjcyMDDIMGmNxdje0\n8eaeFv70ajVPb95DPOEqPz8rwriiHMYWZDN7YhGnHj+GRceWsqe5nTf3NNPR6Zw8tZzivOjw77eI\niBzRjoikHgZyHnALEAHucPdvmNmNwCp3XxmucwOQ4+773fLWn5Ql9UQ1m+C1x2Dz4/DGU9DWz512\nGdEwyY+H3FLILoK8cqhYAJOWwJjpQdJ33y/517d2sKaqjt0NMXY3xqhujLG7sY2d9W28tK2eWHz/\nnvcZBvMnlzB3YjHjCrMZV5jDzGOKmHFMIdHIUT/cgIhI2jpiknoqHJak3ld7MzTuhKZd/b+31UFb\nAzRXQ6xh/+9HsiGnGHKKwvfwlR1OZxVAJAqRLDrI5M36drbWx8nPy6OsqICujCgv7Wzl+W3NvFEf\npzYWIUaUNrLwSA6VE8qZUF7CMaVFHDeugIVTSqksz8MGqEkQEZHR42CSuhpzk5GVD+XHB6/BdHVB\n7UbY+jTUbQ2v0A3irdBWHyT+tvrgVbc1eI81QHxfG3wUOD58JZoOvLd7om9fu5rg1elGjCzaiLLL\nsolbNm2WRTtROjNyIJoDmTl4Zi5kZhPNyaeoqJiyklJy8wshKy84wcgphpyS4D03fI/mDv33ExGR\nw0JJfThlZMDYtwSvg9HVCZ0d0Nme8B6Dznj4uXt+DOKx4CQg3gYdbcEJQ/hu7W20NTZSs7eO+oZG\n6Gwj29uJeoxovA3a68ls203UY2R7OznEyCVGtsUPHGMke1+Czy0L+hTklgXzuj/3956p3v4iIoeL\nkvqRICMSvKI5h1YMUBa+DsTd2d0Y48WdjbxSVcvf129lw9ad5NFGES2URlqYmB1jQnaMcdFWJuZ0\nUJnfzvisNjLb9kLdm7D9BWjd06umYT/R/DDJl/aT9MuhqAJKj4XiyUHtQIb6B4iIDJXa1KXH7sY2\n/vZaLdWNMWqb26ltirGnuZ3qxhjrdzYSi3eRYVCWn82YgixysyLUNMWob2igMjfG+2fncd60bEpp\nCpJ9y97wfc/+7231QJ9/exYJk395kPjzyvuZLus9nVMcnBCJiKQpdZSTYdfW0clzb+zlmS172NXQ\nRk1TOy3tccYUZDOuMJsNu5r486vVZGYYZflZOBAxY0p5HieML6CiJBd36OxySvKiHF+ey7TCdvLa\ndpFR/ybRpm1kxeqgpXZf4m/Zs2+6s33g4LIKe3dE7O6AmNgxsde8kt7TmTkD3p4oIjLSlNRlRGyp\naea+1VXUNMUwg45OZ0tNM6/uaqSh7cDt9pNKc5kxoYhZxxQy85giZh5TxOSyPCIGtDf1TvI9n+uC\nzobdHRC7Ox8mdkzsf6DCfSJZ+58IZPe5U6G/k4XshHc1G4hIiqj3u4yIyjH5fOGc/TsJujttHV1k\nZARX77XN7by2u4nNNc3E4l24O63tnWzY1ci6HQ08tn4X3WPzRCNGRUkuE0tyyYlGyDAjK7OU4tzx\nlORFqSjOYcbUImZMKMTM2NPUTnN7nMryfHKzIsE4Ae3N+yf6WMO+WxH7OxFo2LFvXkfLgXe+O8Hn\nlgzcaTC3DLILg7sponn7agcyc4LlusNARA6RkrqknJkFCTY0viiH8UU5vG3amH7Xb23vZOPuIMG/\nXtNC1d4Wtte10tDWQWcXtMc7qW+NU9fS3mtkvt7bhKlj8lk4uZRlCyo4ddoxRIqC8fzdfb97+Hc3\ntNHe2cWk0rz9C+vsCJJ9rL6fE4M+typ21yLsfCnsQ7CX/foODCSat+8EoL8+BN13G2QXBbUF3e9Z\nBWo+EBFA1e8yirk7O+rbWL+zgfU7G4lY0J6fE43wWnUTr2xv4O+ba2loizO+KJtJpXnsqGtld2OM\nMQXZHDc2n9K8LF6sqqNqbysAx5bncdr0Mcw8pohjinM4pjiX48bmD/1xu12dQbLvTvDtjUHNQUdr\nUIsAQU1Af/0Iuj/3N6JhIssI+xUM0nTQd+CjnOLgzoTM7LCmoBwiOscXORKpTV0k1NbRyWPrd/Pb\n57fR0NbBxJI8xhZms7uxjc3VzdQ2x5hTUcyJx5YSyTCe3FjD3zbX0tK+rx0+M8OYPr6QqWPyaI87\nsXgnpXlZzJ9cwoLJJcyuKCInmsIe+J3xILG37NnXZBALX22J7419mhYSahYOVFtgESiaCMUTgyaC\naN6+ZoKsvOAEIKt7Xvi51zp91tUJgsiwUVIXOQTxzi6qm2LsqG+jam8r63Y0sHZ7A1V7W8jOjJCd\nmcGuhraep/FlZhgzjylizsQiinOzKMiOUF6QzfRxBUwfX0iG0XPHwJiCLCaV5qX2JKCvrq6go2Fi\nZ8K2OmhvCQY06mgNhjuu3wr124J1O1qC5R3NQc3CYHcf9CeS3ftkYKCThEhWcEtiJCu4KyGvLFjP\nu4K4swuDhygVTgg+Z+aqU6IcdZTURQ6DXQ1tvLC1jhe21vHi1jrW72yksa2Djs4D/5+aWJLLKceX\nc/oJY1k4uYSxhdnkRCNUN8Z4YWsdr9c0cfJx5cydWNzT/l/TFCMrM4OinBF4Wl9nPEzwLWHCb97/\nvedzy8Dr9l2nMx7cnRCPHfguhW6ZOUGTAxb0JbCM4D1vDJRMgeJJwToZmeHATpnBKyvss5BbGnRK\njGQFr8zs8NkL2T3PYOg9L0s1DzKilNRFRlB7vItdDW1s3N3Iq7uaAJhQlEN5QRa1Te28uaeF9Tsb\n+OumWupbO3q+l5cV6VXtD0Eb/5yKYtZsq2PrnlYiGcbiY0s5c+Y43nZ80PYfyUiDTnLuQfNB654g\n4VtG8Gqrh6bw4UmxsAahoxXw4DvuweeuzmCdujegYXtQs9DVBV3x8NURXP0PlWXsOwnoORno/px4\nMtBnXmbCsl7zEsvqZ96A30s80Uj4ngZgSmtK6iKjQGeXs6aqjld3NVLT1E5NU4yK4lwWTilhSlke\nT2yoZuWL23m9ppl5k4pZOKWEupYOHlu/m/U7GwEozM5kwZQSZh1TxAnjC4lmZrB2ez3rdjQysSSX\n9y6ayOJjS/XEPghOBro7LMZj4fMV2iHenvCMhfAVj+173kL38xh6zUv8XrLzEspO9o6IZFmkz0lF\nn1f3iY9l7Os0Gc3bd3KQeIKx3+fovs8Z0X1lQTC0dTQvWO4enDj1eoXzMiJBTUr+2H0DPmWGw2J7\nV1CmBoEakJK6SJrbUd/KM6/v4ZnX9/D8m3Vsqm6iPR5ciWZFMjh+XAFbappp7eikojiHMYXZmBnR\nDKMkL4vSvCil+VmU5EUpzcsKX1HyszPZ09zO7sYYDa0ddLnT5c6YgmxmTCji+HGHcCeA7NPzsKbw\nRCEe63My0N+8gU5A2vspq6P3vO4mCvd9/So6Wvs8RKpjX5nJNoUMt2jevldWXhB3T21LfN/JRG5p\n8Mou7N3EYpH9m126p7trOTKz+ryHtR2WEdTodIY1O50dwe8QzQ+20/MqCk9iOoPaIO8MTkw6O/Z1\nWu1sh9nvGbafRUld5CgT7+zijT0txDq6mDaugKzMDJpjcR5Zu5P/WbeLlvbOYByeeBd1rR3UtbSz\nt6Wdto6Dq5LOzDDmTCzmpOPKmDexBLOgzG11raypqmPt9gaOLc/jPQsmsnTOBPKzMmmLd2L0HqtA\njnBdnQmJviNMcuF0dz8GCE4g2puD+RmRhD4O3X0ews+d7cHtmc3VQTNL95MmIUjEEEx397noaA0+\n4/snZ/egtqVlT7COd+6f+Pd7T0jSh0N2EXxp67AVp6QuIklpbe9kb5jg61o6aGyLU5afxbjCbEry\nomRkGBlm7KxvZd2ORtZub+DZLXtYU1W3X4fAyvI8ZlcU8/L2et6obem5MOxWUZzDtPGFTCzJoSA7\nk6KcKAunlHLScWVEI+rRLodBV+e+ppd4e+937wqaFyLR8Eo/fG9vDu4IiTWGt402BCcLlhGeyIQn\nLpFo7zEiyqYOW9hK6iKSUq3tnWyuaSIzI4NoxCjPz6Y4L+iV7+48v7WOJzZUYwQdADs6u3itupmN\nuxvZ3RCjsS1Oa0dw1VSYk8kpx5WTlZlBZ5eTGcmgPD+LsYXZFOVkkhONkJ+dSWV5vqr/5ah0xIz9\nbmZLge8CEeAn7n5TP+tcAtxA0HPkRXf/QCpjEpFDl5sVYXZFcb/LzIxFU0pZNKV00DJa2uM8ubGG\n/1m3i1Vb9gIQyTA6OruoaWqnKbb/Q4AyM4JnAUDQ0TAnmkFFSS4VxblEIkZbRyctsU5qm2NUN8aI\ndzknjC9kxoRCji0PBh4aV5jDcWPzycvSbWqSflJ2pW5mEeBV4B+AKuBZ4DJ3fyVhnenAvcC73H2v\nmY1z992DlasrdZGjQ2t7J02xOG0dnTS0dfBadTMbdjb03NoXyTCaY3G217Wyra4NcLIzI+RlRSgv\nyGJsYdC7+tWdjbxW3dTrOQEZBseNLWDmMUVUlOQwoSiHY4pzmFCcy4SinJ72/wyDguzMnrsHtu5p\n4YE1O3Cc8+dWMKU8eFaAuxOLdx3eQYXkqHGkXKkvATa5++YwqBXAMuCVhHU+Dtzq7nsBDpTQReTo\nkZsV6dW5bnZFMcyvGFJZ7fEudje2Ud0YY2d9G+t3Bv0DXti6l0dejtHeOXCHweLcKMeNzccdXti6\nbxz+b/1hA3MnFpNh8Fp1M83tcS6cX8Gn3zWdaeMK2N3Qxt8211LX0kE0kkFmJKiFiHUEyf+8uRMo\nycs6YOz9PYBIZCCpTOoTgcTuf1XASX3WOQHAzP5KUEV/g7v/IYUxichRKCszg0mleT1P4Tt37jE9\ny9ydPc3t7Khv6xn+NxbeHhjv7OLNPS28Vt1Ea0cX/2fpW7hwfgVmxgMvbue/X9lFbjTC+xZNxMz4\n1bNbWfnido4ty2NL7eCP7L3xgbVctHASC6eU8NruJjbtDgYqKs6Nkh2NsKWmmVd3NRKLd3HB/Aou\nfetk5k0Kmjy6nPQYdEiGXSqr3y8Glrr7VeH0FcBJ7v6phHUeADqAS4BJwJ+Bue5e16esq4GrAaZM\nmXLiG2+8kZKYRUQORW1TjJ88+Tqv7mxkydQyTp02hoqSXDo6u+jo7CIrkkF2ZoRtda38/O9b+M1z\n24jFg/nHjc0nkmHUtXTQ2tHJlLI83jK+kPbOLh5+eQdtHV1EMozOsBkhJ5pBSW4WpflZTCnLpbI8\nn7GF2T2xFOVGGRf2ISjOi1KQnUlBduZBnwx0djlbapt5ZXsDm6ubWXRsCW87foxOKg6jI6L3u5md\nQnDlfU44/SUAd/9mwjo/Ap529zvD6UeB5e7+7EDlqk1dRNJFfUsHtc0xppTlkTnIbX0NbR088OIO\nttW1EMnIIMOgpb2TupZ2asKhh9+sbRm0GaFbXlaEguxMpo7J55zZEzh79njaOrp4eVs9r+5qpDkW\np62jiz0t7WypaeaNfsqtKM7hPQsnctr0sSycUjJoX4J4ZxdbapuJxbvo6HQmluT2Ovno1tXl/ODx\nTazf2cD/vWhuUk0TR4sjJalnEnSUOxPYRtBR7gPuvjZhnaUEnec+bGZjgOeBBe5eO1C5SuoiIvvr\n7HKa24M7BtyhobWDXQ1BP4LGtjiNsThNbXGaYsF4BM+/WceGXY29ysjMMPKzM8mJZlCcG6WyPJ+p\nY/I5flwBsyuKmFKWx59erea+1VX8+dVqujxo2lg4uYSTjyvn5OPKOX5sPiV5WcTinfzq2a3c+dct\nbKtr7bWNs2aO57KTpvTcytgci/O5e1/gkbW7yDCoHJPPnVe+lUmleTz88g5++/x23lpZyqVLplCc\n2/uBRvWtHfzttVrmTCzqaV5JN0dEUg8DOQ+4haC9/A53/4aZ3QiscveVFvT++A9gKdAJfMPdVwxW\nppK6iMjw2FzdxGPrd1OUG2XepGKmjS0YtMYgUX1rB6u27OHvm2v5++Y9rN1eT8INBj1NBUsqy7h4\n8SSKc6NkZhhPv76H+1ZXsae5naxIBjOOKaSpLc6W2ma+ev4s5kws5uN3rSLDjJLcKJtrminPz6K2\nuZ28rAjnzT2GktwokYixYWcjf91UQ0enk58V4WsXzuYfT5yEWXB7Y11LB+OLskd9R8MjJqmngpK6\niMiRp761g+fe2Mu2ulb2NrfT0tHJ0tkTmD+5ZL91Y/FOnthQzXNv7mXN1npqmmJ89fxZnH7CWCA4\n2bjqrlXkZUX43++cxjmzJ7BuRwN3PPk6j23YTUe8i3iXM64om3PnHMOp08Zw2xOb+PvmPZw6rZzW\n9k5e2lZPR6dTmhdlzsRipo8LxiqYXJZLrKOLmuZ2dta38sr2BtZubyAnGuET7ziOfzxxMlmZwYlN\nZ5cfVN8Bd2ft9gZe2d7AJW+dPDw/LErqIiIyyh3srXxdXc5PntzM7X/azLHlebx1ahnHFOWwfmcj\nL22rZ3N1c88oht0iGcb0cQXMqiji9Zpmnn+zjoriHCaX5bGltpldDTGKc6NMKg3GL8jLziQ/vNUy\nPyuT3KwIOdEIOdEMGlrj/O6Fbazf2UhhdibPfuWsYRu3QEldREQkgbtT3Rijqq6VnMwIYwqCOwe6\nnzvg7jy5qYbb/7SZto5OKsfkU1Gcw56WdrbtbWVXQ4zWjk6aY3Fa2ztpbo/3am4AmD+pmIsXT+bC\neRU9wyYPhyNl8BkREZEjgpkxriiHcUU5Ay4/bfpYTps+NqnyukcRjHV0EYt3gsG4wv7LPpyU1EVE\nRA6SmYVV7xFg+K7KD5WedygiIpImlNRFRETShJK6iIhImlBSFxERSRNK6iIiImlCSV1ERCRNKKmL\niIikCSV1ERGRNKGkLiIikiaU1EVERNKEkrqIiEiaUFIXERFJE0rqIiIiaUJJXUREJE2kNKmb2VIz\n22Bmm8xseT/LrzSzajN7IXxdlcp4RERE0lnKnqduZhHgVuAfgCrgWTNb6e6v9Fn1V+7+qVTFISIi\ncrRI5ZX6EmCTu29293ZgBbAshdsTERE5qqUyqU8EtiZMV4Xz+nqfma0xs/vMbHIK4xEREUlrI91R\n7vdApbvPA/4I/Ky/lczsajNbZWarqqurD2uAIiIio0Uqk/o2IPHKe1I4r4e717p7LJz8CXBifwW5\n+4/dfbG7Lx47dmxKghURERntDpjUww5vQ/EsMN3MpppZFnApsLJP2cckTF4IrBvitkRERI56yfR+\n32hmvwbu7Kfn+oDcPW5mnwIeASLAHe6+1sxuBFa5+0rgM2Z2IRAH9gBXHvQeiIiICADm7oOvYFZI\ncJX9EYIr+zuAFe7ekPrw9rd48WJftWrVSGxaRETksDOz1e6+OJl1D1j97u6N7v6f7v424IvA14Ad\nZvYzM5t2iLGKiIjIMEmqTd3MLjSz+4FbgP8AjiPouf5QiuMTERGRJCXVpg48Dvy7uz+VMP8+Mzs9\nNWGJiIjIwUomqc9z96b+Frj7Z4Y5HhERERmiZO5TH2dmvzezGjPbbWa/M7PjUh6ZiIiIHJRkkvrd\nwL3ABKAC+C/gnlQGJSIiIgcvmaSe5+4/d/d4+PoFkJPqwEREROTgJNOm/nD4LPQVgAPvBx4yszIA\nd9+TwvhEREQkSckk9UvC908t8qxNAAAYPElEQVT0mX8pQZJX+7qIiMgR4IBJ3d2nHo5ARERE5NAc\nMKmbWRS4Bui+J/0J4HZ370hhXCIiInKQkql+vw2IAj8Mp68I512VqqBERETk4CWT1N/q7vMTph8z\nsxdTFZCIiIgMTTK3tHWa2fHdE+HAM52pC0lERESGIpkr9euAx81sM2DAsQSPYRUREZEjyKBJ3cwy\ngFZgOvCWcPYGd4+lOjARERE5OIMmdXfvMrNb3X0hsOYwxSQiIiJDkEyb+qNm9j4zs5RHIyIiIkOW\nTFL/BMFDXGJm1mBmjWbWkEzhZrbUzDaY2aZwqNmB1nufmbmZLU4ybhEREekjmRHlCodSsJlFgFuB\nfwCqgGfNbKW7v9JnvULgWuDpoWxHREREAge8UjezR5OZ148lwCZ33+zu7QQPhFnWz3r/Cvwb0JZE\nmSIiIjKAAZO6meWET2IbY2alZlYWviqBiUmUPRHYmjBd1fd7ZrYImOzuDx505CIiItLLYNXvnwD+\nCagAVhPcow7QAPzgUDcc3i53M3BlEuteDVwNMGXKlEPdtIiISFoa8Erd3b8bPqHtC+5+nLtPDV/z\n3T2ZpL4NmJwwPSmc160QmAM8YWZbgJOBlf11lnP3H7v7YndfPHbs2CQ2LSIicvRJpqPc983sbUBl\n4vruftcBvvosMN3MphIk80uBDyR8vx4Y0z1tZk8QnECsOoj4RUREJJTMo1d/DhwPvMC+Md8dGDSp\nu3vczD4FPAJEgDvcfa2Z3QiscveVhxS5iIiI9JLM2O+LgVnu7gdbuLs/BDzUZ971A6z7zoMtX0RE\nRPZJZvCZl4EJqQ5EREREDk0yV+pjgFfM7Bmg50Eu7n5hyqISERGRg5ZMUr8h1UGIiIjIoRswqZvZ\nDHdf7+5/MrPsxMetmtnJhyc8ERERSdZgbep3J3z+W59lP0xBLCIiInIIBkvqNsDn/qZFRERkhA2W\n1H2Az/1Ni4iIyAgbrKPcJDP7HsFVefdnwulkHugiIiIih9FgSf26hM99h27VUK4iIiJHmAGTurv/\n7HAGIiIiIocmmRHlREREZBRQUhcREUkTSuoiIiJp4oBJ3cy+ZWZFZhY1s0fNrNrMLj8cwYmIiEjy\nkrlSP9vdG4DzgS3ANHr3jBcREZEjQDJJvbuH/LuB/3L3+hTGIyIiIkOUzFPaHjCz9UArcI2ZjQXa\nUhuWiIiIHKwDXqm7+3LgbcBid+8AmoFlqQ5MREREDk4yHeX+Eehw904z+wrwC6AimcLNbKmZbTCz\nTWa2vJ/l/8vMXjKzF8zsSTObddB7ICIiIkBybepfdfdGM3s7cBbw/4DbDvQlM4sAtwLnArOAy/pJ\n2ne7+1x3XwB8C7j5oKIXERGRHskk9c7w/d3Aj939QSArie8tATa5+2Z3bwdW0KfaPuxV3y0fPf1N\nRERkyJLpKLfNzG4H/gH4NzPLJrmTgYnA1oTpKuCkviuZ2SeBzxGcKLwriXJFRESkH8kk50uAR4Bz\n3L0OKGMY71N391vd/Xjgi8BX+lvHzK42s1Vmtqq6unq4Ni0iIpJWkun93gK8BpxjZp8Cxrn7fydR\n9jZgcsL0pHDeQFYA7xkghh+7+2J3Xzx27NgkNi0iInL0Sab3+7XAL4Fx4esXZvbpJMp+FphuZlPN\nLAu4FFjZp+zpCZPvBjYmG7iIiIj0lkyb+seAk9y9GcDM/g34G/D9wb7k7vHwyv4RIALc4e5rzexG\nYJW7rwQ+ZWZnAR3AXuDDQ98VERGRo1sySd3Y1wOe8LMlU7i7PwQ81Gfe9Qmfr02mHBERETmwZJL6\nncDTZnZ/OP0egnvVRURE5AhywKTu7jeb2RPA28NZH3H351MalYiIiBy0QZN6OCrcWnefATx3eEIS\nERGRoRi097u7dwIbzGzKYYpHREREhiiZNvVSYK2ZPUPwhDYA3P3ClEUlIiIiBy2ZpP7VlEchIiIi\nh2zApG5m04Dx7v6nPvPfDuxIdWAiIiJycAZrU78FaOhnfn24TERERI4ggyX18e7+Ut+Z4bzKlEUk\nIiIiQzJYUi8ZZFnucAciIiIih2awpL7KzD7ed6aZXQWsTl1IIiIiMhSD9X7/J+B+M/sg+5L4YiAL\nuCjVgYmIiMjBGTCpu/su4G1mdgYwJ5z9oLs/dlgiExERkYOSzNjvjwOPH4ZYRERE5BAMOkysiIiI\njB5K6iIiImlCSV1ERCRNKKmLiIikiZQmdTNbamYbzGyTmS3vZ/nnzOwVM1tjZo+a2bGpjEdERCSd\npSypm1kEuBU4F5gFXGZms/qs9jyw2N3nAfcB30pVPCIiIukulVfqS4BN7r7Z3duBFcCyxBXc/XF3\nbwkn/w5MSmE8IiIiaS2VSX0isDVhuiqcN5CPAQ+nMB4REZG0dsDBZw4HM7ucYAjadwyw/GrgaoAp\nU6YcxshERERGj1ReqW8DJidMTwrn9WJmZwH/DFzo7rH+CnL3H7v7YndfPHbs2JQEKyIiMtqlMqk/\nC0w3s6lmlgVcCqxMXMHMFgK3EyT03SmMRUREJO2lLKm7exz4FPAIsA64193XmtmNZnZhuNq/AwXA\nf5nZC2a2coDiRERE5ABS2qbu7g8BD/WZd33C57NSuX0REZGjiUaUExERSRNK6iIiImlCSV1ERCRN\nKKmLiIikCSV1ERGRNKGkLiIikiaU1EVERNKEkrqIiEiaUFIXERFJE0rqIiIiaUJJXUREJE0cEc9T\nFxGR4dPR0UFVVRVtbW0jHYochJycHCZNmkQ0Gh1yGUrqIiJppqqqisLCQiorKzGzkQ5HkuDu1NbW\nUlVVxdSpU4dcjqrfRUTSTFtbG+Xl5Uroo4iZUV5efsi1K0rqIiJpSAl99BmOY6akLiIiw6q2tpYF\nCxawYMECJkyYwMSJE3um29vbkyrjIx/5CBs2bBh0nVtvvZVf/vKXwxEyb3/723nhhReGpayRpDZ1\nEREZVuXl5T0J8oYbbqCgoIAvfOELvdZxd9ydjIz+ry3vvPPOA27nk5/85KEHm2Z0pS4iIofFpk2b\nmDVrFh/84AeZPXs2O3bs4Oqrr2bx4sXMnj2bG2+8sWfd7ivneDxOSUkJy5cvZ/78+Zxyyins3r0b\ngK985SvccsstPesvX76cJUuW8Ja3vIWnnnoKgObmZt73vvcxa9YsLr74YhYvXpz0FXlraysf/vCH\nmTt3LosWLeLPf/4zAC+99BJvfetbWbBgAfPmzWPz5s00NjZy7rnnMn/+fObMmcN99903nD9d0lJ6\npW5mS4HvAhHgJ+5+U5/lpwO3APOAS919ZH4FEZE09S+/X8sr2xuGtcxZFUV87YLZQ/ru+vXrueuu\nu1i8eDEAN910E2VlZcTjcc444wwuvvhiZs2a1es79fX1vOMd7+Cmm27ic5/7HHfccQfLly/fr2x3\n55lnnmHlypXceOON/OEPf+D73/8+EyZM4Ne//jUvvvgiixYtSjrW733ve2RnZ/PSSy+xdu1azjvv\nPDZu3MgPf/hDvvCFL/D+97+fWCyGu/O73/2OyspKHn744Z6YR0LKrtTNLALcCpwLzAIuM7NZfVZ7\nE7gSuDtVcYiIyJHj+OOP70noAPfccw+LFi1i0aJFrFu3jldeeWW/7+Tm5nLuuecCcOKJJ7Jly5Z+\ny37ve9+73zpPPvkkl156KQDz589n9uzkT0aefPJJLr/8cgBmz55NRUUFmzZt4m1vextf//rX+da3\nvsXWrVvJyclh3rx5/OEPf2D58uX89a9/pbi4OOntDKdUXqkvATa5+2YAM1sBLAN6jpi7bwmXdaUw\nDhGRo9ZQr6hTJT8/v+fzxo0b+e53v8szzzxDSUkJl19+eb+3dGVlZfV8jkQixOPxfsvOzs4+4DrD\n4YorruCUU07hwQcfZOnSpdxxxx2cfvrprFq1ioceeojly5dz7rnn8uUvfzllMQwklW3qE4GtCdNV\n4TwREREaGhooLCykqKiIHTt28Mgjjwz7Nk499VTuvfdeIGgL768mYCCnnXZaT+/6devWsWPHDqZN\nm8bmzZuZNm0a1157Leeffz5r1qxh27ZtFBQUcMUVV/D5z3+e5557btj3JRmjove7mV0NXA0wZcqU\nEY5GRESGw6JFi5g1axYzZszg2GOP5dRTTx32bXz605/mQx/6ELNmzep5DVQ1fs455/QM0Xraaadx\nxx138IlPfIK5c+cSjUa56667yMrK4u677+aee+4hGo1SUVHBDTfcwFNPPcXy5cvJyMggKyuLH/3o\nR8O+L8kwd09NwWanADe4+znh9JcA3P2b/az7U+CBZDrKLV682FetWjXM0YqIpI9169Yxc+bMkQ7j\niBCPx4nH4+Tk5LBx40bOPvtsNm7cSGbmkXlN29+xM7PV7r54gK/0ksq9ehaYbmZTgW3ApcAHUrg9\nERGRXpqamjjzzDOJx+O4O7fffvsRm9CHQ8r2zN3jZvYp4BGCW9rucPe1ZnYjsMrdV5rZW4H7gVLg\nAjP7F3c/snp1iIjIqFVSUsLq1atHOozDJqWnK+7+EPBQn3nXJ3x+FpiUyhhERESOFhpRTkREJE0o\nqYuIiKQJJXUREZE0oaQuIiLD6owzzthvIJlbbrmFa665ZtDvFRQUALB9+3Yuvvjiftd55zvfyYFu\na77llltoaWnpmT7vvPOoq6tLJvRB3XDDDXz7298+5HJSSUldRESG1WWXXcaKFSt6zVuxYgWXXXZZ\nUt+vqKg4pKec9U3qDz30ECUlJUMubzRRUhcRkWF18cUX8+CDD9Le3g7Ali1b2L59O6eddlrPfeOL\nFi1i7ty5/O53v9vv+1u2bGHOnDlA8PjTSy+9lJkzZ3LRRRfR2tras94111zT89jWr33ta0DwZLXt\n27dzxhlncMYZZwBQWVlJTU0NADfffDNz5sxhzpw5PY9t3bJlCzNnzuTjH/84s2fP5uyzz+61nQPp\nr8zm5mbe/e539zyK9Ve/+hUAy5cvZ9asWcybN2+/Z8wPh/S9A19ERODh5bDzpeEtc8JcOPemAReX\nlZWxZMkSHn74YZYtW8aKFSu45JJLMDNycnK4//77KSoqoqamhpNPPpkLL7wQM+u3rNtuu428vDzW\nrVvHmjVrej069Rvf+AZlZWV0dnZy5plnsmbNGj7zmc9w88038/jjjzNmzJheZa1evZo777yTp59+\nGnfnpJNO4h3veAelpaVs3LiRe+65h//8z//kkksu4de//nXPE9oGM1CZmzdvpqKiggcffBAIHsVa\nW1vL/fffz/r16zGzYWkS6EtX6iIiMuwSq+ATq97dnS9/+cvMmzePs846i23btrFr164By/nzn//c\nk1znzZvHvHnzepbde++9LFq0iIULF7J27doDPqzlySef5KKLLiI/P5+CggLe+9738pe//AWAqVOn\nsmDBAmDwx7smW+bcuXP54x//yBe/+EX+8pe/UFxcTHFxMTk5OXzsYx/jN7/5DXl5eUlt42DoSl1E\nJJ0NckWdSsuWLeOzn/0szz33HC0tLZx44okA/PKXv6S6uprVq1cTjUaprKzs93GrB/L666/z7W9/\nm2effZbS0lKuvPLKIZXTrfuxrRA8uvVgqt/7c8IJJ/Dcc8/x0EMP8ZWvfIUzzzyT66+/nmeeeYZH\nH32U++67jx/84Ac89thjh7SdvnSlLiIiw66goIAzzjiDj370o706yNXX1zNu3Dii0SiPP/44b7zx\nxqDlnH766dx9990AvPzyy6xZswYIHtuan59PcXExu3bt4uGHH+75TmFhIY2NjfuVddppp/Hb3/6W\nlpYWmpubuf/++znttNMOaT8HKnP79u3k5eVx+eWXc9111/Hcc8/R1NREfX095513Ht/5znd48cUX\nD2nb/dGVuoiIpMRll13GRRdd1Ksn/Ac/+EEuuOAC5s6dy+LFi5kxY8agZVxzzTV85CMfYebMmcyc\nObPnin/+/PksXLiQGTNmMHny5F6Pbb366qtZunQpFRUVPP744z3zFy1axJVXXsmSJUsAuOqqq1i4\ncGHSVe0AX//613s6wwFUVVX1W+YjjzzCddddR0ZGBtFolNtuu43GxkaWLVtGW1sb7s7NN9+c9HaT\nlbJHr6aKHr0qIjI4PXp19DrUR6+q+l1ERCRNKKmLiIikCSV1ERGRNKGkLiKShkZbfykZnmOmpC4i\nkmZycnKora1VYh9F3J3a2lpycnIOqRzd0iYikmYmTZpEVVUV1dXVIx2KHIScnBwmTZp0SGWkNKmb\n2VLgu0AE+Im739RneTZwF3AiUAu83923pDImEZF0F41GmTp16kiHISMgZdXvZhYBbgXOBWYBl5nZ\nrD6rfQzY6+7TgO8A/5aqeERERNJdKtvUlwCb3H2zu7cDK4BlfdZZBvws/HwfcKYN9KgeERERGVQq\nk/pEYGvCdFU4r9913D0O1APlKYxJREQkbY2KjnJmdjVwdTjZZGYbhrH4MUDNMJZ3JEnXfUvX/YL0\n3bd03S9I331L1/2C0bdvxya7YiqT+jZgcsL0pHBef+tUmVkmUEzQYa4Xd/8x8ONUBGlmq5IdU3e0\nSdd9S9f9gvTdt3TdL0jffUvX/YL03rdUVr8/C0w3s6lmlgVcCqzss85K4MPh54uBx1w3VoqIiAxJ\nyq7U3T1uZp8CHiG4pe0Od19rZjcCq9x9JfD/gJ+b2SZgD0HiFxERkSFIaZu6uz8EPNRn3vUJn9uA\nf0xlDElISbX+ESJd9y1d9wvSd9/Sdb8gffctXfcL0njfRt3z1EVERKR/GvtdREQkTRzVSd3MlprZ\nBjPbZGbLRzqeoTKzyWb2uJm9YmZrzezacH6Zmf3RzDaG76UjHetQmFnEzJ43swfC6alm9nR43H4V\ndsQcdcysxMzuM7P1ZrbOzE5Jo2P22fDf4stmdo+Z5YzG42Zmd5jZbjN7OWFev8fIAt8L92+NmS0a\nucgPbIB9+/fw3+MaM7vfzEoSln0p3LcNZnbOyER9YP3tV8Kyz5uZm9mYcHpUHbNkHLVJPclhbEeL\nOPB5d58FnAx8MtyX5cCj7j4deDScHo2uBdYlTP8b8J1weOG9BMMNj0bfBf7g7jOA+QT7OOqPmZlN\nBD4DLHb3OQQdZS9ldB63nwJL+8wb6BidC0wPX1cDtx2mGIfqp+y/b38E5rj7POBV4EsA4d+TS4HZ\n4Xd+GP4NPRL9lP33CzObDJwNvJkwe7QdswM6apM6yQ1jOyq4+w53fy783EiQHCbSexjenwHvGZkI\nh87MJgHvBn4SThvwLoJhhWH07lcxcDrBHSC4e7u715EGxyyUCeSG40/kATsYhcfN3f9McGdOooGO\n0TLgLg/8HSgxs2MOT6QHr799c/f/Dkf3BPg7wfgiEOzbCnePufvrwCaCv6FHnAGOGQTPF/k/QGJH\nslF1zJJxNCf1ZIaxHXXMrBJYCDwNjHf3HeGincD4EQrrUNxC8B+xK5wuB+oS/vCM1uM2FagG7gyb\nFn5iZvmkwTFz923AtwmuiHYQDP+8mvQ4bjDwMUq3vykfBR4OP4/qfTOzZcA2d3+xz6JRvV/9OZqT\netoxswLg18A/uXtD4rJwUJ9RdauDmZ0P7Hb31SMdSwpkAouA29x9IdBMn6r20XjMAMI25mUEJy4V\nQD79VIemg9F6jA7EzP6ZoFnvlyMdy6Eyszzgy8D1B1o3HRzNST2ZYWxHDTOLEiT0X7r7b8LZu7qr\nksL33SMV3xCdClxoZlsImkfeRdAOXRJW68LoPW5VQJW7Px1O30eQ5Ef7MQM4C3jd3avdvQP4DcGx\nTIfjBgMfo7T4m2JmVwLnAx9MGOFzNO/b8QQnmC+Gf0smAc+Z2QRG937162hO6skMYzsqhO3M/w9Y\n5+43JyxKHIb3w8DvDndsh8Ldv+Tuk9y9kuD4PObuHwQeJxhWGEbhfgG4+05gq5m9JZx1JvAKo/yY\nhd4ETjazvPDfZve+jfrjFhroGK0EPhT2qD4ZqE+oph8VzGwpQXPXhe7ekrBoJXCpmWWb2VSCjmXP\njESMB8vdX3L3ce5eGf4tqQIWhf8HR/0x24+7H7Uv4DyCHp6vAf880vEcwn68naAKcA3wQvg6j6D9\n+VFgI/A/QNlIx3oI+/hO4IHw83EEf1A2Af8FZI90fEPcpwXAqvC4/RYoTZdjBvwLsB54Gfg5kD0a\njxtwD0G/gA6CZPCxgY4RYAR31LwGvETQ+3/E9+Eg920TQRtz99+RHyWs/8/hvm0Azh3p+A9mv/os\n3wKMGY3HLJmXRpQTERFJE0dz9buIiEhaUVIXERFJE0rqIiIiaUJJXUREJE0oqYuIiKQJJXUREZE0\noaQuIiKSJpTURURE0sT/B+/0OpiZHL+KAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POHLQd9PPMLX",
        "colab_type": "code",
        "outputId": "98d19923-4369-48e5-8857-43696ef07d4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "_, acc = model.evaluate_generator(generator=train_generator)\n",
        "print('Accuracy on training set: {}%'.format(round(acc, 3)*100))\n",
        "\n",
        "_, acc = model.evaluate_generator(generator=validation_generator)\n",
        "print('Accuracy on validation set: {}%'.format(round(acc, 3)*100))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set: 74.50000047683716%\n",
            "Accuracy on validation set: 68.00000071525574%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeN1ZqTAM42i",
        "colab_type": "text"
      },
      "source": [
        "**Export Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkUfyxqDM3NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keras library import  for Saving and loading model and weights\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwvXWQnGEENs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "model_json = model.to_json()\n",
        "\n",
        "\n",
        "with open(\"drive/Colab Notebooks/model/3_model_num.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"drive/Colab Notebooks/model/3_model_num.h5\")\n",
        "model.save('drive/Colab Notebooks/model/3_model_num.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}