{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Housing 4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caroref/model/blob/master/Housing_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ascUYWYXFlTS",
        "colab_type": "code",
        "outputId": "12303abe-3db9-423a-9d00-905ccbccb201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TensorFlow version is \", tf.__version__)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0605 17:19:07.263085 140700573890432 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version is  1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs-4SeS3Fpc5",
        "colab_type": "code",
        "outputId": "f2db083c-f2d9-4460-8b1c-e1a273a293a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Allow Colab to access drive\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "#linking Colab to use drive as if local\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDNLdWVxP2jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main directories\n",
        "trn_dir = 'drive/Colab/Final/Test4/Train/'\n",
        "tst_dir = 'drive/Colab/Final/Test4/Test/'\n",
        "\n",
        "#subdirectories\n",
        "trn_like = os.path.join(trn_dir, 'Like')\n",
        "tst_like = os.path.join(tst_dir, 'Like')\n",
        "\n",
        "trn_disl = os.path.join(trn_dir, 'Dislike')\n",
        "tst_disl = os.path.join(tst_dir, 'Dislike')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo3GVjj1FsEn",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5BtzOSmbZB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All images will be resized to image_size\n",
        "\n",
        "image_size = 640 \n",
        "batch_size = 100\n",
        "\n",
        "# create Image Data Generator for Image Data Augmentation\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "                rescale=1./255, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTvB7EieGDAN",
        "colab_type": "code",
        "outputId": "16d2db7a-3458-433e-af08-f231a672b34a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Create the train and validation generators and specify the train dataset directory, \n",
        "# image size, batch size, binary classification, and subset (training or validation).\n",
        "\n",
        "\n",
        "# Flow training images in batches using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,  \n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode ='binary',\n",
        "                subset = 'training') \n",
        "\n",
        "# Flow validation images in batches using validation_datagen generator\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,\n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode='binary',\n",
        "                subset = 'validation')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ur5r8uGWQO",
        "colab_type": "text"
      },
      "source": [
        "# Create the base model from the pre-trained convnets\n",
        "\n",
        "Using the output of last layer (bottleneck layer). Keep other layers frozen (these are specific to their training, less useful here?) Bottleneck layer should be more general.\n",
        "\n",
        "Using Inception v3 model with weights trained on ImageNet.\n",
        "The argument, include_top=False, means that we have a network that isn't including classification layers at the top. This is good for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCLmagoGbc3",
        "colab_type": "code",
        "outputId": "35697788-2ba7-4ed2-eb34-098bd11b8545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "IMG_SHAPE = (image_size, image_size, 3)\n",
        "\n",
        "base_model = keras.applications.inception_v3.InceptionV3(\n",
        "             include_top=False, weights='imagenet', input_shape=IMG_SHAPE)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0605 17:20:32.857091 140700573890432 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wXzV2zlGduw",
        "colab_type": "text"
      },
      "source": [
        "# Feature extraction\n",
        "We will freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.\n",
        "\n",
        "It's important to freeze the convolutional based before we compile and train the model. By freezing (or setting layer.trainable = False), we prevent the weights in these layers from being updated during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCk3KhmGGhaH",
        "colab_type": "code",
        "outputId": "add7498a-447d-48b4-f907-9823d5e551e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11821
        }
      },
      "source": [
        "#freeze base\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "# Look at the base model architecture\n",
        "base_model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 640, 640, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 319, 319, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 319, 319, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 319, 319, 32) 0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 317, 317, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 317, 317, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 317, 317, 32) 0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 317, 317, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 317, 317, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 317, 317, 64) 0           batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 158, 158, 64) 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 158, 158, 80) 5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_3 (Batch (None, 158, 158, 80) 240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 158, 158, 80) 0           batch_normalization_v1_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 156, 156, 192 138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 156, 156, 192 576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 156, 156, 192 0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 77, 77, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_8 (Batch (None, 77, 77, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 77, 77, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 77, 77, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_6 (Batch (None, 77, 77, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_9 (Batch (None, 77, 77, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 77, 77, 48)   0           batch_normalization_v1_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 77, 77, 96)   0           batch_normalization_v1_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 77, 77, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 77, 77, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 77, 77, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 77, 77, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_5 (Batch (None, 77, 77, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_7 (Batch (None, 77, 77, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_10 (Batc (None, 77, 77, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_11 (Batc (None, 77, 77, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_10[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 77, 77, 32)   0           batch_normalization_v1_11[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 77, 77, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_15 (Batc (None, 77, 77, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_15[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 77, 77, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 77, 77, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_13 (Batc (None, 77, 77, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_16 (Batc (None, 77, 77, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_13[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_16[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 77, 77, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 77, 77, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 77, 77, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 77, 77, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_12 (Batc (None, 77, 77, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_14 (Batc (None, 77, 77, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_17 (Batc (None, 77, 77, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_18 (Batc (None, 77, 77, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_12[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_14[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_17[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_18[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 77, 77, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_22 (Batc (None, 77, 77, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_22[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 77, 77, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 77, 77, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_20 (Batc (None, 77, 77, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_23 (Batc (None, 77, 77, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_20[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_23[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 77, 77, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 77, 77, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 77, 77, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 77, 77, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_19 (Batc (None, 77, 77, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_21 (Batc (None, 77, 77, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_24 (Batc (None, 77, 77, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_25 (Batc (None, 77, 77, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_19[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_21[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_24[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_25[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 77, 77, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 77, 77, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_27 (Batc (None, 77, 77, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_27[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 77, 77, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_28 (Batc (None, 77, 77, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_28[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 38, 38, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 38, 38, 96)   82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_26 (Batc (None, 38, 38, 384)  1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_29 (Batc (None, 38, 38, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 38, 38, 384)  0           batch_normalization_v1_26[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 38, 38, 96)   0           batch_normalization_v1_29[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 38, 38, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 38, 38, 768)  0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_34 (Batc (None, 38, 38, 128)  384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_34[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 38, 38, 128)  114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_35 (Batc (None, 38, 38, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_35[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 38, 38, 128)  114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_31 (Batc (None, 38, 38, 128)  384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_36 (Batc (None, 38, 38, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_31[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_36[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 38, 38, 128)  114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 38, 38, 128)  114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_32 (Batc (None, 38, 38, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_37 (Batc (None, 38, 38, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_32[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_37[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 38, 38, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 38, 38, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 38, 38, 192)  172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 38, 38, 192)  172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_30 (Batc (None, 38, 38, 192)  576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_33 (Batc (None, 38, 38, 192)  576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_38 (Batc (None, 38, 38, 192)  576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_39 (Batc (None, 38, 38, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_30[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_33[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_38[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_39[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 38, 38, 768)  0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_44 (Batc (None, 38, 38, 160)  480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_44[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 38, 38, 160)  179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_45 (Batc (None, 38, 38, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_45[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 38, 38, 160)  179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_41 (Batc (None, 38, 38, 160)  480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_46 (Batc (None, 38, 38, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_41[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_46[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 38, 38, 160)  179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 38, 38, 160)  179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_42 (Batc (None, 38, 38, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_47 (Batc (None, 38, 38, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_42[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_47[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 38, 38, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 38, 38, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 38, 38, 192)  215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 38, 38, 192)  215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_40 (Batc (None, 38, 38, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_43 (Batc (None, 38, 38, 192)  576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_48 (Batc (None, 38, 38, 192)  576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_49 (Batc (None, 38, 38, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_40[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_43[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_48[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_49[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 38, 38, 768)  0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_54 (Batc (None, 38, 38, 160)  480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_54[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 38, 38, 160)  179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_55 (Batc (None, 38, 38, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_55[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 38, 38, 160)  179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_51 (Batc (None, 38, 38, 160)  480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_56 (Batc (None, 38, 38, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_51[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_56[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 38, 38, 160)  179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 38, 38, 160)  179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_52 (Batc (None, 38, 38, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_57 (Batc (None, 38, 38, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_52[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_57[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 38, 38, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 38, 38, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 38, 38, 192)  215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 38, 38, 192)  215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_50 (Batc (None, 38, 38, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_53 (Batc (None, 38, 38, 192)  576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_58 (Batc (None, 38, 38, 192)  576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_59 (Batc (None, 38, 38, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_50[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_53[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_58[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_59[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 38, 38, 768)  0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_64 (Batc (None, 38, 38, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_64[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 38, 38, 192)  258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_65 (Batc (None, 38, 38, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_65[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 38, 38, 192)  258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_61 (Batc (None, 38, 38, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_66 (Batc (None, 38, 38, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_61[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_66[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 38, 38, 192)  258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 38, 38, 192)  258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_62 (Batc (None, 38, 38, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_67 (Batc (None, 38, 38, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_62[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_67[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 38, 38, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 38, 38, 192)  258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 38, 38, 192)  258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_60 (Batc (None, 38, 38, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_63 (Batc (None, 38, 38, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_68 (Batc (None, 38, 38, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_69 (Batc (None, 38, 38, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_60[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_63[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_68[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_69[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 38, 38, 768)  0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_72 (Batc (None, 38, 38, 192)  576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_72[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 38, 38, 192)  258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_73 (Batc (None, 38, 38, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_73[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 38, 38, 192)  258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_70 (Batc (None, 38, 38, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_74 (Batc (None, 38, 38, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_70[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_74[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 18, 18, 320)  552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 18, 18, 192)  331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_71 (Batc (None, 18, 18, 320)  960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_75 (Batc (None, 18, 18, 192)  576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_71[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_75[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 18, 18, 768)  0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 18, 18, 1280) 0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 18, 18, 448)  573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_80 (Batc (None, 18, 18, 448)  1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_80[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 18, 18, 384)  491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 18, 18, 384)  1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_77 (Batc (None, 18, 18, 384)  1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_81 (Batc (None, 18, 18, 384)  1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_77[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_81[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 18, 18, 1280) 0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 18, 18, 320)  409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_78 (Batc (None, 18, 18, 384)  1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_79 (Batc (None, 18, 18, 384)  1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_82 (Batc (None, 18, 18, 384)  1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_83 (Batc (None, 18, 18, 384)  1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 18, 18, 192)  245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_76 (Batc (None, 18, 18, 320)  960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_78[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_79[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_82[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_83[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_84 (Batc (None, 18, 18, 192)  576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_76[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 18, 18, 768)  0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 18, 18, 768)  0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_84[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 18, 18, 2048) 0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 18, 18, 448)  917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_89 (Batc (None, 18, 18, 448)  1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_89[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 18, 18, 384)  786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 18, 18, 384)  1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_86 (Batc (None, 18, 18, 384)  1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_90 (Batc (None, 18, 18, 384)  1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_86[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_90[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 18, 18, 2048) 0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 18, 18, 320)  655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_87 (Batc (None, 18, 18, 384)  1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_88 (Batc (None, 18, 18, 384)  1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_91 (Batc (None, 18, 18, 384)  1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_92 (Batc (None, 18, 18, 384)  1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 18, 18, 192)  393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_85 (Batc (None, 18, 18, 320)  960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_87[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_88[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_91[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_92[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_93 (Batc (None, 18, 18, 192)  576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_85[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 18, 18, 768)  0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 18, 18, 768)  0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_93[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 18, 18, 2048) 0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 0\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUc1ad5qGnMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add a classification head\n",
        "#adding a few layers on top of the base model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  keras.layers.GlobalAveragePooling2D(),\n",
        "  keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "#relatively basic layer. no deep neural networks (all happening beforehand). Similar to a logit regression. plain classification\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbhLA3TGr1X",
        "colab_type": "code",
        "outputId": "0471956a-3ee0-41fa-f54c-f83b622c51f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "#Compile the model (Must do before training!)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 18, 18, 2048)      21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 21,804,833\n",
            "Trainable params: 2,049\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th655DVEGwop",
        "colab_type": "code",
        "outputId": "81e37cf7-d81a-4c97-b35d-04ad53197774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#These 2K trainable parameters are divided among 2 TensorFlow Variable objects, the weights and biases of the two dense layers:\n",
        "\n",
        "\n",
        "len(model.trainable_variables)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZeMfEwoG3wm",
        "colab_type": "text"
      },
      "source": [
        "**Train the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNgpgcJjG8xl",
        "colab_type": "code",
        "outputId": "6294bd12-4192-4496-9c41-bea7eedd1f7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8144
        }
      },
      "source": [
        "epochs = 150\n",
        "steps_per_epoch = train_generator.n // batch_size\n",
        "validation_steps = validation_generator.n // batch_size\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch = steps_per_epoch,\n",
        "                              epochs=epochs,\n",
        "                              workers=4,\n",
        "                              validation_data=validation_generator,\n",
        "                              validation_steps=validation_steps)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0605 17:20:48.169045 140700573890432 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "1/1 [==============================] - 41s 41s/step - loss: 0.7147 - acc: 0.5000\n",
            "4/4 [==============================] - 106s 27s/step - loss: 0.7068 - acc: 0.4775 - val_loss: 0.7147 - val_acc: 0.5000\n",
            "Epoch 2/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7114 - acc: 0.4600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.7002 - acc: 0.4525 - val_loss: 0.7114 - val_acc: 0.4600\n",
            "Epoch 3/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7099 - acc: 0.5000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6971 - acc: 0.4700 - val_loss: 0.7099 - val_acc: 0.5000\n",
            "Epoch 4/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7079 - acc: 0.4900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6949 - acc: 0.4900 - val_loss: 0.7079 - val_acc: 0.4900\n",
            "Epoch 5/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7081 - acc: 0.5100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6916 - acc: 0.5175 - val_loss: 0.7081 - val_acc: 0.5100\n",
            "Epoch 6/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7081 - acc: 0.5400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6919 - acc: 0.4950 - val_loss: 0.7081 - val_acc: 0.5400\n",
            "Epoch 7/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7062 - acc: 0.5300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6905 - acc: 0.5075 - val_loss: 0.7062 - val_acc: 0.5300\n",
            "Epoch 8/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7051 - acc: 0.5300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6890 - acc: 0.5150 - val_loss: 0.7051 - val_acc: 0.5300\n",
            "Epoch 9/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7031 - acc: 0.5000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6887 - acc: 0.5200 - val_loss: 0.7031 - val_acc: 0.5000\n",
            "Epoch 10/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7019 - acc: 0.4800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6861 - acc: 0.5225 - val_loss: 0.7019 - val_acc: 0.4800\n",
            "Epoch 11/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7011 - acc: 0.4800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6839 - acc: 0.5450 - val_loss: 0.7011 - val_acc: 0.4800\n",
            "Epoch 12/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7009 - acc: 0.4900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6817 - acc: 0.5425 - val_loss: 0.7009 - val_acc: 0.4900\n",
            "Epoch 13/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7008 - acc: 0.5000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6817 - acc: 0.5400 - val_loss: 0.7008 - val_acc: 0.5000\n",
            "Epoch 14/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.7006 - acc: 0.5200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6799 - acc: 0.5725 - val_loss: 0.7006 - val_acc: 0.5200\n",
            "Epoch 15/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6990 - acc: 0.5100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6794 - acc: 0.5800 - val_loss: 0.6990 - val_acc: 0.5100\n",
            "Epoch 16/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6980 - acc: 0.5300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6773 - acc: 0.5675 - val_loss: 0.6980 - val_acc: 0.5300\n",
            "Epoch 17/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6971 - acc: 0.5200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6759 - acc: 0.5900 - val_loss: 0.6971 - val_acc: 0.5200\n",
            "Epoch 18/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6968 - acc: 0.5300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6746 - acc: 0.5800 - val_loss: 0.6968 - val_acc: 0.5300\n",
            "Epoch 19/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6959 - acc: 0.5400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6721 - acc: 0.6000 - val_loss: 0.6959 - val_acc: 0.5400\n",
            "Epoch 20/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6957 - acc: 0.5600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6708 - acc: 0.6000 - val_loss: 0.6957 - val_acc: 0.5600\n",
            "Epoch 21/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6947 - acc: 0.5700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6695 - acc: 0.6150 - val_loss: 0.6947 - val_acc: 0.5700\n",
            "Epoch 22/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6933 - acc: 0.5500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6695 - acc: 0.6025 - val_loss: 0.6933 - val_acc: 0.5500\n",
            "Epoch 23/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6923 - acc: 0.5600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6677 - acc: 0.6325 - val_loss: 0.6923 - val_acc: 0.5600\n",
            "Epoch 24/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6911 - acc: 0.5500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6666 - acc: 0.6150 - val_loss: 0.6911 - val_acc: 0.5500\n",
            "Epoch 25/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6912 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6665 - acc: 0.6150 - val_loss: 0.6912 - val_acc: 0.6000\n",
            "Epoch 26/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6907 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6650 - acc: 0.6225 - val_loss: 0.6907 - val_acc: 0.6000\n",
            "Epoch 27/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6898 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6635 - acc: 0.6125 - val_loss: 0.6898 - val_acc: 0.6000\n",
            "Epoch 28/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6893 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6637 - acc: 0.6300 - val_loss: 0.6893 - val_acc: 0.6100\n",
            "Epoch 29/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6882 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6598 - acc: 0.6200 - val_loss: 0.6882 - val_acc: 0.6000\n",
            "Epoch 30/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6875 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6601 - acc: 0.6400 - val_loss: 0.6875 - val_acc: 0.6100\n",
            "Epoch 31/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6873 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6577 - acc: 0.6550 - val_loss: 0.6873 - val_acc: 0.6200\n",
            "Epoch 32/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6862 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6563 - acc: 0.6525 - val_loss: 0.6862 - val_acc: 0.5900\n",
            "Epoch 33/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6853 - acc: 0.5800\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6569 - acc: 0.6425 - val_loss: 0.6853 - val_acc: 0.5800\n",
            "Epoch 34/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6848 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6554 - acc: 0.6475 - val_loss: 0.6848 - val_acc: 0.5900\n",
            "Epoch 35/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6840 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6546 - acc: 0.6625 - val_loss: 0.6840 - val_acc: 0.6100\n",
            "Epoch 36/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6834 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6519 - acc: 0.6825 - val_loss: 0.6834 - val_acc: 0.6100\n",
            "Epoch 37/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6827 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6523 - acc: 0.6675 - val_loss: 0.6827 - val_acc: 0.6000\n",
            "Epoch 38/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6822 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6497 - acc: 0.6650 - val_loss: 0.6822 - val_acc: 0.6200\n",
            "Epoch 39/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6820 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6525 - acc: 0.6525 - val_loss: 0.6820 - val_acc: 0.6300\n",
            "Epoch 40/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6815 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6483 - acc: 0.6700 - val_loss: 0.6815 - val_acc: 0.6000\n",
            "Epoch 41/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6814 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6477 - acc: 0.6825 - val_loss: 0.6814 - val_acc: 0.6200\n",
            "Epoch 42/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6811 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6471 - acc: 0.6750 - val_loss: 0.6811 - val_acc: 0.6100\n",
            "Epoch 43/150\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6808 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6458 - acc: 0.6750 - val_loss: 0.6808 - val_acc: 0.6000\n",
            "Epoch 44/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6808 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6451 - acc: 0.6625 - val_loss: 0.6808 - val_acc: 0.6400\n",
            "Epoch 45/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6805 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6438 - acc: 0.6750 - val_loss: 0.6805 - val_acc: 0.6400\n",
            "Epoch 46/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6802 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6412 - acc: 0.6900 - val_loss: 0.6802 - val_acc: 0.6400\n",
            "Epoch 47/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6799 - acc: 0.5800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6422 - acc: 0.6975 - val_loss: 0.6799 - val_acc: 0.5800\n",
            "Epoch 48/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6796 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6439 - acc: 0.6800 - val_loss: 0.6796 - val_acc: 0.6400\n",
            "Epoch 49/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6793 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6398 - acc: 0.6700 - val_loss: 0.6793 - val_acc: 0.6300\n",
            "Epoch 50/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6791 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6372 - acc: 0.6825 - val_loss: 0.6791 - val_acc: 0.6400\n",
            "Epoch 51/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6787 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6374 - acc: 0.6800 - val_loss: 0.6787 - val_acc: 0.6400\n",
            "Epoch 52/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6788 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6385 - acc: 0.6725 - val_loss: 0.6788 - val_acc: 0.6400\n",
            "Epoch 53/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6785 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6352 - acc: 0.6925 - val_loss: 0.6785 - val_acc: 0.6400\n",
            "Epoch 54/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6781 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6365 - acc: 0.6900 - val_loss: 0.6781 - val_acc: 0.6400\n",
            "Epoch 55/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6779 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6343 - acc: 0.6925 - val_loss: 0.6779 - val_acc: 0.6500\n",
            "Epoch 56/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6773 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6318 - acc: 0.6900 - val_loss: 0.6773 - val_acc: 0.6300\n",
            "Epoch 57/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6770 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6332 - acc: 0.7025 - val_loss: 0.6770 - val_acc: 0.6300\n",
            "Epoch 58/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6768 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6305 - acc: 0.6825 - val_loss: 0.6768 - val_acc: 0.6400\n",
            "Epoch 59/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6764 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6286 - acc: 0.7000 - val_loss: 0.6764 - val_acc: 0.6300\n",
            "Epoch 60/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6764 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6325 - acc: 0.6575 - val_loss: 0.6764 - val_acc: 0.6400\n",
            "Epoch 61/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6759 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6267 - acc: 0.6925 - val_loss: 0.6759 - val_acc: 0.6400\n",
            "Epoch 62/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6756 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6287 - acc: 0.7000 - val_loss: 0.6756 - val_acc: 0.6000\n",
            "Epoch 63/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6752 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6263 - acc: 0.6875 - val_loss: 0.6752 - val_acc: 0.6200\n",
            "Epoch 64/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6748 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6235 - acc: 0.7025 - val_loss: 0.6748 - val_acc: 0.6300\n",
            "Epoch 65/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6745 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6257 - acc: 0.6925 - val_loss: 0.6745 - val_acc: 0.6500\n",
            "Epoch 66/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6741 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6246 - acc: 0.7200 - val_loss: 0.6741 - val_acc: 0.6200\n",
            "Epoch 67/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6738 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6226 - acc: 0.6950 - val_loss: 0.6738 - val_acc: 0.6400\n",
            "Epoch 68/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6737 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6211 - acc: 0.7125 - val_loss: 0.6737 - val_acc: 0.6100\n",
            "Epoch 69/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6732 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6199 - acc: 0.7025 - val_loss: 0.6732 - val_acc: 0.6100\n",
            "Epoch 70/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6729 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6206 - acc: 0.7000 - val_loss: 0.6729 - val_acc: 0.6500\n",
            "Epoch 71/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6726 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6194 - acc: 0.7125 - val_loss: 0.6726 - val_acc: 0.6100\n",
            "Epoch 72/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6723 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6167 - acc: 0.7050 - val_loss: 0.6723 - val_acc: 0.6100\n",
            "Epoch 73/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6721 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6163 - acc: 0.7025 - val_loss: 0.6721 - val_acc: 0.6500\n",
            "Epoch 74/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6719 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6176 - acc: 0.7025 - val_loss: 0.6719 - val_acc: 0.6100\n",
            "Epoch 75/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6720 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6182 - acc: 0.7125 - val_loss: 0.6720 - val_acc: 0.6000\n",
            "Epoch 76/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6716 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6174 - acc: 0.7075 - val_loss: 0.6716 - val_acc: 0.6500\n",
            "Epoch 77/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6714 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6119 - acc: 0.7125 - val_loss: 0.6714 - val_acc: 0.6500\n",
            "Epoch 78/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6712 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6156 - acc: 0.7025 - val_loss: 0.6712 - val_acc: 0.6300\n",
            "Epoch 79/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6710 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6111 - acc: 0.7125 - val_loss: 0.6710 - val_acc: 0.6100\n",
            "Epoch 80/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6708 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6146 - acc: 0.7100 - val_loss: 0.6708 - val_acc: 0.6100\n",
            "Epoch 81/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6706 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6105 - acc: 0.7150 - val_loss: 0.6706 - val_acc: 0.6400\n",
            "Epoch 82/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6705 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6084 - acc: 0.7200 - val_loss: 0.6705 - val_acc: 0.6400\n",
            "Epoch 83/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6703 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6095 - acc: 0.7150 - val_loss: 0.6703 - val_acc: 0.6400\n",
            "Epoch 84/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6702 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6074 - acc: 0.7225 - val_loss: 0.6702 - val_acc: 0.6100\n",
            "Epoch 85/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6699 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6083 - acc: 0.7125 - val_loss: 0.6699 - val_acc: 0.6400\n",
            "Epoch 86/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6698 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6076 - acc: 0.7225 - val_loss: 0.6698 - val_acc: 0.6000\n",
            "Epoch 87/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6694 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6063 - acc: 0.7150 - val_loss: 0.6694 - val_acc: 0.6400\n",
            "Epoch 88/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6693 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6059 - acc: 0.7075 - val_loss: 0.6693 - val_acc: 0.6400\n",
            "Epoch 89/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6691 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6035 - acc: 0.7225 - val_loss: 0.6691 - val_acc: 0.6400\n",
            "Epoch 90/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6690 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6041 - acc: 0.7225 - val_loss: 0.6690 - val_acc: 0.6300\n",
            "Epoch 91/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6688 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6034 - acc: 0.7325 - val_loss: 0.6688 - val_acc: 0.6300\n",
            "Epoch 92/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6686 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6008 - acc: 0.7150 - val_loss: 0.6686 - val_acc: 0.6400\n",
            "Epoch 93/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6684 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5987 - acc: 0.7200 - val_loss: 0.6684 - val_acc: 0.6300\n",
            "Epoch 94/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6686 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5987 - acc: 0.7300 - val_loss: 0.6686 - val_acc: 0.6000\n",
            "Epoch 95/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6683 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6027 - acc: 0.7175 - val_loss: 0.6683 - val_acc: 0.6000\n",
            "Epoch 96/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6682 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5980 - acc: 0.7250 - val_loss: 0.6682 - val_acc: 0.6100\n",
            "Epoch 97/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6683 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6007 - acc: 0.7250 - val_loss: 0.6683 - val_acc: 0.6000\n",
            "Epoch 98/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6680 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5958 - acc: 0.7275 - val_loss: 0.6680 - val_acc: 0.6300\n",
            "Epoch 99/150\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6680 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5986 - acc: 0.7250 - val_loss: 0.6680 - val_acc: 0.6600\n",
            "Epoch 100/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6678 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5994 - acc: 0.7125 - val_loss: 0.6678 - val_acc: 0.6300\n",
            "Epoch 101/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6677 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5974 - acc: 0.7350 - val_loss: 0.6677 - val_acc: 0.6500\n",
            "Epoch 102/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6677 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5951 - acc: 0.7275 - val_loss: 0.6677 - val_acc: 0.6300\n",
            "Epoch 103/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6675 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5947 - acc: 0.7100 - val_loss: 0.6675 - val_acc: 0.6300\n",
            "Epoch 104/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6674 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5942 - acc: 0.7325 - val_loss: 0.6674 - val_acc: 0.6500\n",
            "Epoch 105/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6673 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5936 - acc: 0.7200 - val_loss: 0.6673 - val_acc: 0.6500\n",
            "Epoch 106/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6675 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5898 - acc: 0.7350 - val_loss: 0.6675 - val_acc: 0.6000\n",
            "Epoch 107/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6672 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5903 - acc: 0.7275 - val_loss: 0.6672 - val_acc: 0.6100\n",
            "Epoch 108/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6670 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5923 - acc: 0.7325 - val_loss: 0.6670 - val_acc: 0.6200\n",
            "Epoch 109/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6668 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5892 - acc: 0.7350 - val_loss: 0.6668 - val_acc: 0.6300\n",
            "Epoch 110/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6667 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5892 - acc: 0.7250 - val_loss: 0.6667 - val_acc: 0.6600\n",
            "Epoch 111/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6667 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5866 - acc: 0.7350 - val_loss: 0.6667 - val_acc: 0.6100\n",
            "Epoch 112/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6666 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5870 - acc: 0.7375 - val_loss: 0.6666 - val_acc: 0.6100\n",
            "Epoch 113/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6665 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5898 - acc: 0.7175 - val_loss: 0.6665 - val_acc: 0.6600\n",
            "Epoch 114/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6663 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5835 - acc: 0.7400 - val_loss: 0.6663 - val_acc: 0.6400\n",
            "Epoch 115/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6663 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5856 - acc: 0.7450 - val_loss: 0.6663 - val_acc: 0.6100\n",
            "Epoch 116/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6663 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5859 - acc: 0.7300 - val_loss: 0.6663 - val_acc: 0.6200\n",
            "Epoch 117/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6659 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5847 - acc: 0.7400 - val_loss: 0.6659 - val_acc: 0.6400\n",
            "Epoch 118/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6661 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5836 - acc: 0.7500 - val_loss: 0.6661 - val_acc: 0.6200\n",
            "Epoch 119/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6658 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5811 - acc: 0.7375 - val_loss: 0.6658 - val_acc: 0.6100\n",
            "Epoch 120/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6657 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5842 - acc: 0.7300 - val_loss: 0.6657 - val_acc: 0.6200\n",
            "Epoch 121/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6655 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5799 - acc: 0.7425 - val_loss: 0.6655 - val_acc: 0.6300\n",
            "Epoch 122/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6652 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5845 - acc: 0.7250 - val_loss: 0.6652 - val_acc: 0.6400\n",
            "Epoch 123/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6651 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5842 - acc: 0.7350 - val_loss: 0.6651 - val_acc: 0.6400\n",
            "Epoch 124/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6652 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5785 - acc: 0.7500 - val_loss: 0.6652 - val_acc: 0.6200\n",
            "Epoch 125/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6648 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5787 - acc: 0.7475 - val_loss: 0.6648 - val_acc: 0.6400\n",
            "Epoch 126/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6652 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5785 - acc: 0.7425 - val_loss: 0.6652 - val_acc: 0.6200\n",
            "Epoch 127/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6647 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5767 - acc: 0.7400 - val_loss: 0.6647 - val_acc: 0.6400\n",
            "Epoch 128/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6647 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5768 - acc: 0.7475 - val_loss: 0.6647 - val_acc: 0.6300\n",
            "Epoch 129/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6645 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5761 - acc: 0.7400 - val_loss: 0.6645 - val_acc: 0.6300\n",
            "Epoch 130/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6642 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5754 - acc: 0.7525 - val_loss: 0.6642 - val_acc: 0.6400\n",
            "Epoch 131/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6641 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5751 - acc: 0.7400 - val_loss: 0.6641 - val_acc: 0.6400\n",
            "Epoch 132/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6641 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5761 - acc: 0.7475 - val_loss: 0.6641 - val_acc: 0.6500\n",
            "Epoch 133/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6638 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5751 - acc: 0.7325 - val_loss: 0.6638 - val_acc: 0.6400\n",
            "Epoch 134/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6638 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5749 - acc: 0.7450 - val_loss: 0.6638 - val_acc: 0.6400\n",
            "Epoch 135/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6639 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5781 - acc: 0.7425 - val_loss: 0.6639 - val_acc: 0.6300\n",
            "Epoch 136/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6636 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5732 - acc: 0.7250 - val_loss: 0.6636 - val_acc: 0.6400\n",
            "Epoch 137/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6634 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5734 - acc: 0.7475 - val_loss: 0.6634 - val_acc: 0.6400\n",
            "Epoch 138/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6633 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5733 - acc: 0.7475 - val_loss: 0.6633 - val_acc: 0.6400\n",
            "Epoch 139/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6631 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5724 - acc: 0.7600 - val_loss: 0.6631 - val_acc: 0.6400\n",
            "Epoch 140/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6631 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5750 - acc: 0.7350 - val_loss: 0.6631 - val_acc: 0.6400\n",
            "Epoch 141/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6630 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5710 - acc: 0.7475 - val_loss: 0.6630 - val_acc: 0.6400\n",
            "Epoch 142/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6629 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5697 - acc: 0.7425 - val_loss: 0.6629 - val_acc: 0.6400\n",
            "Epoch 143/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6627 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5686 - acc: 0.7525 - val_loss: 0.6627 - val_acc: 0.6400\n",
            "Epoch 144/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6629 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5720 - acc: 0.7550 - val_loss: 0.6629 - val_acc: 0.6500\n",
            "Epoch 145/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6629 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5695 - acc: 0.7375 - val_loss: 0.6629 - val_acc: 0.6400\n",
            "Epoch 146/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6624 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5667 - acc: 0.7475 - val_loss: 0.6624 - val_acc: 0.6400\n",
            "Epoch 147/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6625 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5653 - acc: 0.7475 - val_loss: 0.6625 - val_acc: 0.6500\n",
            "Epoch 148/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6627 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5685 - acc: 0.7600 - val_loss: 0.6627 - val_acc: 0.6400\n",
            "Epoch 149/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6621 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5665 - acc: 0.7450 - val_loss: 0.6621 - val_acc: 0.6400\n",
            "Epoch 150/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6620 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5639 - acc: 0.7550 - val_loss: 0.6620 - val_acc: 0.6400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgtyZLsvG_ls",
        "colab_type": "text"
      },
      "source": [
        "**Learning curves**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-ketbynHDhE",
        "colab_type": "code",
        "outputId": "bbcd355f-ef49-4d0a-a243-bb8f8b534b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "#get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy - 4')\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.title('Training and Validation Loss - 4')\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRUREUsT/BzqttuNfc3EeAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POHLQd9PPMLX",
        "colab_type": "code",
        "outputId": "3f89db97-840f-4257-84f2-fb4e9ed17e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "_, acc = model.evaluate_generator(generator=train_generator)\n",
        "print('Accuracy on training set: {}%'.format(round(acc, 3)*100))\n",
        "\n",
        "_, acc = model.evaluate_generator(generator=validation_generator)\n",
        "print('Accuracy on validation set: {}%'.format(round(acc, 3)*100))\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set: 75.49999952316284%\n",
            "Accuracy on validation set: 63.999998569488525%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeN1ZqTAM42i",
        "colab_type": "text"
      },
      "source": [
        "**Export Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkUfyxqDM3NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keras library import  for Saving and loading model and weights\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwvXWQnGEENs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "model_json = model.to_json()\n",
        "\n",
        "\n",
        "with open(\"drive/Colab Notebooks/model/4_model_num.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"drive/Colab Notebooks/model/4_model_num.h5\")\n",
        "model.save('drive/Colab Notebooks/model/4_model_num.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}