{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Housing 5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caroref/model/blob/master/Housing_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ascUYWYXFlTS",
        "colab_type": "code",
        "outputId": "7e313837-0495-4d50-c8a2-3d38d1b66579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TensorFlow version is \", tf.__version__)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0606 14:43:27.943712 140621293619072 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version is  1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs-4SeS3Fpc5",
        "colab_type": "code",
        "outputId": "7103bec3-7f54-4ca1-9656-aa8fee746862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Allow Colab to access drive\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "#linking Colab to use drive as if local\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDNLdWVxP2jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main directories\n",
        "trn_dir = 'drive/Colab/Final/Test5/Train/'\n",
        "tst_dir = 'drive/Colab/Final/Test5/Test/'\n",
        "\n",
        "#subdirectories\n",
        "trn_like = os.path.join(trn_dir, 'Like')\n",
        "tst_like = os.path.join(tst_dir, 'Like')\n",
        "\n",
        "trn_disl = os.path.join(trn_dir, 'Dislike')\n",
        "tst_disl = os.path.join(tst_dir, 'Dislike')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo3GVjj1FsEn",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5BtzOSmbZB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All images will be resized to image_size\n",
        "\n",
        "image_size = 640 \n",
        "batch_size = 100\n",
        "\n",
        "# create Image Data Generator for Image Data Augmentation\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "                rescale=1./255, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTvB7EieGDAN",
        "colab_type": "code",
        "outputId": "4a21755a-2bc2-470f-cc1e-47f55d2db771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Create the train and validation generators and specify the train dataset directory, \n",
        "# image size, batch size, binary classification, and subset (training or validation).\n",
        "\n",
        "\n",
        "# Flow training images in batches using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,  \n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode ='binary',\n",
        "                subset = 'training') \n",
        "\n",
        "# Flow validation images in batches using validation_datagen generator\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,\n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode='binary',\n",
        "                subset = 'validation')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ur5r8uGWQO",
        "colab_type": "text"
      },
      "source": [
        "# Create the base model from the pre-trained convnets\n",
        "\n",
        "Using the output of last layer (bottleneck layer). Keep other layers frozen (these are specific to their training, less useful here?) Bottleneck layer should be more general.\n",
        "\n",
        "Using Inception v3 model with weights trained on ImageNet.\n",
        "The argument, include_top=False, means that we have a network that isn't including classification layers at the top. This is good for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCLmagoGbc3",
        "colab_type": "code",
        "outputId": "46497249-dfec-49b6-8aaf-c16e672284f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "IMG_SHAPE = (image_size, image_size, 3)\n",
        "\n",
        "base_model = keras.applications.inception_v3.InceptionV3(\n",
        "             include_top=False, weights='imagenet', input_shape=IMG_SHAPE)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0606 14:46:07.588779 140621293619072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wXzV2zlGduw",
        "colab_type": "text"
      },
      "source": [
        "# Feature extraction\n",
        "We will freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.\n",
        "\n",
        "It's important to freeze the convolutional based before we compile and train the model. By freezing (or setting layer.trainable = False), we prevent the weights in these layers from being updated during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCk3KhmGGhaH",
        "colab_type": "code",
        "outputId": "678f1412-d082-42b7-ee9d-d97bf52b41fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11821
        }
      },
      "source": [
        "#freeze base\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "# Look at the base model architecture\n",
        "base_model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 640, 640, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 319, 319, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 319, 319, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 319, 319, 32) 0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 317, 317, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 317, 317, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 317, 317, 32) 0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 317, 317, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 317, 317, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 317, 317, 64) 0           batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 158, 158, 64) 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 158, 158, 80) 5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_3 (Batch (None, 158, 158, 80) 240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 158, 158, 80) 0           batch_normalization_v1_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 156, 156, 192 138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 156, 156, 192 576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 156, 156, 192 0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 77, 77, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_8 (Batch (None, 77, 77, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 77, 77, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 77, 77, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_6 (Batch (None, 77, 77, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_9 (Batch (None, 77, 77, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 77, 77, 48)   0           batch_normalization_v1_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 77, 77, 96)   0           batch_normalization_v1_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 77, 77, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 77, 77, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 77, 77, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 77, 77, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_5 (Batch (None, 77, 77, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_7 (Batch (None, 77, 77, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_10 (Batc (None, 77, 77, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_11 (Batc (None, 77, 77, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_10[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 77, 77, 32)   0           batch_normalization_v1_11[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 77, 77, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_15 (Batc (None, 77, 77, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_15[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 77, 77, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 77, 77, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_13 (Batc (None, 77, 77, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_16 (Batc (None, 77, 77, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_13[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_16[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 77, 77, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 77, 77, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 77, 77, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 77, 77, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_12 (Batc (None, 77, 77, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_14 (Batc (None, 77, 77, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_17 (Batc (None, 77, 77, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_18 (Batc (None, 77, 77, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_12[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_14[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_17[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_18[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 77, 77, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_22 (Batc (None, 77, 77, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_22[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 77, 77, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 77, 77, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_20 (Batc (None, 77, 77, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_23 (Batc (None, 77, 77, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_20[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_23[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 77, 77, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 77, 77, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 77, 77, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 77, 77, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_19 (Batc (None, 77, 77, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_21 (Batc (None, 77, 77, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_24 (Batc (None, 77, 77, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_25 (Batc (None, 77, 77, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_19[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_21[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_24[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_25[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 77, 77, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 77, 77, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_27 (Batc (None, 77, 77, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_27[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 77, 77, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_28 (Batc (None, 77, 77, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_28[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 38, 38, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 38, 38, 96)   82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_26 (Batc (None, 38, 38, 384)  1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_29 (Batc (None, 38, 38, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 38, 38, 384)  0           batch_normalization_v1_26[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 38, 38, 96)   0           batch_normalization_v1_29[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 38, 38, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 38, 38, 768)  0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_34 (Batc (None, 38, 38, 128)  384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_34[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 38, 38, 128)  114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_35 (Batc (None, 38, 38, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_35[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 38, 38, 128)  114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_31 (Batc (None, 38, 38, 128)  384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_36 (Batc (None, 38, 38, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_31[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_36[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 38, 38, 128)  114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 38, 38, 128)  114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_32 (Batc (None, 38, 38, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_37 (Batc (None, 38, 38, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_32[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_37[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 38, 38, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 38, 38, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 38, 38, 192)  172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 38, 38, 192)  172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_30 (Batc (None, 38, 38, 192)  576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_33 (Batc (None, 38, 38, 192)  576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_38 (Batc (None, 38, 38, 192)  576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_39 (Batc (None, 38, 38, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_30[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_33[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_38[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_39[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 38, 38, 768)  0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_44 (Batc (None, 38, 38, 160)  480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_44[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 38, 38, 160)  179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_45 (Batc (None, 38, 38, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_45[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 38, 38, 160)  179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_41 (Batc (None, 38, 38, 160)  480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_46 (Batc (None, 38, 38, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_41[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_46[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 38, 38, 160)  179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 38, 38, 160)  179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_42 (Batc (None, 38, 38, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_47 (Batc (None, 38, 38, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_42[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_47[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 38, 38, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 38, 38, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 38, 38, 192)  215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 38, 38, 192)  215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_40 (Batc (None, 38, 38, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_43 (Batc (None, 38, 38, 192)  576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_48 (Batc (None, 38, 38, 192)  576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_49 (Batc (None, 38, 38, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_40[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_43[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_48[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_49[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 38, 38, 768)  0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_54 (Batc (None, 38, 38, 160)  480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_54[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 38, 38, 160)  179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_55 (Batc (None, 38, 38, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_55[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 38, 38, 160)  179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_51 (Batc (None, 38, 38, 160)  480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_56 (Batc (None, 38, 38, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_51[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_56[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 38, 38, 160)  179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 38, 38, 160)  179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_52 (Batc (None, 38, 38, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_57 (Batc (None, 38, 38, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_52[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_57[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 38, 38, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 38, 38, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 38, 38, 192)  215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 38, 38, 192)  215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_50 (Batc (None, 38, 38, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_53 (Batc (None, 38, 38, 192)  576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_58 (Batc (None, 38, 38, 192)  576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_59 (Batc (None, 38, 38, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_50[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_53[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_58[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_59[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 38, 38, 768)  0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_64 (Batc (None, 38, 38, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_64[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 38, 38, 192)  258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_65 (Batc (None, 38, 38, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_65[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 38, 38, 192)  258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_61 (Batc (None, 38, 38, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_66 (Batc (None, 38, 38, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_61[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_66[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 38, 38, 192)  258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 38, 38, 192)  258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_62 (Batc (None, 38, 38, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_67 (Batc (None, 38, 38, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_62[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_67[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 38, 38, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 38, 38, 192)  258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 38, 38, 192)  258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_60 (Batc (None, 38, 38, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_63 (Batc (None, 38, 38, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_68 (Batc (None, 38, 38, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_69 (Batc (None, 38, 38, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_60[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_63[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_68[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_69[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 38, 38, 768)  0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_72 (Batc (None, 38, 38, 192)  576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_72[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 38, 38, 192)  258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_73 (Batc (None, 38, 38, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_73[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 38, 38, 192)  258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_70 (Batc (None, 38, 38, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_74 (Batc (None, 38, 38, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_70[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_74[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 18, 18, 320)  552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 18, 18, 192)  331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_71 (Batc (None, 18, 18, 320)  960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_75 (Batc (None, 18, 18, 192)  576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_71[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_75[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 18, 18, 768)  0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 18, 18, 1280) 0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 18, 18, 448)  573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_80 (Batc (None, 18, 18, 448)  1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_80[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 18, 18, 384)  491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 18, 18, 384)  1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_77 (Batc (None, 18, 18, 384)  1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_81 (Batc (None, 18, 18, 384)  1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_77[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_81[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 18, 18, 1280) 0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 18, 18, 320)  409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_78 (Batc (None, 18, 18, 384)  1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_79 (Batc (None, 18, 18, 384)  1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_82 (Batc (None, 18, 18, 384)  1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_83 (Batc (None, 18, 18, 384)  1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 18, 18, 192)  245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_76 (Batc (None, 18, 18, 320)  960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_78[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_79[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_82[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_83[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_84 (Batc (None, 18, 18, 192)  576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_76[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 18, 18, 768)  0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 18, 18, 768)  0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_84[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 18, 18, 2048) 0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 18, 18, 448)  917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_89 (Batc (None, 18, 18, 448)  1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_89[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 18, 18, 384)  786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 18, 18, 384)  1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_86 (Batc (None, 18, 18, 384)  1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_90 (Batc (None, 18, 18, 384)  1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_86[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_90[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 18, 18, 2048) 0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 18, 18, 320)  655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_87 (Batc (None, 18, 18, 384)  1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_88 (Batc (None, 18, 18, 384)  1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_91 (Batc (None, 18, 18, 384)  1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_92 (Batc (None, 18, 18, 384)  1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 18, 18, 192)  393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_85 (Batc (None, 18, 18, 320)  960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_87[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_88[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_91[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_92[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_93 (Batc (None, 18, 18, 192)  576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_85[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 18, 18, 768)  0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 18, 18, 768)  0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_93[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 18, 18, 2048) 0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 0\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUc1ad5qGnMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add a classification head\n",
        "#adding a few layers on top of the base model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  keras.layers.GlobalAveragePooling2D(),\n",
        "  keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "#relatively basic layer. no deep neural networks (all happening beforehand). Similar to a logit regression. plain classification\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbhLA3TGr1X",
        "colab_type": "code",
        "outputId": "143318d3-393a-4e63-afd9-1ed6662b94bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "#Compile the model (Must do before training!)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 18, 18, 2048)      21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 21,804,833\n",
            "Trainable params: 2,049\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th655DVEGwop",
        "colab_type": "code",
        "outputId": "341546de-80cf-4d16-a0a5-f9a7cee71edb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#These 2K trainable parameters are divided among 2 TensorFlow Variable objects, the weights and biases of the two dense layers:\n",
        "\n",
        "\n",
        "len(model.trainable_variables)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZeMfEwoG3wm",
        "colab_type": "text"
      },
      "source": [
        "**Train the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNgpgcJjG8xl",
        "colab_type": "code",
        "outputId": "cd2e1289-9618-41d4-a1ad-3f667224cda0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8144
        }
      },
      "source": [
        "epochs = 150\n",
        "steps_per_epoch = train_generator.n // batch_size\n",
        "validation_steps = validation_generator.n // batch_size\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch = steps_per_epoch,\n",
        "                              epochs=epochs,\n",
        "                              workers=4,\n",
        "                              validation_data=validation_generator,\n",
        "                              validation_steps=validation_steps)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0606 14:46:27.711943 140621293619072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "1/1 [==============================] - 66s 66s/step - loss: 0.6942 - acc: 0.4900\n",
            "4/4 [==============================] - 154s 39s/step - loss: 0.7071 - acc: 0.4250 - val_loss: 0.6942 - val_acc: 0.4900\n",
            "Epoch 2/150\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6937 - acc: 0.5000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.7001 - acc: 0.4600 - val_loss: 0.6937 - val_acc: 0.5000\n",
            "Epoch 3/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6935 - acc: 0.5000\n",
            "4/4 [==============================] - 12s 3s/step - loss: 0.6991 - acc: 0.4725 - val_loss: 0.6935 - val_acc: 0.5000\n",
            "Epoch 4/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6919 - acc: 0.5100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6975 - acc: 0.4625 - val_loss: 0.6919 - val_acc: 0.5100\n",
            "Epoch 5/150\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6910 - acc: 0.5000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6955 - acc: 0.4800 - val_loss: 0.6910 - val_acc: 0.5000\n",
            "Epoch 6/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6905 - acc: 0.4900\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6927 - acc: 0.5125 - val_loss: 0.6905 - val_acc: 0.4900\n",
            "Epoch 7/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6895 - acc: 0.5100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6913 - acc: 0.5000 - val_loss: 0.6895 - val_acc: 0.5100\n",
            "Epoch 8/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6894 - acc: 0.5100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6904 - acc: 0.5250 - val_loss: 0.6894 - val_acc: 0.5100\n",
            "Epoch 9/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6877 - acc: 0.5000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6897 - acc: 0.5275 - val_loss: 0.6877 - val_acc: 0.5000\n",
            "Epoch 10/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6865 - acc: 0.4800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6857 - acc: 0.5525 - val_loss: 0.6865 - val_acc: 0.4800\n",
            "Epoch 11/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6861 - acc: 0.4700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6899 - acc: 0.5275 - val_loss: 0.6861 - val_acc: 0.4700\n",
            "Epoch 12/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6855 - acc: 0.4600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6856 - acc: 0.5600 - val_loss: 0.6855 - val_acc: 0.4600\n",
            "Epoch 13/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6844 - acc: 0.4700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6869 - acc: 0.5450 - val_loss: 0.6844 - val_acc: 0.4700\n",
            "Epoch 14/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6829 - acc: 0.5000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6847 - acc: 0.5550 - val_loss: 0.6829 - val_acc: 0.5000\n",
            "Epoch 15/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6823 - acc: 0.4700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6819 - acc: 0.5675 - val_loss: 0.6823 - val_acc: 0.4700\n",
            "Epoch 16/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6815 - acc: 0.4800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6803 - acc: 0.5775 - val_loss: 0.6815 - val_acc: 0.4800\n",
            "Epoch 17/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6803 - acc: 0.5000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6785 - acc: 0.5975 - val_loss: 0.6803 - val_acc: 0.5000\n",
            "Epoch 18/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6794 - acc: 0.5000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6783 - acc: 0.5600 - val_loss: 0.6794 - val_acc: 0.5000\n",
            "Epoch 19/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6787 - acc: 0.5100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6785 - acc: 0.5975 - val_loss: 0.6787 - val_acc: 0.5100\n",
            "Epoch 20/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6778 - acc: 0.5400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6749 - acc: 0.6075 - val_loss: 0.6778 - val_acc: 0.5400\n",
            "Epoch 21/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6769 - acc: 0.5300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6745 - acc: 0.6000 - val_loss: 0.6769 - val_acc: 0.5300\n",
            "Epoch 22/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6763 - acc: 0.5600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6729 - acc: 0.6525 - val_loss: 0.6763 - val_acc: 0.5600\n",
            "Epoch 23/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6756 - acc: 0.5600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6723 - acc: 0.6225 - val_loss: 0.6756 - val_acc: 0.5600\n",
            "Epoch 24/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6749 - acc: 0.5700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6695 - acc: 0.6275 - val_loss: 0.6749 - val_acc: 0.5700\n",
            "Epoch 25/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6743 - acc: 0.5800\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6717 - acc: 0.5900 - val_loss: 0.6743 - val_acc: 0.5800\n",
            "Epoch 26/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6737 - acc: 0.5800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6677 - acc: 0.6475 - val_loss: 0.6737 - val_acc: 0.5800\n",
            "Epoch 27/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6732 - acc: 0.5800\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6686 - acc: 0.6125 - val_loss: 0.6732 - val_acc: 0.5800\n",
            "Epoch 28/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6727 - acc: 0.5900\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6669 - acc: 0.6350 - val_loss: 0.6727 - val_acc: 0.5900\n",
            "Epoch 29/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6723 - acc: 0.5900\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6645 - acc: 0.6500 - val_loss: 0.6723 - val_acc: 0.5900\n",
            "Epoch 30/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6717 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6649 - acc: 0.6325 - val_loss: 0.6717 - val_acc: 0.6000\n",
            "Epoch 31/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6711 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6622 - acc: 0.6500 - val_loss: 0.6711 - val_acc: 0.6000\n",
            "Epoch 32/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6707 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6615 - acc: 0.6600 - val_loss: 0.6707 - val_acc: 0.6100\n",
            "Epoch 33/150\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6700 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6595 - acc: 0.6600 - val_loss: 0.6700 - val_acc: 0.6100\n",
            "Epoch 34/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6692 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6589 - acc: 0.6550 - val_loss: 0.6692 - val_acc: 0.6000\n",
            "Epoch 35/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6685 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6611 - acc: 0.6225 - val_loss: 0.6685 - val_acc: 0.6200\n",
            "Epoch 36/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6679 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6566 - acc: 0.6700 - val_loss: 0.6679 - val_acc: 0.6100\n",
            "Epoch 37/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6673 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6578 - acc: 0.6575 - val_loss: 0.6673 - val_acc: 0.6100\n",
            "Epoch 38/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6666 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6540 - acc: 0.6850 - val_loss: 0.6666 - val_acc: 0.6100\n",
            "Epoch 39/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6660 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6528 - acc: 0.6775 - val_loss: 0.6660 - val_acc: 0.6300\n",
            "Epoch 40/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6656 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6512 - acc: 0.6650 - val_loss: 0.6656 - val_acc: 0.6200\n",
            "Epoch 41/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6651 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6512 - acc: 0.6850 - val_loss: 0.6651 - val_acc: 0.6200\n",
            "Epoch 42/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6647 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6501 - acc: 0.6775 - val_loss: 0.6647 - val_acc: 0.6600\n",
            "Epoch 43/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6646 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6473 - acc: 0.6950 - val_loss: 0.6646 - val_acc: 0.6300\n",
            "Epoch 44/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6643 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6481 - acc: 0.6750 - val_loss: 0.6643 - val_acc: 0.6700\n",
            "Epoch 45/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6640 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6481 - acc: 0.6750 - val_loss: 0.6640 - val_acc: 0.6400\n",
            "Epoch 46/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6638 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6472 - acc: 0.6850 - val_loss: 0.6638 - val_acc: 0.6700\n",
            "Epoch 47/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6635 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6444 - acc: 0.7050 - val_loss: 0.6635 - val_acc: 0.6600\n",
            "Epoch 48/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6631 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6418 - acc: 0.7100 - val_loss: 0.6631 - val_acc: 0.6300\n",
            "Epoch 49/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6627 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6421 - acc: 0.7025 - val_loss: 0.6627 - val_acc: 0.6600\n",
            "Epoch 50/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6623 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6419 - acc: 0.7050 - val_loss: 0.6623 - val_acc: 0.6700\n",
            "Epoch 51/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6619 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6414 - acc: 0.6825 - val_loss: 0.6619 - val_acc: 0.6500\n",
            "Epoch 52/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6616 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6385 - acc: 0.7125 - val_loss: 0.6616 - val_acc: 0.6400\n",
            "Epoch 53/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6613 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6385 - acc: 0.6975 - val_loss: 0.6613 - val_acc: 0.6500\n",
            "Epoch 54/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6612 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6380 - acc: 0.7000 - val_loss: 0.6612 - val_acc: 0.6300\n",
            "Epoch 55/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6615 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6360 - acc: 0.7000 - val_loss: 0.6615 - val_acc: 0.6100\n",
            "Epoch 56/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6610 - acc: 0.6200\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6348 - acc: 0.7175 - val_loss: 0.6610 - val_acc: 0.6200\n",
            "Epoch 57/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6607 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6345 - acc: 0.6975 - val_loss: 0.6607 - val_acc: 0.6500\n",
            "Epoch 58/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6606 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6349 - acc: 0.6975 - val_loss: 0.6606 - val_acc: 0.6500\n",
            "Epoch 59/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6605 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6316 - acc: 0.7200 - val_loss: 0.6605 - val_acc: 0.6300\n",
            "Epoch 60/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6604 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6322 - acc: 0.7325 - val_loss: 0.6604 - val_acc: 0.6300\n",
            "Epoch 61/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6602 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6304 - acc: 0.7350 - val_loss: 0.6602 - val_acc: 0.6300\n",
            "Epoch 62/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6602 - acc: 0.6800\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6294 - acc: 0.7275 - val_loss: 0.6602 - val_acc: 0.6800\n",
            "Epoch 63/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6601 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6285 - acc: 0.7275 - val_loss: 0.6601 - val_acc: 0.6100\n",
            "Epoch 64/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6598 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6286 - acc: 0.7200 - val_loss: 0.6598 - val_acc: 0.6300\n",
            "Epoch 65/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6597 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6266 - acc: 0.7175 - val_loss: 0.6597 - val_acc: 0.6600\n",
            "Epoch 66/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6596 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6263 - acc: 0.7300 - val_loss: 0.6596 - val_acc: 0.6300\n",
            "Epoch 67/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6596 - acc: 0.6900\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6248 - acc: 0.7075 - val_loss: 0.6596 - val_acc: 0.6900\n",
            "Epoch 68/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6595 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6255 - acc: 0.7350 - val_loss: 0.6595 - val_acc: 0.6300\n",
            "Epoch 69/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6597 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6224 - acc: 0.7375 - val_loss: 0.6597 - val_acc: 0.6200\n",
            "Epoch 70/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6594 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6215 - acc: 0.7300 - val_loss: 0.6594 - val_acc: 0.6100\n",
            "Epoch 71/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6593 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6223 - acc: 0.7450 - val_loss: 0.6593 - val_acc: 0.6400\n",
            "Epoch 72/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6592 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6194 - acc: 0.7525 - val_loss: 0.6592 - val_acc: 0.6500\n",
            "Epoch 73/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6594 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6210 - acc: 0.7325 - val_loss: 0.6594 - val_acc: 0.6000\n",
            "Epoch 74/150\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6590 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6191 - acc: 0.7350 - val_loss: 0.6590 - val_acc: 0.6300\n",
            "Epoch 75/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6591 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6183 - acc: 0.7425 - val_loss: 0.6591 - val_acc: 0.6100\n",
            "Epoch 76/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6590 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6196 - acc: 0.7250 - val_loss: 0.6590 - val_acc: 0.6100\n",
            "Epoch 77/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6585 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6159 - acc: 0.7300 - val_loss: 0.6585 - val_acc: 0.6500\n",
            "Epoch 78/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6584 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6196 - acc: 0.7425 - val_loss: 0.6584 - val_acc: 0.6500\n",
            "Epoch 79/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6585 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6168 - acc: 0.7175 - val_loss: 0.6585 - val_acc: 0.6300\n",
            "Epoch 80/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6584 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6142 - acc: 0.7275 - val_loss: 0.6584 - val_acc: 0.6500\n",
            "Epoch 81/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6585 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6176 - acc: 0.7125 - val_loss: 0.6585 - val_acc: 0.6600\n",
            "Epoch 82/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6584 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6138 - acc: 0.7325 - val_loss: 0.6584 - val_acc: 0.6400\n",
            "Epoch 83/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6584 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6110 - acc: 0.7475 - val_loss: 0.6584 - val_acc: 0.6400\n",
            "Epoch 84/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6585 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6132 - acc: 0.7475 - val_loss: 0.6585 - val_acc: 0.6400\n",
            "Epoch 85/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6586 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6117 - acc: 0.7525 - val_loss: 0.6586 - val_acc: 0.6300\n",
            "Epoch 86/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6584 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6088 - acc: 0.7400 - val_loss: 0.6584 - val_acc: 0.6400\n",
            "Epoch 87/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6585 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6092 - acc: 0.7325 - val_loss: 0.6585 - val_acc: 0.6600\n",
            "Epoch 88/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6584 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6093 - acc: 0.7475 - val_loss: 0.6584 - val_acc: 0.6700\n",
            "Epoch 89/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6584 - acc: 0.6400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6088 - acc: 0.7300 - val_loss: 0.6584 - val_acc: 0.6400\n",
            "Epoch 90/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6585 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6077 - acc: 0.7400 - val_loss: 0.6585 - val_acc: 0.6300\n",
            "Epoch 91/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6581 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6063 - acc: 0.7425 - val_loss: 0.6581 - val_acc: 0.6500\n",
            "Epoch 92/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6579 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6043 - acc: 0.7500 - val_loss: 0.6579 - val_acc: 0.6600\n",
            "Epoch 93/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6578 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6045 - acc: 0.7575 - val_loss: 0.6578 - val_acc: 0.6700\n",
            "Epoch 94/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6577 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6044 - acc: 0.7500 - val_loss: 0.6577 - val_acc: 0.6700\n",
            "Epoch 95/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6578 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6014 - acc: 0.7425 - val_loss: 0.6578 - val_acc: 0.6400\n",
            "Epoch 96/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6579 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6064 - acc: 0.7350 - val_loss: 0.6579 - val_acc: 0.6300\n",
            "Epoch 97/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6574 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6008 - acc: 0.7325 - val_loss: 0.6574 - val_acc: 0.6600\n",
            "Epoch 98/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6573 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5998 - acc: 0.7500 - val_loss: 0.6573 - val_acc: 0.6700\n",
            "Epoch 99/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6572 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6000 - acc: 0.7400 - val_loss: 0.6572 - val_acc: 0.6700\n",
            "Epoch 100/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6572 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5975 - acc: 0.7425 - val_loss: 0.6572 - val_acc: 0.6600\n",
            "Epoch 101/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6571 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6010 - acc: 0.7450 - val_loss: 0.6571 - val_acc: 0.6600\n",
            "Epoch 102/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6570 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6016 - acc: 0.7250 - val_loss: 0.6570 - val_acc: 0.6500\n",
            "Epoch 103/150\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6570 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6044 - acc: 0.7400 - val_loss: 0.6570 - val_acc: 0.6600\n",
            "Epoch 104/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6570 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5964 - acc: 0.7525 - val_loss: 0.6570 - val_acc: 0.6500\n",
            "Epoch 105/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6570 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5976 - acc: 0.7450 - val_loss: 0.6570 - val_acc: 0.6500\n",
            "Epoch 106/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6570 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5977 - acc: 0.7525 - val_loss: 0.6570 - val_acc: 0.6700\n",
            "Epoch 107/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6569 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5981 - acc: 0.7450 - val_loss: 0.6569 - val_acc: 0.6600\n",
            "Epoch 108/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6570 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5943 - acc: 0.7550 - val_loss: 0.6570 - val_acc: 0.6600\n",
            "Epoch 109/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6569 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5962 - acc: 0.7300 - val_loss: 0.6569 - val_acc: 0.6600\n",
            "Epoch 110/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6570 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5943 - acc: 0.7575 - val_loss: 0.6570 - val_acc: 0.6600\n",
            "Epoch 111/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6566 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5925 - acc: 0.7475 - val_loss: 0.6566 - val_acc: 0.6600\n",
            "Epoch 112/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6565 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5920 - acc: 0.7500 - val_loss: 0.6565 - val_acc: 0.6600\n",
            "Epoch 113/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6564 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5915 - acc: 0.7525 - val_loss: 0.6564 - val_acc: 0.6500\n",
            "Epoch 114/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6563 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5920 - acc: 0.7600 - val_loss: 0.6563 - val_acc: 0.6700\n",
            "Epoch 115/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6561 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5901 - acc: 0.7525 - val_loss: 0.6561 - val_acc: 0.6500\n",
            "Epoch 116/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6560 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5909 - acc: 0.7625 - val_loss: 0.6560 - val_acc: 0.6600\n",
            "Epoch 117/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6558 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5895 - acc: 0.7525 - val_loss: 0.6558 - val_acc: 0.6500\n",
            "Epoch 118/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6559 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5903 - acc: 0.7500 - val_loss: 0.6559 - val_acc: 0.6600\n",
            "Epoch 119/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6555 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5897 - acc: 0.7375 - val_loss: 0.6555 - val_acc: 0.6600\n",
            "Epoch 120/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6555 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5880 - acc: 0.7625 - val_loss: 0.6555 - val_acc: 0.6600\n",
            "Epoch 121/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6558 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5869 - acc: 0.7625 - val_loss: 0.6558 - val_acc: 0.6600\n",
            "Epoch 122/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6555 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5865 - acc: 0.7550 - val_loss: 0.6555 - val_acc: 0.6600\n",
            "Epoch 123/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6553 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5850 - acc: 0.7600 - val_loss: 0.6553 - val_acc: 0.6600\n",
            "Epoch 124/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6550 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5855 - acc: 0.7525 - val_loss: 0.6550 - val_acc: 0.6700\n",
            "Epoch 125/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6549 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5900 - acc: 0.7475 - val_loss: 0.6549 - val_acc: 0.6700\n",
            "Epoch 126/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6547 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5855 - acc: 0.7500 - val_loss: 0.6547 - val_acc: 0.6500\n",
            "Epoch 127/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6545 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5840 - acc: 0.7550 - val_loss: 0.6545 - val_acc: 0.6500\n",
            "Epoch 128/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6545 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5825 - acc: 0.7600 - val_loss: 0.6545 - val_acc: 0.6700\n",
            "Epoch 129/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6543 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5814 - acc: 0.7525 - val_loss: 0.6543 - val_acc: 0.6500\n",
            "Epoch 130/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6543 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5806 - acc: 0.7825 - val_loss: 0.6543 - val_acc: 0.6700\n",
            "Epoch 131/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6540 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5788 - acc: 0.7525 - val_loss: 0.6540 - val_acc: 0.6500\n",
            "Epoch 132/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6539 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5820 - acc: 0.7625 - val_loss: 0.6539 - val_acc: 0.6500\n",
            "Epoch 133/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6539 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5883 - acc: 0.7375 - val_loss: 0.6539 - val_acc: 0.6700\n",
            "Epoch 134/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6537 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5797 - acc: 0.7550 - val_loss: 0.6537 - val_acc: 0.6500\n",
            "Epoch 135/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6539 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5766 - acc: 0.7675 - val_loss: 0.6539 - val_acc: 0.6700\n",
            "Epoch 136/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6537 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5777 - acc: 0.7625 - val_loss: 0.6537 - val_acc: 0.6700\n",
            "Epoch 137/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6535 - acc: 0.6600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5811 - acc: 0.7450 - val_loss: 0.6535 - val_acc: 0.6600\n",
            "Epoch 138/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6537 - acc: 0.6700\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5779 - acc: 0.7625 - val_loss: 0.6537 - val_acc: 0.6700\n",
            "Epoch 139/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6533 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5804 - acc: 0.7375 - val_loss: 0.6533 - val_acc: 0.6500\n",
            "Epoch 140/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6530 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5801 - acc: 0.7600 - val_loss: 0.6530 - val_acc: 0.6500\n",
            "Epoch 141/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6532 - acc: 0.6700\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5753 - acc: 0.7725 - val_loss: 0.6532 - val_acc: 0.6700\n",
            "Epoch 142/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6530 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5754 - acc: 0.7700 - val_loss: 0.6530 - val_acc: 0.6500\n",
            "Epoch 143/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6527 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5794 - acc: 0.7450 - val_loss: 0.6527 - val_acc: 0.6500\n",
            "Epoch 144/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6526 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5757 - acc: 0.7800 - val_loss: 0.6526 - val_acc: 0.6500\n",
            "Epoch 145/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6528 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5736 - acc: 0.7700 - val_loss: 0.6528 - val_acc: 0.6500\n",
            "Epoch 146/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6525 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5736 - acc: 0.7675 - val_loss: 0.6525 - val_acc: 0.6500\n",
            "Epoch 147/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6523 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5752 - acc: 0.7675 - val_loss: 0.6523 - val_acc: 0.6500\n",
            "Epoch 148/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6522 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5738 - acc: 0.7525 - val_loss: 0.6522 - val_acc: 0.6500\n",
            "Epoch 149/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6521 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5729 - acc: 0.7575 - val_loss: 0.6521 - val_acc: 0.6500\n",
            "Epoch 150/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6520 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5699 - acc: 0.7650 - val_loss: 0.6520 - val_acc: 0.6500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgtyZLsvG_ls",
        "colab_type": "text"
      },
      "source": [
        "**Learning curves**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-ketbynHDhE",
        "colab_type": "code",
        "outputId": "f807a108-17f5-4bb0-b198-59eea61d758f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "#get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy - 5')\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.title('Training and Validation Loss - 5')\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwJ1HAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSRP/P7C6yXE01pMoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POHLQd9PPMLX",
        "colab_type": "code",
        "outputId": "672d8a33-2124-45ff-d0eb-7b308a23cd4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "_, acc = model.evaluate_generator(generator=train_generator)\n",
        "print('Accuracy on training set: {}%'.format(round(acc, 3)*100))\n",
        "\n",
        "_, acc = model.evaluate_generator(generator=validation_generator)\n",
        "print('Accuracy on validation set: {}%'.format(round(acc, 3)*100))\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set: 77.99999713897705%\n",
            "Accuracy on validation set: 64.99999761581421%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeN1ZqTAM42i",
        "colab_type": "text"
      },
      "source": [
        "**Export Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkUfyxqDM3NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keras library import  for Saving and loading model and weights\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwvXWQnGEENs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "model_json = model.to_json()\n",
        "\n",
        "\n",
        "with open(\"drive/Colab Notebooks/model/5_model_num.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"drive/Colab Notebooks/model/5_model_num.h5\")\n",
        "model.save('drive/Colab Notebooks/model/5_model_num.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}