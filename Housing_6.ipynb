{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Housing 6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caroref/model/blob/master/Housing_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ascUYWYXFlTS",
        "colab_type": "code",
        "outputId": "aaea4bcd-24d2-4f22-8635-c763b73c422f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TensorFlow version is \", tf.__version__)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0605 22:37:15.620428 140170758981504 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version is  1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs-4SeS3Fpc5",
        "colab_type": "code",
        "outputId": "0bf03fe6-5abe-4d2d-e9cd-67036a430388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Allow Colab to access drive\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "#linking Colab to use drive as if local\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDNLdWVxP2jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main directories\n",
        "trn_dir = 'drive/Colab/Final/Test6/Train/'\n",
        "tst_dir = 'drive/Colab/Final/Test6/Test/'\n",
        "\n",
        "#subdirectories\n",
        "trn_like = os.path.join(trn_dir, 'Like')\n",
        "tst_like = os.path.join(tst_dir, 'Like')\n",
        "\n",
        "trn_disl = os.path.join(trn_dir, 'Dislike')\n",
        "tst_disl = os.path.join(tst_dir, 'Dislike')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo3GVjj1FsEn",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5BtzOSmbZB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All images will be resized to image_size\n",
        "\n",
        "image_size = 640 \n",
        "batch_size = 100\n",
        "\n",
        "# create Image Data Generator for Image Data Augmentation\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "                rescale=1./255, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTvB7EieGDAN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7265df06-21f9-40fd-d15b-bc060eb5d493"
      },
      "source": [
        "# Create the train and validation generators and specify the train dataset directory, \n",
        "# image size, batch size, binary classification, and subset (training or validation).\n",
        "\n",
        "\n",
        "# Flow training images in batches using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,  \n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode ='binary',\n",
        "                subset = 'training') \n",
        "\n",
        "# Flow validation images in batches using validation_datagen generator\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                trn_dir,\n",
        "                target_size=(image_size, image_size),\n",
        "                batch_size=batch_size,\n",
        "                class_mode='binary',\n",
        "                subset = 'validation')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ur5r8uGWQO",
        "colab_type": "text"
      },
      "source": [
        "# Create the base model from the pre-trained convnets\n",
        "\n",
        "Using the output of last layer (bottleneck layer). Keep other layers frozen (these are specific to their training, less useful here?) Bottleneck layer should be more general.\n",
        "\n",
        "Using Inception v3 model with weights trained on ImageNet.\n",
        "The argument, include_top=False, means that we have a network that isn't including classification layers at the top. This is good for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCLmagoGbc3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "15da5abd-ef76-40a5-c150-5c6c5db2f385"
      },
      "source": [
        "IMG_SHAPE = (image_size, image_size, 3)\n",
        "\n",
        "base_model = keras.applications.inception_v3.InceptionV3(\n",
        "             include_top=False, weights='imagenet', input_shape=IMG_SHAPE)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0605 22:44:15.590570 140170758981504 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wXzV2zlGduw",
        "colab_type": "text"
      },
      "source": [
        "# Feature extraction\n",
        "We will freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.\n",
        "\n",
        "It's important to freeze the convolutional based before we compile and train the model. By freezing (or setting layer.trainable = False), we prevent the weights in these layers from being updated during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCk3KhmGGhaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11821
        },
        "outputId": "ebc9ecd5-f7b0-4d63-e1d8-565891ec29fd"
      },
      "source": [
        "#freeze base\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "# Look at the base model architecture\n",
        "base_model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 640, 640, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 319, 319, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 319, 319, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 319, 319, 32) 0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 317, 317, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 317, 317, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 317, 317, 32) 0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 317, 317, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 317, 317, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 317, 317, 64) 0           batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 158, 158, 64) 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 158, 158, 80) 5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_3 (Batch (None, 158, 158, 80) 240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 158, 158, 80) 0           batch_normalization_v1_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 156, 156, 192 138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 156, 156, 192 576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 156, 156, 192 0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 77, 77, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_8 (Batch (None, 77, 77, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 77, 77, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 77, 77, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_6 (Batch (None, 77, 77, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_9 (Batch (None, 77, 77, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 77, 77, 48)   0           batch_normalization_v1_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 77, 77, 96)   0           batch_normalization_v1_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 77, 77, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 77, 77, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 77, 77, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 77, 77, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 77, 77, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_5 (Batch (None, 77, 77, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_7 (Batch (None, 77, 77, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_10 (Batc (None, 77, 77, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_11 (Batc (None, 77, 77, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 77, 77, 64)   0           batch_normalization_v1_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_10[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 77, 77, 32)   0           batch_normalization_v1_11[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 77, 77, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_15 (Batc (None, 77, 77, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_15[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 77, 77, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 77, 77, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_13 (Batc (None, 77, 77, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_16 (Batc (None, 77, 77, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_13[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_16[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 77, 77, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 77, 77, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 77, 77, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 77, 77, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 77, 77, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_12 (Batc (None, 77, 77, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_14 (Batc (None, 77, 77, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_17 (Batc (None, 77, 77, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_18 (Batc (None, 77, 77, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_12[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_14[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_17[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_18[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 77, 77, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_22 (Batc (None, 77, 77, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_22[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 77, 77, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 77, 77, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_20 (Batc (None, 77, 77, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_23 (Batc (None, 77, 77, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 77, 77, 48)   0           batch_normalization_v1_20[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_23[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 77, 77, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 77, 77, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 77, 77, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 77, 77, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 77, 77, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_19 (Batc (None, 77, 77, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_21 (Batc (None, 77, 77, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_24 (Batc (None, 77, 77, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_25 (Batc (None, 77, 77, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_19[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_21[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_24[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_25[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 77, 77, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 77, 77, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_27 (Batc (None, 77, 77, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 77, 77, 64)   0           batch_normalization_v1_27[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 77, 77, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_28 (Batc (None, 77, 77, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 77, 77, 96)   0           batch_normalization_v1_28[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 38, 38, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 38, 38, 96)   82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_26 (Batc (None, 38, 38, 384)  1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_29 (Batc (None, 38, 38, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 38, 38, 384)  0           batch_normalization_v1_26[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 38, 38, 96)   0           batch_normalization_v1_29[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 38, 38, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 38, 38, 768)  0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_34 (Batc (None, 38, 38, 128)  384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_34[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 38, 38, 128)  114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_35 (Batc (None, 38, 38, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_35[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 38, 38, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 38, 38, 128)  114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_31 (Batc (None, 38, 38, 128)  384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_36 (Batc (None, 38, 38, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_31[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_36[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 38, 38, 128)  114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 38, 38, 128)  114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_32 (Batc (None, 38, 38, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_37 (Batc (None, 38, 38, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_32[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 38, 38, 128)  0           batch_normalization_v1_37[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 38, 38, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 38, 38, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 38, 38, 192)  172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 38, 38, 192)  172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_30 (Batc (None, 38, 38, 192)  576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_33 (Batc (None, 38, 38, 192)  576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_38 (Batc (None, 38, 38, 192)  576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_39 (Batc (None, 38, 38, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_30[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_33[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_38[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_39[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 38, 38, 768)  0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_44 (Batc (None, 38, 38, 160)  480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_44[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 38, 38, 160)  179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_45 (Batc (None, 38, 38, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_45[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 38, 38, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 38, 38, 160)  179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_41 (Batc (None, 38, 38, 160)  480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_46 (Batc (None, 38, 38, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_41[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_46[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 38, 38, 160)  179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 38, 38, 160)  179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_42 (Batc (None, 38, 38, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_47 (Batc (None, 38, 38, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_42[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_47[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 38, 38, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 38, 38, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 38, 38, 192)  215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 38, 38, 192)  215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_40 (Batc (None, 38, 38, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_43 (Batc (None, 38, 38, 192)  576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_48 (Batc (None, 38, 38, 192)  576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_49 (Batc (None, 38, 38, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_40[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_43[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_48[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_49[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 38, 38, 768)  0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_54 (Batc (None, 38, 38, 160)  480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_54[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 38, 38, 160)  179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_55 (Batc (None, 38, 38, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_55[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 38, 38, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 38, 38, 160)  179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_51 (Batc (None, 38, 38, 160)  480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_56 (Batc (None, 38, 38, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_51[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_56[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 38, 38, 160)  179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 38, 38, 160)  179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_52 (Batc (None, 38, 38, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_57 (Batc (None, 38, 38, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_52[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 38, 38, 160)  0           batch_normalization_v1_57[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 38, 38, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 38, 38, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 38, 38, 192)  215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 38, 38, 192)  215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_50 (Batc (None, 38, 38, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_53 (Batc (None, 38, 38, 192)  576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_58 (Batc (None, 38, 38, 192)  576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_59 (Batc (None, 38, 38, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_50[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_53[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_58[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_59[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 38, 38, 768)  0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_64 (Batc (None, 38, 38, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_64[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 38, 38, 192)  258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_65 (Batc (None, 38, 38, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_65[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 38, 38, 192)  258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_61 (Batc (None, 38, 38, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_66 (Batc (None, 38, 38, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_61[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_66[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 38, 38, 192)  258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 38, 38, 192)  258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_62 (Batc (None, 38, 38, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_67 (Batc (None, 38, 38, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_62[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_67[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 38, 38, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 38, 38, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 38, 38, 192)  258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 38, 38, 192)  258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 38, 38, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_60 (Batc (None, 38, 38, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_63 (Batc (None, 38, 38, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_68 (Batc (None, 38, 38, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_69 (Batc (None, 38, 38, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_60[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_63[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_68[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_69[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 38, 38, 768)  0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_72 (Batc (None, 38, 38, 192)  576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_72[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 38, 38, 192)  258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_73 (Batc (None, 38, 38, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_73[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 38, 38, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 38, 38, 192)  258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_70 (Batc (None, 38, 38, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_74 (Batc (None, 38, 38, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_70[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 38, 38, 192)  0           batch_normalization_v1_74[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 18, 18, 320)  552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 18, 18, 192)  331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_71 (Batc (None, 18, 18, 320)  960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_75 (Batc (None, 18, 18, 192)  576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_71[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_75[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 18, 18, 768)  0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 18, 18, 1280) 0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 18, 18, 448)  573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_80 (Batc (None, 18, 18, 448)  1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_80[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 18, 18, 384)  491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 18, 18, 384)  1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_77 (Batc (None, 18, 18, 384)  1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_81 (Batc (None, 18, 18, 384)  1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_77[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_81[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 18, 18, 384)  442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 18, 18, 384)  442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 18, 18, 1280) 0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 18, 18, 320)  409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_78 (Batc (None, 18, 18, 384)  1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_79 (Batc (None, 18, 18, 384)  1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_82 (Batc (None, 18, 18, 384)  1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_83 (Batc (None, 18, 18, 384)  1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 18, 18, 192)  245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_76 (Batc (None, 18, 18, 320)  960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_78[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_79[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_82[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_83[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_84 (Batc (None, 18, 18, 192)  576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_76[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 18, 18, 768)  0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 18, 18, 768)  0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_84[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 18, 18, 2048) 0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 18, 18, 448)  917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_89 (Batc (None, 18, 18, 448)  1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 18, 18, 448)  0           batch_normalization_v1_89[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 18, 18, 384)  786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 18, 18, 384)  1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_86 (Batc (None, 18, 18, 384)  1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_90 (Batc (None, 18, 18, 384)  1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_86[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_90[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 18, 18, 384)  442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 18, 18, 384)  442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 18, 18, 2048) 0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 18, 18, 320)  655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_87 (Batc (None, 18, 18, 384)  1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_88 (Batc (None, 18, 18, 384)  1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_91 (Batc (None, 18, 18, 384)  1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_92 (Batc (None, 18, 18, 384)  1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 18, 18, 192)  393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_85 (Batc (None, 18, 18, 320)  960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_87[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_88[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_91[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 18, 18, 384)  0           batch_normalization_v1_92[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_93 (Batc (None, 18, 18, 192)  576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 18, 18, 320)  0           batch_normalization_v1_85[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 18, 18, 768)  0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 18, 18, 768)  0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 18, 18, 192)  0           batch_normalization_v1_93[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 18, 18, 2048) 0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 0\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUc1ad5qGnMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add a classification head\n",
        "#adding a few layers on top of the base model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  keras.layers.GlobalAveragePooling2D(),\n",
        "  keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "#relatively basic layer. no deep neural networks (all happening beforehand). Similar to a logit regression. plain classification\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbhLA3TGr1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "fae4e7c5-1c92-40d0-f2f1-01c1f97dccc8"
      },
      "source": [
        "#Compile the model (Must do before training!)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 18, 18, 2048)      21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 21,804,833\n",
            "Trainable params: 2,049\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th655DVEGwop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b95da69c-b617-45b7-8651-f3b319ee7bc2"
      },
      "source": [
        "#These 2K trainable parameters are divided among 2 TensorFlow Variable objects, the weights and biases of the two dense layers:\n",
        "\n",
        "\n",
        "len(model.trainable_variables)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZeMfEwoG3wm",
        "colab_type": "text"
      },
      "source": [
        "**Train the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNgpgcJjG8xl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8144
        },
        "outputId": "669418ac-7c13-426d-8b30-9e166f68be4a"
      },
      "source": [
        "epochs = 150\n",
        "steps_per_epoch = train_generator.n // batch_size\n",
        "validation_steps = validation_generator.n // batch_size\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch = steps_per_epoch,\n",
        "                              epochs=epochs,\n",
        "                              workers=4,\n",
        "                              validation_data=validation_generator,\n",
        "                              validation_steps=validation_steps)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0605 22:44:31.793896 140170758981504 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "1/1 [==============================] - 64s 64s/step - loss: 0.6859 - acc: 0.5400\n",
            "4/4 [==============================] - 151s 38s/step - loss: 0.7039 - acc: 0.4750 - val_loss: 0.6859 - val_acc: 0.5400\n",
            "Epoch 2/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6879 - acc: 0.5300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.7018 - acc: 0.4650 - val_loss: 0.6879 - val_acc: 0.5300\n",
            "Epoch 3/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6916 - acc: 0.5200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6987 - acc: 0.4675 - val_loss: 0.6916 - val_acc: 0.5200\n",
            "Epoch 4/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6914 - acc: 0.5300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6984 - acc: 0.4900 - val_loss: 0.6914 - val_acc: 0.5300\n",
            "Epoch 5/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6938 - acc: 0.5200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6962 - acc: 0.4850 - val_loss: 0.6938 - val_acc: 0.5200\n",
            "Epoch 6/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6932 - acc: 0.5600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6959 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.5600\n",
            "Epoch 7/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6946 - acc: 0.5500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6974 - acc: 0.4975 - val_loss: 0.6946 - val_acc: 0.5500\n",
            "Epoch 8/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6956 - acc: 0.5600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6929 - acc: 0.5325 - val_loss: 0.6956 - val_acc: 0.5600\n",
            "Epoch 9/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6972 - acc: 0.5400\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6919 - acc: 0.5075 - val_loss: 0.6972 - val_acc: 0.5400\n",
            "Epoch 10/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6990 - acc: 0.5300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6925 - acc: 0.5350 - val_loss: 0.6990 - val_acc: 0.5300\n",
            "Epoch 11/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6983 - acc: 0.5000\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6890 - acc: 0.5375 - val_loss: 0.6983 - val_acc: 0.5000\n",
            "Epoch 12/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6979 - acc: 0.5000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6900 - acc: 0.5350 - val_loss: 0.6979 - val_acc: 0.5000\n",
            "Epoch 13/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6976 - acc: 0.5100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6891 - acc: 0.5400 - val_loss: 0.6976 - val_acc: 0.5100\n",
            "Epoch 14/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6996 - acc: 0.4700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6886 - acc: 0.5250 - val_loss: 0.6996 - val_acc: 0.4700\n",
            "Epoch 15/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6981 - acc: 0.4800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6847 - acc: 0.5500 - val_loss: 0.6981 - val_acc: 0.4800\n",
            "Epoch 16/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6976 - acc: 0.4900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6852 - acc: 0.5525 - val_loss: 0.6976 - val_acc: 0.4900\n",
            "Epoch 17/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6975 - acc: 0.4600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6837 - acc: 0.5600 - val_loss: 0.6975 - val_acc: 0.4600\n",
            "Epoch 18/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6961 - acc: 0.4700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6860 - acc: 0.5625 - val_loss: 0.6961 - val_acc: 0.4700\n",
            "Epoch 19/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6953 - acc: 0.4900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6838 - acc: 0.5550 - val_loss: 0.6953 - val_acc: 0.4900\n",
            "Epoch 20/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6940 - acc: 0.5100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6833 - acc: 0.5700 - val_loss: 0.6940 - val_acc: 0.5100\n",
            "Epoch 21/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6936 - acc: 0.5100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6798 - acc: 0.5825 - val_loss: 0.6936 - val_acc: 0.5100\n",
            "Epoch 22/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6919 - acc: 0.5100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6780 - acc: 0.5850 - val_loss: 0.6919 - val_acc: 0.5100\n",
            "Epoch 23/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6907 - acc: 0.5100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6780 - acc: 0.5900 - val_loss: 0.6907 - val_acc: 0.5100\n",
            "Epoch 24/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6900 - acc: 0.5100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6758 - acc: 0.6025 - val_loss: 0.6900 - val_acc: 0.5100\n",
            "Epoch 25/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6893 - acc: 0.5200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6750 - acc: 0.5725 - val_loss: 0.6893 - val_acc: 0.5200\n",
            "Epoch 26/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6893 - acc: 0.5300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6753 - acc: 0.6200 - val_loss: 0.6893 - val_acc: 0.5300\n",
            "Epoch 27/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6881 - acc: 0.5200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6729 - acc: 0.6100 - val_loss: 0.6881 - val_acc: 0.5200\n",
            "Epoch 28/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6871 - acc: 0.5500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6719 - acc: 0.6025 - val_loss: 0.6871 - val_acc: 0.5500\n",
            "Epoch 29/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6872 - acc: 0.5600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6712 - acc: 0.6350 - val_loss: 0.6872 - val_acc: 0.5600\n",
            "Epoch 30/150\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6865 - acc: 0.5400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6707 - acc: 0.6225 - val_loss: 0.6865 - val_acc: 0.5400\n",
            "Epoch 31/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6864 - acc: 0.5700\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6701 - acc: 0.6275 - val_loss: 0.6864 - val_acc: 0.5700\n",
            "Epoch 32/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6863 - acc: 0.5600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6678 - acc: 0.6200 - val_loss: 0.6863 - val_acc: 0.5600\n",
            "Epoch 33/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6856 - acc: 0.5800\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6675 - acc: 0.6400 - val_loss: 0.6856 - val_acc: 0.5800\n",
            "Epoch 34/150\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6846 - acc: 0.5600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6692 - acc: 0.6275 - val_loss: 0.6846 - val_acc: 0.5600\n",
            "Epoch 35/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6835 - acc: 0.5600\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6664 - acc: 0.6425 - val_loss: 0.6835 - val_acc: 0.5600\n",
            "Epoch 36/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6831 - acc: 0.5600\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6642 - acc: 0.6375 - val_loss: 0.6831 - val_acc: 0.5600\n",
            "Epoch 37/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6823 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6644 - acc: 0.6250 - val_loss: 0.6823 - val_acc: 0.5900\n",
            "Epoch 38/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6819 - acc: 0.5800\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6613 - acc: 0.6525 - val_loss: 0.6819 - val_acc: 0.5800\n",
            "Epoch 39/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6806 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6610 - acc: 0.6425 - val_loss: 0.6806 - val_acc: 0.6100\n",
            "Epoch 40/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6802 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6619 - acc: 0.6425 - val_loss: 0.6802 - val_acc: 0.5900\n",
            "Epoch 41/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6794 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6599 - acc: 0.6575 - val_loss: 0.6794 - val_acc: 0.6100\n",
            "Epoch 42/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6794 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6596 - acc: 0.6325 - val_loss: 0.6794 - val_acc: 0.6100\n",
            "Epoch 43/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6787 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6583 - acc: 0.6300 - val_loss: 0.6787 - val_acc: 0.6000\n",
            "Epoch 44/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6784 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6556 - acc: 0.6675 - val_loss: 0.6784 - val_acc: 0.6100\n",
            "Epoch 45/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6783 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6565 - acc: 0.6775 - val_loss: 0.6783 - val_acc: 0.6100\n",
            "Epoch 46/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6776 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6544 - acc: 0.6725 - val_loss: 0.6776 - val_acc: 0.6100\n",
            "Epoch 47/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6777 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6536 - acc: 0.6525 - val_loss: 0.6777 - val_acc: 0.6100\n",
            "Epoch 48/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6765 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6548 - acc: 0.6500 - val_loss: 0.6765 - val_acc: 0.6100\n",
            "Epoch 49/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6761 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6537 - acc: 0.6650 - val_loss: 0.6761 - val_acc: 0.6200\n",
            "Epoch 50/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6762 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6516 - acc: 0.6750 - val_loss: 0.6762 - val_acc: 0.6100\n",
            "Epoch 51/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6758 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6503 - acc: 0.6700 - val_loss: 0.6758 - val_acc: 0.6100\n",
            "Epoch 52/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6755 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6520 - acc: 0.6475 - val_loss: 0.6755 - val_acc: 0.6300\n",
            "Epoch 53/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6754 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6497 - acc: 0.6575 - val_loss: 0.6754 - val_acc: 0.6300\n",
            "Epoch 54/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6752 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6506 - acc: 0.6600 - val_loss: 0.6752 - val_acc: 0.6300\n",
            "Epoch 55/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6749 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6516 - acc: 0.6425 - val_loss: 0.6749 - val_acc: 0.6200\n",
            "Epoch 56/150\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6748 - acc: 0.6100\n",
            "4/4 [==============================] - 15s 4s/step - loss: 0.6481 - acc: 0.6625 - val_loss: 0.6748 - val_acc: 0.6100\n",
            "Epoch 57/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6746 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6477 - acc: 0.6550 - val_loss: 0.6746 - val_acc: 0.6200\n",
            "Epoch 58/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6745 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6484 - acc: 0.6575 - val_loss: 0.6745 - val_acc: 0.6300\n",
            "Epoch 59/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6737 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6446 - acc: 0.6775 - val_loss: 0.6737 - val_acc: 0.6200\n",
            "Epoch 60/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6734 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6448 - acc: 0.6800 - val_loss: 0.6734 - val_acc: 0.6100\n",
            "Epoch 61/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6730 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6462 - acc: 0.6825 - val_loss: 0.6730 - val_acc: 0.6100\n",
            "Epoch 62/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6727 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6448 - acc: 0.6675 - val_loss: 0.6727 - val_acc: 0.6100\n",
            "Epoch 63/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6722 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6429 - acc: 0.7000 - val_loss: 0.6722 - val_acc: 0.6100\n",
            "Epoch 64/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6718 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6412 - acc: 0.6775 - val_loss: 0.6718 - val_acc: 0.6100\n",
            "Epoch 65/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6715 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6394 - acc: 0.6750 - val_loss: 0.6715 - val_acc: 0.6300\n",
            "Epoch 66/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6716 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6447 - acc: 0.6575 - val_loss: 0.6716 - val_acc: 0.6300\n",
            "Epoch 67/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6709 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6390 - acc: 0.6900 - val_loss: 0.6709 - val_acc: 0.6100\n",
            "Epoch 68/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6705 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6395 - acc: 0.6700 - val_loss: 0.6705 - val_acc: 0.6200\n",
            "Epoch 69/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6703 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6379 - acc: 0.6850 - val_loss: 0.6703 - val_acc: 0.6200\n",
            "Epoch 70/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6703 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6378 - acc: 0.6750 - val_loss: 0.6703 - val_acc: 0.6300\n",
            "Epoch 71/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6698 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6381 - acc: 0.6750 - val_loss: 0.6698 - val_acc: 0.6100\n",
            "Epoch 72/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6699 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6368 - acc: 0.6875 - val_loss: 0.6699 - val_acc: 0.6400\n",
            "Epoch 73/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6690 - acc: 0.6000\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6377 - acc: 0.6800 - val_loss: 0.6690 - val_acc: 0.6000\n",
            "Epoch 74/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6688 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6347 - acc: 0.6775 - val_loss: 0.6688 - val_acc: 0.6100\n",
            "Epoch 75/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6685 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6352 - acc: 0.6850 - val_loss: 0.6685 - val_acc: 0.6100\n",
            "Epoch 76/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6682 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6363 - acc: 0.6800 - val_loss: 0.6682 - val_acc: 0.5900\n",
            "Epoch 77/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6678 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6350 - acc: 0.6900 - val_loss: 0.6678 - val_acc: 0.6000\n",
            "Epoch 78/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6675 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6329 - acc: 0.7025 - val_loss: 0.6675 - val_acc: 0.6000\n",
            "Epoch 79/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6673 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6303 - acc: 0.6925 - val_loss: 0.6673 - val_acc: 0.6100\n",
            "Epoch 80/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6669 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6312 - acc: 0.6900 - val_loss: 0.6669 - val_acc: 0.5900\n",
            "Epoch 81/150\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6666 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6316 - acc: 0.6750 - val_loss: 0.6666 - val_acc: 0.6000\n",
            "Epoch 82/150\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6663 - acc: 0.5900\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6325 - acc: 0.6925 - val_loss: 0.6663 - val_acc: 0.5900\n",
            "Epoch 83/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6661 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6313 - acc: 0.6900 - val_loss: 0.6661 - val_acc: 0.6100\n",
            "Epoch 84/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6657 - acc: 0.6000\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6273 - acc: 0.6925 - val_loss: 0.6657 - val_acc: 0.6000\n",
            "Epoch 85/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6655 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6298 - acc: 0.6725 - val_loss: 0.6655 - val_acc: 0.6100\n",
            "Epoch 86/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6652 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6268 - acc: 0.6900 - val_loss: 0.6652 - val_acc: 0.6200\n",
            "Epoch 87/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6651 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6266 - acc: 0.6875 - val_loss: 0.6651 - val_acc: 0.6200\n",
            "Epoch 88/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6646 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6263 - acc: 0.6975 - val_loss: 0.6646 - val_acc: 0.6300\n",
            "Epoch 89/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6642 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6274 - acc: 0.7075 - val_loss: 0.6642 - val_acc: 0.6200\n",
            "Epoch 90/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6640 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6255 - acc: 0.6850 - val_loss: 0.6640 - val_acc: 0.6100\n",
            "Epoch 91/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6637 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6255 - acc: 0.6975 - val_loss: 0.6637 - val_acc: 0.6100\n",
            "Epoch 92/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6634 - acc: 0.6100\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6242 - acc: 0.7000 - val_loss: 0.6634 - val_acc: 0.6100\n",
            "Epoch 93/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6631 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6232 - acc: 0.7050 - val_loss: 0.6631 - val_acc: 0.6200\n",
            "Epoch 94/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6629 - acc: 0.6100\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6216 - acc: 0.7050 - val_loss: 0.6629 - val_acc: 0.6100\n",
            "Epoch 95/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6626 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6200 - acc: 0.7075 - val_loss: 0.6626 - val_acc: 0.6200\n",
            "Epoch 96/150\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6627 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6255 - acc: 0.6925 - val_loss: 0.6627 - val_acc: 0.6200\n",
            "Epoch 97/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6623 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6185 - acc: 0.7100 - val_loss: 0.6623 - val_acc: 0.6200\n",
            "Epoch 98/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6622 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6215 - acc: 0.6800 - val_loss: 0.6622 - val_acc: 0.6300\n",
            "Epoch 99/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6622 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6197 - acc: 0.6950 - val_loss: 0.6622 - val_acc: 0.6300\n",
            "Epoch 100/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6619 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6172 - acc: 0.7025 - val_loss: 0.6619 - val_acc: 0.6200\n",
            "Epoch 101/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6615 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6180 - acc: 0.7100 - val_loss: 0.6615 - val_acc: 0.6200\n",
            "Epoch 102/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6612 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6193 - acc: 0.7150 - val_loss: 0.6612 - val_acc: 0.6200\n",
            "Epoch 103/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6610 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6190 - acc: 0.7000 - val_loss: 0.6610 - val_acc: 0.6300\n",
            "Epoch 104/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6607 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6150 - acc: 0.7050 - val_loss: 0.6607 - val_acc: 0.6300\n",
            "Epoch 105/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6605 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6168 - acc: 0.7125 - val_loss: 0.6605 - val_acc: 0.6300\n",
            "Epoch 106/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6604 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6161 - acc: 0.7075 - val_loss: 0.6604 - val_acc: 0.6200\n",
            "Epoch 107/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6600 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6137 - acc: 0.7125 - val_loss: 0.6600 - val_acc: 0.6300\n",
            "Epoch 108/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6598 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6145 - acc: 0.7075 - val_loss: 0.6598 - val_acc: 0.6300\n",
            "Epoch 109/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6596 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6134 - acc: 0.7050 - val_loss: 0.6596 - val_acc: 0.6300\n",
            "Epoch 110/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6594 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6121 - acc: 0.7175 - val_loss: 0.6594 - val_acc: 0.6300\n",
            "Epoch 111/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6591 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6117 - acc: 0.7200 - val_loss: 0.6591 - val_acc: 0.6300\n",
            "Epoch 112/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6590 - acc: 0.6200\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6099 - acc: 0.7175 - val_loss: 0.6590 - val_acc: 0.6200\n",
            "Epoch 113/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6589 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6122 - acc: 0.7200 - val_loss: 0.6589 - val_acc: 0.6200\n",
            "Epoch 114/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6586 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6110 - acc: 0.7175 - val_loss: 0.6586 - val_acc: 0.6300\n",
            "Epoch 115/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6584 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6084 - acc: 0.7200 - val_loss: 0.6584 - val_acc: 0.6300\n",
            "Epoch 116/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6583 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6131 - acc: 0.7250 - val_loss: 0.6583 - val_acc: 0.6300\n",
            "Epoch 117/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6581 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6084 - acc: 0.7125 - val_loss: 0.6581 - val_acc: 0.6300\n",
            "Epoch 118/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6580 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6096 - acc: 0.7350 - val_loss: 0.6580 - val_acc: 0.6400\n",
            "Epoch 119/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6579 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6070 - acc: 0.7125 - val_loss: 0.6579 - val_acc: 0.6500\n",
            "Epoch 120/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6579 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6079 - acc: 0.7225 - val_loss: 0.6579 - val_acc: 0.6300\n",
            "Epoch 121/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6577 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6066 - acc: 0.7250 - val_loss: 0.6577 - val_acc: 0.6500\n",
            "Epoch 122/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6576 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6053 - acc: 0.7225 - val_loss: 0.6576 - val_acc: 0.6500\n",
            "Epoch 123/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6574 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6059 - acc: 0.7225 - val_loss: 0.6574 - val_acc: 0.6500\n",
            "Epoch 124/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6573 - acc: 0.6300\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.6063 - acc: 0.7100 - val_loss: 0.6573 - val_acc: 0.6300\n",
            "Epoch 125/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6573 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6049 - acc: 0.7250 - val_loss: 0.6573 - val_acc: 0.6200\n",
            "Epoch 126/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6571 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6019 - acc: 0.7250 - val_loss: 0.6571 - val_acc: 0.6300\n",
            "Epoch 127/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6570 - acc: 0.6200\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6047 - acc: 0.7275 - val_loss: 0.6570 - val_acc: 0.6200\n",
            "Epoch 128/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6567 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.6041 - acc: 0.7175 - val_loss: 0.6567 - val_acc: 0.6500\n",
            "Epoch 129/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6566 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6052 - acc: 0.7275 - val_loss: 0.6566 - val_acc: 0.6400\n",
            "Epoch 130/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6566 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6022 - acc: 0.7275 - val_loss: 0.6566 - val_acc: 0.6300\n",
            "Epoch 131/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6564 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6029 - acc: 0.7300 - val_loss: 0.6564 - val_acc: 0.6300\n",
            "Epoch 132/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6562 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6030 - acc: 0.7125 - val_loss: 0.6562 - val_acc: 0.6400\n",
            "Epoch 133/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6561 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6000 - acc: 0.7250 - val_loss: 0.6561 - val_acc: 0.6300\n",
            "Epoch 134/150\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6560 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6016 - acc: 0.7275 - val_loss: 0.6560 - val_acc: 0.6500\n",
            "Epoch 135/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6558 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6007 - acc: 0.7300 - val_loss: 0.6558 - val_acc: 0.6500\n",
            "Epoch 136/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6559 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5990 - acc: 0.7175 - val_loss: 0.6559 - val_acc: 0.6300\n",
            "Epoch 137/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6556 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5998 - acc: 0.7325 - val_loss: 0.6556 - val_acc: 0.6400\n",
            "Epoch 138/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6555 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6022 - acc: 0.7325 - val_loss: 0.6555 - val_acc: 0.6400\n",
            "Epoch 139/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6554 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6010 - acc: 0.7200 - val_loss: 0.6554 - val_acc: 0.6500\n",
            "Epoch 140/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6553 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.6008 - acc: 0.7275 - val_loss: 0.6553 - val_acc: 0.6500\n",
            "Epoch 141/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6552 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5964 - acc: 0.7150 - val_loss: 0.6552 - val_acc: 0.6400\n",
            "Epoch 142/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6551 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5970 - acc: 0.7250 - val_loss: 0.6551 - val_acc: 0.6400\n",
            "Epoch 143/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6549 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5992 - acc: 0.7200 - val_loss: 0.6549 - val_acc: 0.6500\n",
            "Epoch 144/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6549 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5970 - acc: 0.7200 - val_loss: 0.6549 - val_acc: 0.6400\n",
            "Epoch 145/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6547 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5950 - acc: 0.7325 - val_loss: 0.6547 - val_acc: 0.6400\n",
            "Epoch 146/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6546 - acc: 0.6600\n",
            "4/4 [==============================] - 14s 4s/step - loss: 0.5953 - acc: 0.7300 - val_loss: 0.6546 - val_acc: 0.6600\n",
            "Epoch 147/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6544 - acc: 0.6500\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5959 - acc: 0.7125 - val_loss: 0.6544 - val_acc: 0.6500\n",
            "Epoch 148/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6544 - acc: 0.6400\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5938 - acc: 0.7425 - val_loss: 0.6544 - val_acc: 0.6400\n",
            "Epoch 149/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6544 - acc: 0.6300\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5921 - acc: 0.7325 - val_loss: 0.6544 - val_acc: 0.6300\n",
            "Epoch 150/150\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6541 - acc: 0.6500\n",
            "4/4 [==============================] - 14s 3s/step - loss: 0.5942 - acc: 0.7325 - val_loss: 0.6541 - val_acc: 0.6500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgtyZLsvG_ls",
        "colab_type": "text"
      },
      "source": [
        "**Learning curves**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-ketbynHDhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "8b8b56c9-a4d3-48f7-b1bf-b80f0e886eb8"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "#get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy - 6')\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.title('Training and Validation Loss - 6')\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAD1CAYAAABJP2ylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POHLQd9PPMLX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "13b3fcf7-c725-4a6b-ddfd-b45ca17f04ba"
      },
      "source": [
        "_, acc = model.evaluate_generator(generator=train_generator)\n",
        "print('Accuracy on training set: {}%'.format(round(acc, 3)*100))\n",
        "\n",
        "_, acc = model.evaluate_generator(generator=validation_generator)\n",
        "print('Accuracy on validation set: {}%'.format(round(acc, 3)*100))\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set: 72.50000238418579%\n",
            "Accuracy on validation set: 64.99999761581421%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeN1ZqTAM42i",
        "colab_type": "text"
      },
      "source": [
        "**Export Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkUfyxqDM3NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keras library import  for Saving and loading model and weights\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwvXWQnGEENs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "model_json = model.to_json()\n",
        "\n",
        "\n",
        "with open(\"drive/Colab Notebooks/model/6_model_num.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"drive/Colab Notebooks/model/6_model_num.h5\")\n",
        "model.save('drive/Colab Notebooks/model/6_model_num.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}